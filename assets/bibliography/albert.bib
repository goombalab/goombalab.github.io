%%% CNNs

@article{krizhevsky2017imagenet,
  IDS={alexnet},
  title={Image{N}et classification with Deep Convolutional Neural Networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{szegedy2015going,
  title={Going Deeper with Convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{he2016deep,
  title={Deep Residual Learning for Image Recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{oord2016wavenet,
  title={Wave{N}et: A Generative Model for Raw Audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@article{paine2016fast,
  title={Fast {Wave{N}et} Generation Algorithm},
  author={Paine, Tom Le and Khorrami, Pooya and Chang, Shiyu and Zhang, Yang and Ramachandran, Prajit and Hasegawa-Johnson, Mark A and Huang, Thomas S},
  journal={arXiv preprint arXiv:1611.09482},
  year={2016}
}

@inproceedings{chollet2017xception,
  title={Xception: Deep Learning with Depthwise Separable Convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1251--1258},
  year={2017}
}

@inproceedings{liu2022convnet,
  IDS={convnext},
  title={A {Conv{N}et} for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11976--11986},
  year={2022}
}


% RNN Related Work
@article{lstm,
  title={Long Short-Term Memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{hochreiter1991untersuchungen,
  title={Untersuchungen zu dynamischen neuronalen Netzen},
  author={Hochreiter, Sepp},
  journal={Diploma, Technische Universit{\"a}t M{\"u}nchen},
  volume={91},
  number={1},
  pages={31},
  year={1991}
}

@misc{hochreiter2001gradient,
  title={Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-term Dependencies},
  author={Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen and others},
  year={2001},
  publisher={A field guide to dynamical recurrent neural networks. IEEE Press}
}

@article{jaeger2004harnessing,
  title={Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
  author={Jaeger, Herbert and Haas, Harald},
  journal={Science},
  volume={304},
  number={5667},
  pages={78--80},
  year={2004},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{bengio2013advances,
  title={Advances in optimizing recurrent networks},
  author={Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8624--8628},
  year={2013},
  organization={IEEE}
}

@inproceedings{pascanu2013difficulty,
  title={On the Difficulty of Training Recurrent Neural Networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1310--1318},
  year={2013}
}

@inproceedings{cho2014learning,
  title={Learning Phrase Representations using {R}{N}{N} Encoder-decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2014}
}

@article{chung2014empirical,
  IDS={gru},
  title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@inproceedings{koutnik2014clockwork,
  title={A Clockwork {R}{N}{N}},
  author={Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen},
  booktitle={International Conference on Machine Learning},
  pages={1863--1871},
  year={2014},
  organization={PMLR}
}

@inproceedings{chung2017hierarchical,
  title={Hierarchical Multiscale Recurrent Neural Networks},
  author={Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@inproceedings{chang2017dilated,
  title={Dilated Recurrent Neural Networks},
  author={Chang, Shiyu and Zhang, Yang and Han, Wei and Yu, Mo and Guo, Xiaoxiao and Tan, Wei and Cui, Xiaodong and Witbrock, Michael and Hasegawa-Johnson, Mark A and Huang, Thomas S},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{shen2019ordered,
  title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
  author={Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
}

@article{zhou2016minimal,
  title={Minimal Gated Unit for Recurrent Neural Networks},
  author={Zhou, Guo-Bing and Wu, Jianxin and Zhang, Chen-Lin and Zhou, Zhi-Hua},
  journal={International Journal of Automation and Computing},
  volume={13},
  number={3},
  pages={226--234},
  year={2016},
  publisher={Springer}
}

@inproceedings{heck2017simplified,
  title={Simplified Minimal Gated Unit Variations for Recurrent Neural Networks},
  author={Heck, Joel C and Salem, Fathi M},
  booktitle={2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)},
  pages={1593--1596},
  year={2017},
  organization={IEEE}
}

@inproceedings{neil2016phased,
  title={Phased {L}{S}{T}{M}: Accelerating Recurrent Network Training for Long or Event-based Sequences},
  author={Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  year={2016}
}

@article{van2018unreasonable,
  title={The Unreasonable Effectiveness of the Forget Gate},
  author={Van Der Westhuizen, Jos and Lasenby, Joan},
  journal={arXiv preprint arXiv:1804.04849},
  year={2018}
}

@article{lei2017simple,
  IDS={sru},
  title={Simple Recurrent Units for Highly Parallelizable Recurrence},
  author={Lei, Tao and Zhang, Yu and Wang, Sida I and Dai, Hui and Artzi, Yoav},
  journal={arXiv preprint arXiv:1709.02755},
  year={2017}
}

@article{bradbury2016quasi,
  IDS={qrnn},
  title={Quasi-recurrent Neural Networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}

@article{merity2018scalable,
  title={Scalable Language Modeling: {W}iki{T}ext-103 on a Single {G}{P}{U} in 12 Hours},
  author={Merity, Stephen and Keskar, Nitish Shirish and Bradbury, James and Socher, Richard},
  journal={SysML},
  year={2018}
}

@inproceedings{balduzzi2016strongly,
  title={Strongly-typed Recurrent Neural Networks},
  author={Balduzzi, David and Ghifary, Muhammad},
  booktitle={International Conference on Machine Learning},
  pages={1292--1300},
  year={2016},
  organization={PMLR}
}

@article{srivastava2014dropout,
  IDS={dropout},
  title={Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR.org}
}

@inproceedings{tompson2015efficient,
  title={Efficient Object Localization using Convolutional Networks},
  author={Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={648--656},
  year={2015}
}

@inproceedings{jozefowicz2015empirical,
  title={An Empirical Exploration of Recurrent Network Architectures},
  author={Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={2342--2350},
  year={2015}
}

@article{le2015simple,
  title={A Simple Way to Initialize Recurrent Networks of Rectified Linear Units},
  author={Le, Quoc V and Jaitly, Navdeep and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1504.00941},
  year={2015}
}

@article{greff2016lstm,
  title={{L}{S}{T}{M}: A Search Space Odyssey},
  author={Greff, Klaus and Srivastava, Rupesh K and Koutn{\'i}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"u}rgen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={28},
  number={10},
  pages={2222--2232},
  year={2016},
  publisher={IEEE}
}

@inproceedings{gulcehre2016noisy,
  title={Noisy Activation Functions},
  author={Gulcehre, Caglar and Moczulski, Marcin and Denil, Misha and Bengio, Yoshua},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={3059--3068},
  year={2016}
}

@article{jing2019gated,
  title={Gated Orthogonal Recurrent Units: On Learning to Forget},
  author={Jing, Li and Gulcehre, Caglar and Peurifoy, John and Shen, Yichen and Tegmark, Max and Soljacic, Marin and Bengio, Yoshua},
  journal={Neural Computation},
  volume={31},
  number={4},
  pages={765--783},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{krueger2016zoneout,
  title={Zoneout: Regularizing {R}{N}{N}s by randomly preserving hidden activations},
  author={Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2017}
}

@inproceedings{tallec2018can,
  title={Can Recurrent Neural Networks Warp Time?},
  author={Tallec, Corentin and Ollivier, Yann},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2018}
}

@inproceedings{zhang2018learning,
  title={Learning Long Term Dependencies via {F}ourier Recurrent Units},
  author={Zhang, Jiong and Lin, Yibo and Song, Zhao and Dhillon, Inderjit S},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2018}
}

@inproceedings{chandar2019towards,
  title={Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies},
  author={Chandar, Sarath and Sankar, Chinnadhurai and Vorontsov, Eugene and Kahou, Samira Ebrahimi and Bengio, Yoshua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3280--3287},
  year={2019}
}

@inproceedings{gu2020improving,
  title={Improving the Gating Mechanism of Recurrent Neural Networks},
  author={Gu, Albert and Gulcehre, Caglar and Paine, Tom Le and Hoffman, Matt and Pascanu, Razvan},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2020},
}

@inproceedings{rusch2021coupled,
  title={Coupled Oscillatory Recurrent Neural Network (coRNN): An Accurate and (Gradient) Stable Architecture for Learning Long Time Dependencies},
  author={Rusch, T. Konstantin and Mishra, Siddhartha},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{chang2019antisymmetricrnn,
  title={Antisymmetric{R}{N}{N}: A Dynamical System View on Recurrent Neural Networks},
  author={Chang, Bo and Chen, Minmin and Haber, Eldad and Chi, Ed H},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{kag2020rnns,
  title={R{n}Ns Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?},
  author={Kag, Anil and Zhang, Ziming and Saligrama, Venkatesh},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{erichson2021lipschitz,
  title={Lipschitz Recurrent Neural Networks},
  author={Erichson, N Benjamin and Azencot, Omri and Queiruga, Alejandro and Hodgkinson, Liam and Mahoney, Michael W},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{lei2021attention,
  title={When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute},
  author={Lei, Tao},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7633--7648},
  year={2021}
}


%% Orthogonal RNNs
@inproceedings{arjovsky2016unitary,
  title={Unitary Evolution Recurrent Neural Networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={1120--1128},
  year={2016}
}

@inproceedings{henaff2016recurrent,
  title={Recurrent Orthogonal Networks and Long-Memory Tasks},
  author={Henaff, Mikael and Szlam, Arthur and LeCun, Yann},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2016}
}

@inproceedings{mhammedi2017efficient,
  title={Efficient Orthogonal Parametrisation of Recurrent Neural Networks using {H}ouseholder Reflections},
  author={Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
  booktitle={International Conference on Machine Learning},
  pages={2401--2409},
  year={2017},
  organization={PMLR}
}

@inproceedings{vorontsov2017orthogonality,
  title={On Orthogonality and Learning Recurrent Networks with Long Term Dependencies},
  author={Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
  booktitle={International Conference on Machine Learning},
  pages={3570--3578},
  year={2017},
  organization={PMLR}
}

@inproceedings{lezcano2019cheap,
  title={Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group},
  author={Lezcano-Casado, Mario and Mart{\'i}nez-Rubio, David},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2019}
}

% Imputation methods
@article{che2018recurrent,
  title={Recurrent Neural Networks for Multivariate Time Series with Missing Values},
  author={Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
  journal={Scientific Reports},
  volume={8},
  number={1},
  pages={1--12},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{rubanova2019latent,
  title={Latent Ordinary Differential Equations for Irregularly-Sampled Time Series},
  author={Rubanova, Yulia and Chen, Tian Qi and Duvenaud, David K},
  booktitle={Advances in Neural Information Processing Systems ({NeurIPS})},
  pages={5321--5331},
  year={2019}
}

@inproceedings{de2019gru,
  title={GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series},
  author={De Brouwer, Edward and Simm, Jaak and Arany, Adam and Moreau, Yves},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  year={2019}
}

@article{lechner2020learning,
  title={Learning Long-term Dependencies in Irregularly-sampled Time Series},
  author={Lechner, Mathias and Hasani, Ramin},
  journal={arXiv preprint arXiv:2006.04418},
  year={2020}
}

@article{kidger2020neural,
  title={Neural Controlled Differential Equations for Irregular Time Series},
  author={Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={6696--6707},
  year={2020}
}

@article{morrill2021neural,
  title={Neural Rough Differential Equations for Long Time Series},
  author={Morrill, James and Salvi, Cristopher and Kidger, Patrick and Foster, James and Lyons, Terry},
  journal={The International Conference on Machine Learning (ICML)},
  year={2021}
}


% MNIST baselines
@inproceedings{li2018independently,
  IDS={indrnn},
  title={Independently Recurrent Neural Network ({Ind{R}{N}{N}}): Building a Longer and Deeper {R}{N}{N}},
  author={Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5457--5466},
  year={2018}
}

@inproceedings{trinh2018learning,
  title={Learning Longer-term Dependencies in {R}{N}{N}s with Auxiliary Losses},
  author={Trinh, Trieu H and Dai, Andrew M and Luong, Minh-Thang and Le, Quoc V},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2018}
}

@article{bai2018empirical,
  title={An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@inproceedings{trellisnet,
  title={Trellis Networks for Sequence Modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2019}
}

%%% Attention and variants

@inproceedings{vaswani2017attention,
title={Attention Is All You Need},
author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
year={2017},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
}

@inproceedings{dai2019transformer,
  title={Transformer-{X}{L}: Attentive Language Models Beyond a Fixed-length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@article{child2019generating,
  title={Generating Long Sequences with Sparse {T}ransformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{wu2019pay,
  title={Pay Less Attention with Lightweight and Dynamic Convolutions},
  author={Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N and Auli, Michael},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2019}
}

@inproceedings{sukhbaatar2019adaptive,
  title={Adaptive Attention Span in {T}ransformers},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@inproceedings{rae2019compressive,
  title={Compressive {T}ransformers for Long-Range Sequence Modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2020}
}

@inproceedings{kitaev2020reformer,
  title={Reformer: The Efficient {T}ransformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2020}
}

@article{roy2020efficient,
  title={Efficient Content-Based Sparse Attention with Routing {T}ransformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={arXiv preprint arXiv:2003.05997},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{rahimi2007random,
  title={Random Features for Large-Scale Kernel Machines},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={20},
  year={2007}
}

@inproceedings{peng2021random,
  title={Random Feature Attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A and Kong, Lingpeng},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{choromanski2021rethinking,
  title={Rethinking Attention with Performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-Based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  year={2021}
}

@inproceedings{qin2022cosformer,
  title={CosFormer: Rethinking Softmax in Attention},
  author={Qin, Zhen and Sun, Weixuan and Deng, Hui and Li, Dongxu and Wei, Yunshen and Lv, Baohong and Yan, Junjie and Kong, Lingpeng and Zhong, Yiran},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{qin2022devil,
  title={The devil in linear transformer},
  author={Qin, Zhen and Han, Xiaodong and Sun, Weixuan and Li, Dongxu and Kong, Lingpeng and Barnes, Nick and Zhong, Yiran},
  journal={arXiv preprint arXiv:2210.10340},
  year={2022}
}

@inproceedings{zheng2022linear,
  title={Linear Complexity Randomized Self-Attention Mechanism},
  author={Zheng, Lin and Wang, Chong and Kong, Lingpeng},
  booktitle={International Conference on Machine Learning},
  pages={27011--27041},
  year={2022},
  organization={PMLR}
}

@inproceedings{qin2023toeplitz,
  title={Toeplitz Neural Network for Sequence Modeling},
  author={Qin, Zhen and Han, Xiaodong and Sun, Weixuan and He, Bowen and Li, Dong and Li, Dongxu and Dai, Yuchao and Kong, Lingpeng and Zhong, Yiran},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2023}
}

@article{sun2023retentive,
  title={Retentive Network: A Successor to Transformer for Large Language Models},
  author={Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  journal={arXiv preprint arXiv:2307.08621},
  year={2023}
}

%%% Orthogonal Polynomials, discrete transforms

@book{chihara,
  title={An Introduction to Orthogonal Polynomials},
  author={Chihara, T S},
  isbn={9780486479293},
  lccn={2010043412},
  series={Dover Books on Mathematics},
  year={2011},
  publisher={Dover Publications}
}

@book{trefethen2019approximation,
  title={Approximation Theory and Approximation Practice},
  author={Trefethen, Lloyd N},
  volume={164},
  year={2019},
  publisher={SIAM}
}

@book{szego,
  title={Orthogonal Polynomials},
  author={Szeg{\"o}, G},
  number={v.23},
  isbn={9780821889527},
  lccn={lc39033497},
  series={American Mathematical Society Colloquium Publications},
  year={1967},
  publisher={American Mathematical Society}
}

@book{szego1975orthogonal,
  title={Orthogonal Polynomials},
  author={Szeg{\"{o}}, G},
  isbn={9780821810231},
  lccn={77476087},
  series={American Mathematical Society: Colloquium Publications},
  year={1975},
  publisher={American Mathematical Society}
}

@book{proakis2001digital,
  title={Digital Signal Processing: Principles Algorithms and Applications},
  author={Proakis, John G},
  year={2001},
  publisher={Pearson Education India}
}

@book{korner1989fourier,
  title={{F}ourier Analysis},
  author={K{\"o}rner, Thomas William},
  year={1989},
  publisher={Cambridge University Press}
}

@book{boyd2001chebyshev,
  title={Chebyshev and Fourier Spectral Methods},
  author={Boyd, John P},
  year={2001},
  publisher={Courier Corporation}
}

@book{arfken2005mathematical,
  title={Mathematical Methods for Physicists},
  author={Arfken, George B and Weber, Hans J},
  year={2005},
  publisher={Elsevier Academic Press}
}

% OP Related Work
@inproceedings{defferrard2016convolutional,
  title={Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
  author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={3844--3852},
  year={2016}
}

@inproceedings{dao2017gaussian,
  title = {{G}aussian Quadrature for Kernel Features},
  author = {Dao, Tri and De Sa, Christopher M and R\'{e}, Christopher},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages = {6107--6117},
  year = {2017}
}

@inproceedings{de2018two,
  title={A Two-Pronged Progress in Structured Dense Matrix Vector Multiplication},
  author={De Sa, Christopher and Gu, Albert and Puttagunta, Rohan and R{\'e}, Christopher and Rudra, Atri},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1060--1079},
  year={2018},
  organization={SIAM}
}

@inproceedings{thomas2018learning,
  title={Learning Compressed Transforms with Low Displacement Rank},
  author={Thomas, Anna and Gu, Albert and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={9052--9060},
  year={2018}
}

@inproceedings{dao2019learning,
  title={Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations},
  author={Dao, Tri and Gu, Albert and Eichhorn, Matthew and Rudra, Atri and R{\'e}, Christopher},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2019},
}

@inproceedings{alizadeh2019butterfly,
  title={Butterfly Transform: An Efficient {F}{F}{T} Based Neural Architecture Design},
  author={Alizadeh, Keivan and Farhadi, Ali and Rastegari, Mohammad},
  booktitle = {The Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
}

@inproceedings{dao2020kaleidoscope,
  title={Kaleidoscope: An Efficient, Learnable Representation for All Structured Linear Maps},
  author={Dao, Tri and Sohoni, Nimit and Gu, Albert and Eichhorn, Matthew and Blonder, Amit and Leszczynski, Megan and Rudra, Atri and Ré, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2020}
}

@inproceedings{dao2022monarch,
  title={Monarch: Expressive Structured Matrices for Efficient and Accurate Training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={4690--4721},
  year={2022},
  organization={PMLR}
}

@article{fu2024monarch,
  title={Monarch Mixer: A Simple Sub-quadratic GEMM-based Architecture},
  author={Fu, Dan and Arora, Simran and Grogan, Jessica and Johnson, Isys and Eyuboglu, Evan Sabri and Thomas, Armin and Spector, Benjamin and Poli, Michael and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{yang2019mean,
  title={A Mean Field Theory of Batch Normalization},
  author={Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2019}
}

@article{berthier2020accelerated,
  title={Accelerated Gossip in Networks of Given Dimension Using {J}acobi Polynomial Iterations},
  author={Berthier, Rapha{\"e}l and Bach, Francis and Gaillard, Pierre},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={1},
  pages={24--47},
  year={2020},
  publisher={SIAM}
}

% sliding transforms

@article{chen2015fast,
  title={Fast Computation of Sliding Discrete {T}chebichef Moments and its Application in Duplicated Regions Detection},
  author={Chen, Beijing and Coatrieux, Gouenou and Wu, Jiasong and Dong, Zhifang and Coatrieux, Jean Louis and Shu, Huazhong},
  journal={IEEE Transactions on Signal Processing},
  volume={63},
  number={20},
  pages={5424--5436},
  year={2015},
  publisher={IEEE}
}

@article{farhang1994generalized,
  title={Generalized Sliding {F}{F}{T} and its Application to Implementation of Block {L}{M}{S} Adaptive Filters},
  author={Farhang-Boroujeny, Behrouz and Gazor, Saeed},
  journal={IEEE Transactions on Signal Processing},
  volume={42},
  number={3},
  pages={532--538},
  year={1994},
  publisher={IEEE}
}

@article{jacobsen2003sliding,
  title={The Sliding {D}{F}{T}},
  author={Jacobsen, Eric and Lyons, Richard},
  journal={IEEE Signal Processing Magazine},
  volume={20},
  number={2},
  pages={74--80},
  year={2003},
  publisher={IEEE}
}

@article{jacobsen2004update,
  title={An update to the sliding {DFT}},
  author={Jacobsen, Eric and Lyons, Richard},
  journal={IEEE Signal Processing Magazine},
  volume={21},
  number={1},
  pages={110--111},
  year={2004},
  publisher={IEEE}
}

@article{duda2010accurate,
  title={Accurate, Guaranteed Stable, Sliding Discrete {F}ourier Transform [{D}{S}{P} tips \& tricks]},
  author={Duda, Krzysztof},
  journal={IEEE Signal Processing Magazine},
  volume={27},
  number={6},
  pages={124--127},
  year={2010},
  publisher={IEEE}
}

@article{kober2004fast,
  title={Fast Algorithms for the Computation of Sliding Discrete Sinusoidal Transforms},
  author={Kober, Vitaly},
  journal={IEEE Transactions on Signal Processing},
  volume={52},
  number={6},
  pages={1704--1710},
  year={2004},
  publisher={IEEE}
}

@article{kober2007fast,
  title={Fast Algorithms for the Computation of Sliding Discrete {H}artley Transforms},
  author={Kober, Vitaly},
  journal={IEEE Transactions on Signal Processing},
  volume={55},
  number={6},
  pages={2937--2944},
  year={2007},
  publisher={IEEE}
}

@inproceedings{mozafari2007efficient,
  title={An Efficient Recursive Algorithm and an Explicit Formula for Calculating Update Vectors of Running {W}alsh-{H}adamard Transform},
  author={Mozafari, Barzan and Savoji, Mohammad H},
  booktitle={2007 9th International Symposium on Signal Processing and Its Applications},
  pages={1--4},
  year={2007},
  organization={IEEE}
}
@article{ouyang2009fast,
  title={Fast Algorithm for {W}alsh {H}adamard Transform on Sliding Windows},
  author={Ouyang, Wanli and Cham, Wai-Kuen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={32},
  number={1},
  pages={165--171},
  year={2009},
  publisher={IEEE}
}
@article{wu2012sliding,
  title={Sliding Conjugate Symmetric Sequency-ordered Complex {H}adamard Transform: Fast Algorithm and Applications},
  author={Wu, Jiasong and Wang, Lu and Yang, Guanyu and Senhadji, Lotfi and Luo, Limin and Shu, Huazhong},
  journal={IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume={59},
  number={6},
  pages={1321--1334},
  year={2012},
  publisher={IEEE}
}

@article{macias2005efficient,
  title={Efficient Computation of the Running Discrete {H}aar Transform},
  author={Macias, Jose A Rosendo and Exposito, Antonio Gomez},
  journal={IEEE Transactions on Power Delivery},
  volume={21},
  number={1},
  pages={504--505},
  year={2005},
  publisher={IEEE}
}

% EEG citation
@article{saab2020weak,
  title={Weak Supervision as an Efficient Approach for Automated Seizure Detection in Electroencephalography},
  author={Saab, Khaled and Dunnmon, Jared and R{\'e}, Christopher and Rubin, Daniel and Lee-Messer, Christopher},
  journal={{NPJ} Digital Medicine},
  volume={3},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{shah2018temple,
  title={The {T}emple {U}niversity Hospital Seizure Detection Corpus},
  author={Shah, Vinit and Von Weltin, Eva and Lopez, Silvia and McHugh, James Riley and Veloso, Lillian and Golmohammadi, Meysam and Obeid, Iyad and Picone, Joseph},
  journal={Frontiers in Neuroinformatics},
  volume={12},
  pages={83},
  year={2018},
  publisher={Frontiers}
}

% ODEs and discretization
@book{iserles2009first,
  title={A First Course in the Numerical Analysis of Differential Equations},
  author={Iserles, Arieh},
  number={44},
  year={2009},
  publisher={Cambridge University Press}
}

@inproceedings{dupont2019augmented,
  title={Augmented Neural {O}{D}{E}s},
  author={Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={3134--3144},
  year={2019}
}

@inproceedings{quaglino2020snode,
  title={{S}{N}{O}{D}{E}: Spectral Discretization of Neural {O}{D}{E}s for System Identification},
  author={Quaglino, Alessio and Gallieri, Marco and Masci, Jonathan and Koutn{\'i}k, Jan},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2020}
}

@inproceedings{finlay2020train,
  title={How to Train Your Neural {O}{D}{E}: The World of {J}acobian and Kinetic Regularization},
  author={Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2020},
}

@article{massaroli2020stable,
  title={Stable Neural Flows},
  author={Massaroli, Stefano and Poli, Michael and Bin, Michelangelo and Park, Jinkyoo and Yamashita, Atsushi and Asama, Hajime},
  journal={arXiv preprint arXiv:2003.08063},
  year={2020}
}

@inproceedings{zhang2019approximation,
  title={Approximation Capabilities of Neural Ordinary Differential Equations},
  author={Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom},
  booktitle = {The International Conference on Machine Learning ({ICML})},
  year={2020},
}

@article{zhang2007performance,
  title={Performance Recovery in Digital Implementation of Analogue Systems},
  author={Zhang, Guofeng and Chen, Tongwen and Chen, Xiang},
  journal={{SIAM} Journal on Control and Optimization},
  volume={45},
  number={6},
  pages={2207--2223},
  year={2007},
  publisher={SIAM}
}

@book{decarlo1989linear,
  title={Linear Systems: A State Variable Approach with Numerical Implementation},
  author={DeCarlo, Raymond A},
  year={1989},
  publisher={Prentice-Hall, Inc.}
}

@article{funahashi1993approximation,
  title={Approximation of Dynamical Systems by Continuous Time Recurrent Neural Networks},
  author={Funahashi, Ken-ichi and Nakamura, Yuichi},
  journal={Neural Networks},
  volume={6},
  number={6},
  pages={801--806},
  year={1993},
  publisher={Elsevier}
}

@article{zhang2014comprehensive,
  title={A Comprehensive Review of Stability Analysis of Continuous-time Recurrent Neural Networks},
  author={Zhang, Huaguang and Wang, Zhanshan and Liu, Derong},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={25},
  number={7},
  pages={1229--1262},
  year={2014},
  publisher={IEEE}
}

@article{niu2019recurrent,
  title={Recurrent Neural Networks in the Eye of Differential Equations},
  author={Niu, Murphy Yuezhen and Horesh, Lior and Chuang, Isaac},
  journal={arXiv preprint arXiv:1904.12933},
  year={2019}
}

@article{jordan2021gated,
  title={Gated Recurrent Units Viewed Through the Lens of Continuous Time Dynamical Systems},
  author={Jordan, Ian D and Sok{\'o}{\l}, Piotr Aleksander and Park, Il Memming},
  journal={Frontiers in Computational Neuroscience},
  pages={67},
  year={2021},
  publisher={Frontiers}
}

@article{friston2003dynamic,
  title={Dynamic Causal Modelling},
  author={Friston, Karl J and Harrison, Lee and Penny, Will},
  journal={Neuroimage},
  volume={19},
  number={4},
  pages={1273--1302},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{hasani2021liquid,
  title={Liquid Time-constant Networks},
  author={Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}

@article{hasani2021closed,
  title={Closed-form Continuous-Depth Models},
  author={Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Liebenwein, Lucas and Tschaikowski, Max and Teschl, Gerald and Rus, Daniela},
  journal={Nature Machine Intelligence},
  year={2022}
}

%%% Datasets

@article{bagnall2018uea,
  title={The {U}{E}{A} Multivariate Time Series Classification Archive, 2018},
  author={Bagnall, Anthony and Dau, Hoang Anh and Lines, Jason and Flynn, Michael and Large, James and Bostrom, Aaron and Southam, Paul and Keogh, Eamonn},
  journal={arXiv preprint arXiv:1811.00075},
  year={2018}
}

@misc{Dua:2019,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@inproceedings{kingma2014adam,
  IDS={adam},
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2015}
}

@inproceedings{loshchilov2017decoupled,
  IDS={adamw},
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2019}
}

@inproceedings{you2019large,
  IDS={lamb},
  title={Large Batch Optimization for Deep Learning: Training {B}{E}{R}{T} in 76 Minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2020}
}

@article{mackey1977oscillation,
  title={Oscillation and Chaos in Physiological Control Systems},
  author={Mackey, Michael C and Glass, Leon},
  journal={Science},
  volume={197},
  number={4300},
  pages={287--289},
  year={1977},
  publisher={American Association for the Advancement of Science}
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}



@article{
  Tan2020TSER,
  title={Time Series Extrinsic Regression}, 
  author={Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I},
  journal={Data Mining and Knowledge Discovery},
  pages={1--29},
  year={2021},
  publisher={Springer},
  doi={https://doi.org/10.1007/s10618-021-00745-9}
}

@book{golub2013matrix,
  title={Matrix Computations},
  author={Golub, Gene H and Van Loan, Charles F},
  volume={3},
  year={2013},
  publisher={JHU press}
}

@article{van2021deep,
  title={Deep Learning in Histopathology: The Path to the Clinic},
  author={van der Laak, Jeroen and Litjens, Geert and Ciompi, Francesco},
  journal={Nature Medicine},
  volume={27},
  number={5},
  pages={775--784},
  year={2021},
  publisher={Nature Publishing Group}
}

@book{shoup2009computational,
  title={A Computational Introduction to Number Theory and Algebra},
  author={Shoup, Victor},
  year={2009},
  publisher={Cambridge university press}
}

@inproceedings{ravanelli2019pytorch,
  IDS={pytorch-kaldi},
  title={The Pytorch-Kaldi Speech Recognition Toolkit},
  author={Ravanelli, Mirco and Parcollet, Titouan and Bengio, Yoshua},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6465--6469},
  year={2019},
  organization={IEEE}
}

@book{williams2007linear,
  title={Linear State-Space Control Systems},
  author={Williams, Robert L and Lawrence, Douglas A and others},
  year={2007},
  publisher={Wiley Online Library}
}

@inproceedings{dauphin2017language,
  title={Language Modeling with Gated Convolutional Networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@article{shazeer2020glu,
  title={G{L}{U} Variants Improve {T}ransformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{rae2018fast,
  title={Fast Parametric Learning with Activation Memorization},
  author={Rae, Jack and Dyer, Chris and Dayan, Peter and Lillicrap, Timothy},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2018}
}

@article{baevski2019adaptive,
  title={Adaptive Input Representations for Neural Language Modeling},
  author={Baevski, Alexei and Auli, Michael},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2019}
}

@inproceedings{lioutas2020time,
  title={Time-aware Large Kernel Convolutions},
  author={Lioutas, Vasileios and Guo, Yuhong},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={6172--6183},
  year={2020},
  organization={PMLR}
}

@article{lutati2023focus,
  title={Focus Your Attention (with Adaptive IIR Filters)},
  author={Lutati, Shahar and Zimerman, Itamar and Wolf, Lior},
  journal={arXiv preprint arXiv:2305.14952},
  year={2023}
}

@inproceedings{dosovitskiy2021image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{tolstikhin2021mlp,
  title={MLP-Mixer: An All-MLP Architecture for Vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@article{lee2021fnet,
  title={FNet: Mixing Tokens with Fourier Transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}

%%% Generation baselines
@article{van2016conditional,
  title={Conditional Image Generation with Pixel{C}{N}{N} Decoders},
  author={Van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={29},
  year={2016}
}

@inproceedings{van2016pixel,
  title={Pixel Recurrent Neural Networks},
  author={Van Den Oord, A{\"a}ron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1747--1756},
  year={2016},
  organization={PMLR}
}

@article{salimans2017pixelcnn++,
  title={PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications},
  author={Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1701.05517},
  year={2017}
}

@article{ramachandran2017fast,
  title={Fast Generation for Convolutional Autoregressive Models},
  author={Ramachandran, Prajit and Paine, Tom Le and Khorrami, Pooya and Babaeizadeh, Mohammad and Chang, Shiyu and Zhang, Yang and Hasegawa-Johnson, Mark A and Campbell, Roy H and Huang, Thomas S},
  journal={arXiv preprint arXiv:1704.06001},
  year={2017}
}

@inproceedings{parmar2018image,
  title={Image {T}ransformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International Conference on Machine Learning},
  pages={4055--4064},
  year={2018},
  organization={PMLR}
}

@inproceedings{peebles2023scalable,
  title={Scalable Diffusion Models with Transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4195--4205},
  year={2023}
}

@inproceedings{liu2015deep,
  title={Deep Learning Face Attributes in the Wild},
  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3730--3738},
  year={2015}
}

%%% Atri OP citations

@book{ArfkenGeorgeB.GeorgeBrown2013Mmfp,
publisher = {Academic Press},
booktitle = {Mathematical Methods for Physicists : A Comprehensive Guide},
isbn = {0-12-384654-4},
year = {2013},
title = {Mathematical Methods for Physicists : A Comprehensive Guide / George B. Arfken, Hans J. Weber, Frank E. Harris},
edition = {7th ed.},
language = {eng},
address = {Amsterdam},
author = {Arfken, George B. (George Brown) and Weber, Hans Jürgen and Harris, Frank E},
keywords = {Mathematics; Mathematical physics; Mathematics; Physical Sciences & Mathematics; Mathematics - General},
}

@Inbook{Shen2011,
author="Shen, Jie
and Tang, Tao
and Wang, Li-Lian",
title="Orthogonal Polynomials and Related Approximation Results",
bookTitle="Spectral Methods: Algorithms, Analysis and Applications",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="47--140",
abstract="The Fourier spectral method is only appropriate for problems with periodic boundary conditions. If a Fourier method is applied to a non-periodic problem, it inevitably induces the so-called Gibbs phenomenon, and reduces the global convergence rate to O(N-1) (cf. Gottlieb and Orszag (1977)). Consequently, one should not apply a Fourier method to problems with non-periodic boundary conditions. Instead, one should use orthogonal polynomials which are eigenfunctions of some singular Sturm-Liouville problems. The commonly used orthogonal polynomials include the Legendre, Chebyshev, Hermite and Laguerre polynomials.",
isbn="978-3-540-71041-7",
doi="10.1007/978-3-540-71041-7_3",
url="https://doi.org/10.1007/978-3-540-71041-7_3"
}

@article{woodbury1950,
  title={Inverting Modified Matrices},
  author={Woodbury, Max A},
  journal={Memorandum report},
  volume={42},
  pages={106},
  year={1950},
  publisher={Statistical Research Group, Princeton University}
}

@book{butcher2008numerical,
  title={Numerical Methods for Ordinary Differential Equations},
  author={Butcher, John Charles and Goodwin, Nicolette},
  volume={2},
  year={2008},
  publisher={Wiley Online Library}
}

@article{petersen2012matrix,
  title={The Matrix Cookbook, version 20121115},
  author={Petersen, KB and Pedersen, MS},
  journal={Technical Univ. Denmark, Kongens Lyngby, Denmark, Tech. Rep},
  volume={3274},
  year={2012}
}

%%% Pan structured matrices

@book{pan2001structured,
  title={Structured Matrices and Polynomials: Unified Superfast Algorithms},
  author={Pan, Victor},
  year={2001},
  publisher={Springer Science \& Business Media}
}

@article{pan2015transformations,
  title={Transformations of Matrix Structures Work Again},
  author={Pan, Victor Y},
  journal={Linear Algebra and Its Applications},
  volume={465},
  pages={107--138},
  year={2015},
  publisher={Elsevier}
}

@article{pan2016bad,
  title={How Bad are {V}andermonde Matrices?},
  author={Pan, Victor Y},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={37},
  number={2},
  pages={676--694},
  year={2016},
  publisher={SIAM}
}

@article{pan2017fast,
  title={Fast Approximate Computations with {C}auchy Matrices and Polynomials},
  author={Pan, Victor},
  journal={Mathematics of Computation},
  volume={86},
  number={308},
  pages={2799--2826},
  year={2017}
}

%%% Other structured matrices
@article{eidelman1999new,
  title={On a new class of structured matrices},
  author={Eidelman, Yuli and Gohberg, Israel},
  journal={Integral Equations and Operator Theory},
  volume={34},
  number={3},
  pages={293--324},
  year={1999},
  publisher={Springer}
}

@article{vandebril2005bibliography,
  title={A bibliography on semiseparable matrices},
  author={Vandebril, Raf and Barel, M Van and Golub, Gene and Mastronardi, Nicola},
  journal={Calcolo},
  volume={42},
  pages={249--270},
  year={2005},
  publisher={Springer}
}

@inproceedings{pernet2016computing,
  title={Computing with Quasiseparable Matrices},
  author={Pernet, Cl{\'e}ment},
  booktitle={Proceedings of the ACM on International Symposium on Symbolic and Algebraic Computation},
  pages={389--396},
  year={2016}
}

@article{pernet2018time,
  title={Time and Space Efficient Generators for Quasiseparable Matrices},
  author={Pernet, Cl{\'e}ment and Storjohann, Arne},
  journal={Journal of Symbolic Computation},
  volume={85},
  pages={224--246},
  year={2018},
  publisher={Elsevier}
}

@article{pernet2023exact,
  title={Exact Computations with Quasiseparable Matrices},
  author={Pernet, Cl{\'e}ment and Signargout, Hippolyte and Villard, Gilles},
  journal={arXiv preprint arXiv:2302.04515},
  year={2023}
}


@inproceedings{tay2021long,
  IDS={lra},
  title={Long {R}ange {A}rena: A Benchmark for Efficient {T}ransformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021},
}

@article{tustin1947method,
  title={A Method of Analysing the Behaviour of Linear Systems in Terms of Time Series},
  author={Tustin, Arnold},
  journal={Journal of the Institution of Electrical Engineers-Part IIA: Automatic Regulators and Servo Mechanisms},
  volume={94},
  number={1},
  pages={130--142},
  year={1947},
  publisher={IET}
}

@article{Warden2018SpeechCA,
  title={Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},
  author={Pete Warden},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.03209}
}

@inproceedings{haoyietal-informer-2021,
  IDS={informer},
  author    = {Haoyi Zhou and
               Shanghang Zhang and
               Jieqi Peng and
               Shuai Zhang and
               Jianxin Li and
               Hui Xiong and
               Wancai Zhang},
  title     = {Informer: Beyond Efficient {T}ransformer for Long Sequence Time-Series Forecasting},
  booktitle = {The Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021, Virtual Conference},
  volume    = {35},
  number    = {12},
  pages     = {11106--11115},
  publisher = {{AAAI} Press},
  year      = {2021},
}

%%% SSM literature

@article{kalman1960new,
  title={A New Approach to Linear Filtering and Prediction Problems},
  author={Kalman, Rudolph Emil},
  year={1960}
}

%%% S4 precursors 

% LMU

@article{voelker2018improving,
  title={Improving Spiking Dynamical Networks: Accurate Delays, Higher-order Synapses, and Time Cells},
  author={Voelker, Aaron R and Eliasmith, Chris},
  journal={Neural Computation},
  volume={30},
  number={3},
  pages={569--609},
  year={2018},
  publisher={MIT Press}
}

@phdthesis{voelker2019dynamical,
  title={Dynamical Systems in Spiking Neuromorphic Hardware},
  author={Voelker, Aaron R},
  year={2019},
  school={University of Waterloo}
}

@inproceedings{voelker2019legendre,
  title={Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks},
  author={Voelker, Aaron R and Kaji{\'c}, Ivana and Eliasmith, Chris},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={15544--15553},
  year={2019}
}

@inproceedings{chen2018neural,
  title={Neural Ordinary Differential Equations},
  author={Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={6571--6583},
  year={2018}
}

@article{chilkuri2021parallelizing,
  title={Parallelizing {L}egendre Memory Unit Training},
  author={Chilkuri, Narsimha and Eliasmith, Chris},
  journal={The International Conference on Machine Learning (ICML)},
  year={2021}
}

@article{rusch2021unicornn,
  title={Un{I}{C}{O}{R}{N}{N}: A Recurrent Model for Learning Very Long Time Dependencies},
  author={Rusch, T Konstantin and Mishra, Siddhartha},
  journal={The International Conference on Machine Learning (ICML)},
  year={2021}
}

% CKConv

@article{romero2021ckconv,
  IDS={ckconv},
  title={C{K}{C}onv: Continuous Kernel Convolution For Sequential Data},
  author={Romero, David W and Kuzina, Anna and Bekkers, Erik J and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal={arXiv preprint arXiv:2102.02611},
  year={2021}
}

@inproceedings{romero2022flexconv,
  title={Flex{C}onv: Continuous Kernel Convolutions with Differentiable Kernel Sizes},
  author={Romero, David W and Bruintjes, Robert-Jan and Tomczak, Jakub M and Bekkers, Erik J and Hoogendoorn, Mark and van Gemert, Jan C},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2022}
}

@inproceedings{romero2023modelling,
  IDS={ccnn},
  title={Modelling Long Range Dependencies in $N${D}: From Task-Specific to a General Purpose {C}{N}{N}},
  author={Romero, David W and Knigge, David M and Gu, Albert and Bekkers, Erik J and Gavves, Efstratios and Tomczak, Jakub M and Hoogendoorn, Mark},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2023}
}


%%% S4 papers

@phdthesis{gu2023thesis,
  title={Modeling Sequences with Structured State Spaces},
  author={Gu, Albert},
  year={2023},
  school={Stanford University},
  type         = {PhD thesis},
}

@inproceedings{gu2020hippo,
  IDS={hippo},
  title     = {HIPPO: Recurrent Memory with Optimal Polynomial Projections},
  author    = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'{e}}, Christopher},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2020}
}

@inproceedings{gu2021combining,
  IDS={lssl},
  title = {Combining Recurrent, Convolutional, and Continuous-time Models with the Linear State Space Layer},
  author = {Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and Ré, Christopher},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}

@inproceedings{gu2022efficiently,
  IDS={s4},
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2022}
}

@inproceedings{goel2022raw,
  IDS={sashimi},
  title={It's Raw! Audio Generation with State-Space Models},
  author={Goel, Karan and Gu, Albert and Donahue, Chris and R{\'e}, Christopher},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2022}
}

@article{gupta2022diagonal,
  title={Diagonal State Spaces are as Effective as Structured State Spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22982--22994},
  year={2022}
}

@inproceedings{gu2022parameterization,
  IDS={s4d},
  title={On the Parameterization and Initialization of Diagonal State Space Models},
  author={Gu, Albert and Gupta, Ankit and Goel, Karan and R\'e, Christopher},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@inproceedings{gu2023train,
  IDS={httyh},
  title={How to Train Your HIPPO: State Space Models with Generalized Basis Projections},
  author={Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and R\'e, Christopher},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2023}
}

@inproceedings{nguyen2022s4nd,
  IDS={s4nd},
  title={S4{N}{D}: Modeling Images and Videos as Multidimensional Signals with State Spaces},
  author={Nguyen, Eric and Goel, Karan and Gu, Albert and Downs, Gordon and Shah, Preey and Dao, Tri and Baccus, Stephen and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{gu2023mamba,
  IDS={mamba},
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{gu2024mamba,
  IDS={mamba},
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  booktitle={Conference on Language Modeling (COLM)},
  year={2024}
}

@inproceedings{dao2024transformers,
  IDS={mamba2},
  title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={Forty-first International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{waleffe2024empirical,
  title={An Empirical Study of Mamba-based Language Models},
  author={Waleffe, Roger and Byeon, Wonmin and Riach, Duncan and Norick, Brandon and Korthikanti, Vijay and Dao, Tri and Gu, Albert and Hatamizadeh, Ali and Singh, Sudhakar and Narayanan, Deepak and others},
  journal={arXiv preprint arXiv:2406.07887},
  year={2024}
}

%%% S4 applications and literature
@inproceedings{mehta2023long,
  IDS={gss},
  title={Long Range Language Modeling via Gated State Spaces},
  author={Mehta, Harsh and Gupta, Ankit and Cutkosky, Ashok and Neyshabur, Behnam},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2023}
}

@inproceedings{smith2023s5,
  IDS={s5},
  title={Simplified State Space Layers for Sequence Modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2023}
}

@inproceedings{li2023makes,
  IDS={sgconv},
  title={What Makes Convolutional Models Great on Long Sequence Modeling?},
  author={Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2023}
}

@inproceedings{hasani2023liquid,
  IDS={liquids4},
  title={Liquid Structural State-Space Models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2023}
}

@inproceedings{ma2023mega,
  IDS={mega},
  title={Mega: Moving Average Equipped Gated Attention},
  author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2023}
}

@inproceedings{dao2023hungry,
  IDS={h3},
  title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  author={Dao, Tri and Fu, Daniel Y and Saab, Khaled K and Thomas, Armin W and Rudra, Atri and R{\'e}, Christopher},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2023}
}

@inproceedings{zhang2023effectively,
  IDS={spacetime},
  title={Effectively Modeling Time Series with Simple Discrete State Spaces},
  author={Zhang, Michael and Saab, Khaled K and Poli, Michael and Dao, Tri and Goel, Karan and R{\'e}, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2023}
}

@article{wang2022pretraining,
  IDS={bigs},
  title={Pretraining Without Attention},
  author={Wang, Junxiong and Yan, Jing Nathan and Gu, Albert and Rush, Alexander M},
  journal={arXiv preprint arXiv:2212.10544},
  year={2022}
}

@article{fu2023simple,
  title={Simple Hardware-efficient Long Convolutions for Sequence Modeling},
  author={Fu, Daniel Y and Epstein, Elliot L and Nguyen, Eric and Thomas, Armin W and Zhang, Michael and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={The International Conference on Machine Learning (ICML)},
  year={2023}
}

@inproceedings{shi2023sequence,
  title={Sequence Modeling with Multiresolution Convolutional Memory},
  author={Shi, Jiaxin and Wang, Ke Alexander and Fox, Emily},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={31312--31327},
  year={2023},
  organization={PMLR}
}

@inproceedings{poli2023hyena,
  IDS={hyena},
  title={Hyena Hierarchy: Towards Larger Convolutional Language Models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2023}
}

@inproceedings{nguyen2023hyenadna,
  IDS={hyenadna},
  title={Hyena{D}{N}{A}: Long-range Genomic Sequence Modeling at Single Nucleotide Resolution},
  author={Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

@inproceedings{lu2023structured,
  title={Structured State Space Models for In-Context Reinforcement Learning},
  author={Lu, Chris and Schroecker, Yannick and Gu, Albert and Parisotto, Emilio and Foerster, Jakob and Singh, Satinder and Behbahani, Feryal},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023}
}

% TODO: spade from MSR


@inproceedings{fathullah2023multi,
  IDS={mhssm},
  title={{Multi-Head State Space Model for Speech Recognition}},
  author={Yassir Fathullah and Chunyang Wu and Yuan Shangguan and Junteng Jia and Wenhan Xiong and Jay Mahadeokar and Chunxi Liu and Yangyang Shi and Ozlem Kalinli and Mike Seltzer and Mark J. F. Gales},
  year={2023},
  booktitle={Proc. INTERSPEECH 2023},
  pages={241--245},
  doi={10.21437/Interspeech.2023-1036},
  issn={2308-457X}
}


@article{zuo2022efficient,
  IDS={spade},
  title={Efficient Long Sequence Modeling via State Space Augmented {T}ransformer},
  author={Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Charles, Denis and Manavoglu, Eren and Zhao, Tuo and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2212.08136},
  year={2022}
}

@inproceedings{saon2023diagonal,
  IDS={dssformer},
  title={Diagonal State Space Augmented {T}ransformers for Speech Recognition},
  author={Saon, George and Gupta, Ankit and Cui, Xiaodong},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{tang2023modeling,
  title={Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models},
  author={Tang, Siyi and Dunnmon, Jared A and Liangqiong, Qu and Saab, Khaled K and Baykaner, Tina and Lee-Messer, Christopher and Rubin, Daniel L},
  booktitle={Conference on Health, Inference, and Learning},
  pages={50--71},
  year={2023},
  organization={PMLR}
}

@inproceedings{zhou2023deep,
  title={Deep Latent State Space Models for Time-Series Generation},
  author={Zhou, Linqi and Poli, Michael and Xu, Winnie and Massaroli, Stefano and Ermon, Stefano},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2023}
}

@article{alcaraz2023diffusion,
  title={Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models},
  author={Alcaraz, Juan Lopez and Strodthoff, Nils},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{islam2022long,
  IDS={vis4mer},
  title={Long Movie Clip Classification with State-Space Video Models},
  author={Islam, Md Mohaiminul and Bertasius, Gedas},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV},
  pages={87--104},
  year={2022},
  organization={Springer}
}

@inproceedings{david2023decision,
  title={Decision {S}4: Efficient Sequence-Based {R}{L} via State Spaces Layers},
  author={David, Shmuel Bar and Zimerman, Itamar and Nachmani, Eliya and Wolf, Lior},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{fathi2023block,
  title={Block-{S}tate {T}ransformer},
  author={Fathi, Mahan and Pilault, Jonathan and Bacon, Pierre-Luc and Pal, Christopher and Firat, Orhan and Goroshin, Ross},
  journal={arXiv preprint arXiv:2306.09539},
  year={2023}
}

@inproceedings{zeng2023transformers,
  title={Are {T}ransformers Effective for Time Series Forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={11121--11128},
  year={2023}
}

@book{box2015time,
  title={Time Series Analysis: Forecasting and Control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}

@article{wang2023state,
  title={State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory},
  author={Wang, Shida and Xue, Beichen},
  journal={arXiv preprint arXiv:2309.13414},
  year={2023}
}

% Normalization stuff

@inproceedings{ioffe2015batch,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{ba2016layer,
  title={Layer Normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{he2015delving,
  title={Delving Deep into Rectifiers: Surpassing Human-level Performance on {I}mage{N}et Classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{glorot2010understanding,
  title={Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{liu2020understanding,
  title={Understanding the Difficulty of Training {T}ransformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={The International Conference on Machine Learning (ICML)},
  year={2020}
}

@inproceedings{davis2021catformer,
  title={Catformer: Designing Stable {T}ransformers via Sensitivity Analysis},
  author={Davis, Jared Quincy and Gu, Albert and Dao, Tri and Choromanski, Krzysztof and R\'e, Christopher and Liang, Percy and Finn, Chelsea},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2021}
}

@book{ralston2001first,
  title={A First Course in Numerical Analysis},
  author={Ralston, Anthony and Rabinowitz, Philip},
  year={2001},
  publisher={Courier Corporation}
}

@inproceedings{nonaka2021depth,
  title={In-depth Benchmarking of Deep Neural Network Architectures for {E}{C}{G} Diagnosis},
  author={Nonaka, Naoki and Seita, Jun},
  booktitle={Machine Learning for Healthcare Conference},
  pages={414--439},
  year={2021},
  organization={PMLR}
}

%%% More miscellaneous uncategorized citations from thesis
@book{baker1996pade,
  title={Pade Approximants: Encyclopedia of Mathematics and It's Applications, Vol. 59 George A. Baker, Jr., Peter Graves-Morris},
  author={Baker, George A and Baker Jr, George A and Graves-Morris, Peter and Baker, Susan S},
  volume={59},
  year={1996},
  publisher={Cambridge University Press}
}

@inproceedings{orvieto2023resurrecting,
  IDS={lru},
  title={Resurrecting Recurrent Neural Networks for Long Sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2023}
}

%%% Some intro citations for sequence models

@article{brown2020language,
  title={Language Models are Few-shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ismail2019deep,
  title={Deep Learning for Time Series Classification: A Review},
  author={Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  number={4},
  pages={917--963},
  year={2019},
  publisher={Springer}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General Perception with Iterative Attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle={International Conference on Machine Learning},
  pages={4651--4664},
  year={2021},
  organization={PMLR}
}

@article{sutskever2014sequence,
  title={Sequence to Sequence Learning with Neural Networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={27},
  year={2014}
}

@inproceedings{bahdanau2015neural,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2015}
}

@inproceedings{cohen2016group,
  title={Group Equivariant Convolutional Networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={2990--2999},
  year={2016},
  organization={PMLR}
}

@article{kidger2022neural,
  title={On Neural Differential Equations},
  author={Kidger, Patrick},
  journal={arXiv preprint arXiv:2202.02435},
  year={2022}
}

@article{tay2022efficient,
  title={Efficient {T}ransformers: A Survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--28},
  year={2022},
  publisher={ACM New York, NY}
}

@article{zhai2021attention,
  IDS={aft},
  title={An Attention Free {T}ransformer},
  author={Zhai, Shuangfei and Talbott, Walter and Srivastava, Nitish and Huang, Chen and Goh, Hanlin and Zhang, Ruixiang and Susskind, Josh},
  journal={arXiv preprint arXiv:2105.14103},
  year={2021}
}

@inproceedings{peng2023rwkv,
  IDS={rwkv},
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin Gregory and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael Nguyen and Derczynski, Leon and others},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023}
}

@inproceedings{peng2024eagle,
  title={Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin Gregory and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Ferdinan, Teddy and GV, Kranthi Kiran and Hou, Haowen and others},
  booktitle={First Conference on Language Modeling (COLM)},
  year={2024}
}

@article{peng2025rwkv,
  title={RWKV-7 "Goose" with Expressive Dynamic State Evolution},
  author={Peng, Bo and Zhang, Ruichong and Goldstein, Daniel and Alcaide, Eric and Du, Xingjian and Hou, Haowen and Lin, Jiaju and Liu, Jiaxing and Lu, Janna and Merrill, William and others},
  journal={arXiv preprint arXiv:2503.14456},
  year={2025}
}

@inproceedings{beck2024xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={Advances in Neural Information Processing Systems ({NeurIPS})},
  year={2024}
}

@article{hendrycks2016gaussian,
  IDS={gelu},
  title={Gaussian Error Linear Units ({G}{E}{L}{U}s)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{ramachandran2017swish,
  title={Swish: A Self-gated Activation Function},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  volume={7},
  number={1},
  pages={5},
  year={2017},
  publisher={Technical report}
}

@inproceedings{ronneberger2015u,
  title={U-{N}et: Convolutional Networks for Biomedical Image Segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{paszke2019pytorch,
  title={Py{T}orch: An Imperative Style, High-performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019}
}

@article{gpt4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/arXiv.2303.08774},
  eprinttype    = {arXiv},
  eprint       = {2303.08774},
  timestamp    = {Mon, 20 Mar 2023 15:23:19 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ahmad2022new,
  title={A New Generative Adversarial Network for Medical Images Super Resolution},
  author={Ahmad, Waqar and Ali, Hazrat and Shah, Zubair and Azmat, Shoaib},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={9533},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{kapturowski2019recurrent,
  title={Recurrent Experience Replay in Distributed Reinforcement Learning},
  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{hooker2021hardware,
  title={The Hardware Lottery},
  author={Hooker, Sara},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={58--65},
  year={2021},
  publisher={ACM New York, NY, USA}
}



%%% State space model literature
@article{teh2004sharing,
  title={Sharing Clusters Among Related Groups: Hierarchical {D}irichlet Processes},
  author={Teh, Yee and Jordan, Michael and Beal, Matthew and Blei, David},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={17},
  year={2004}
}

@article{fox2011sticky,
  title={A Sticky {H}{D}{P}-{H}{M}{M} with Application to Speaker Diarization},
  author={Fox, Emily B and Sudderth, Erik B and Jordan, Michael I and Willsky, Alan S},
  journal={The Annals of Applied Statistics},
  pages={1020--1056},
  year={2011},
  publisher={JSTOR}
}

@article{johnson2013bayesian,
  title={Bayesian Nonparametric Hidden Semi-{M}arkov Models},
  author={Johnson, Matthew James and Willsky, Alan S},
  year={2013},
  publisher={Association for Computing Machinery (ACM)}
}


%%% SLDS

@article{chang1978state,
  title={State Estimation for Discrete Systems with Switching Parameters},
  author={Chang, Chaw-Bing and Athans, Michael},
  journal={IEEE Transactions on Aerospace and Electronic Systems},
  number={3},
  pages={418--425},
  year={1978},
  publisher={IEEE}
}

@article{murphy1998switching,
  title={Switching {K}alman Filters},
  author={Murphy, Kevin P},
  year={1998},
  publisher={Citeseer}
}

@article{ghahramani2000variational,
  title={Variational Learning for Switching State-Space Models},
  author={Ghahramani, Zoubin and Hinton, Geoffrey E},
  journal={Neural Computation},
  volume={12},
  number={4},
  pages={831--864},
  year={2000},
  publisher={MIT Press}
}

@article{fox2008nonparametric,
  title={Nonparametric {B}ayesian Learning of Switching Linear Dynamical Systems},
  author={Fox, Emily and Sudderth, Erik and Jordan, Michael and Willsky, Alan},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={21},
  year={2008}
}

@inproceedings{linderman2017bayesian,
  title={Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems},
  author={Linderman, Scott and Johnson, Matthew and Miller, Andrew and Adams, Ryan and Blei, David and Paninski, Liam},
  booktitle={Artificial Intelligence and Statistics},
  pages={914--922},
  year={2017},
  organization={PMLR}
}


%%% Systems
@article{williams2009roofline,
  title={Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  author={Williams, Samuel and Waterman, Andrew and Patterson, David},
  journal={Communications of the ACM},
  volume={52},
  number={4},
  pages={65--76},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  year={2024},
  booktitle={The International Conference on Learning Representations ({ICLR})}
}

@article{ivanov2021data,
  title={Data Movement is All You Need: A Case Study on Optimizing {T}ransformers},
  author={Ivanov, Andrei and Dryden, Nikoli and Ben-Nun, Tal and Li, Shigang and Hoefler, Torsten},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={711--732},
  year={2021}
}

@article{avsec2021effective,
  IDS={enformer},
  title={Effective Gene Expression Prediction from Sequence by Integrating Long-range Interactions},
  author={Avsec, {\v{Z}}iga and Agarwal, Vikram and Visentin, Daniel and Ledsam, Joseph R and Grabska-Barwinska, Agnieszka and Taylor, Kyle R and Assael, Yannis and Jumper, John and Kohli, Pushmeet and Kelley, David R},
  journal={Nature Methods},
  volume={18},
  number={10},
  pages={1196--1203},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@inproceedings{hafner2020dream,
  title={Dream to Control: Learning Behaviors by Latent Imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2020}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@article{krizhevsky2012imagenet,
  title={Image{N}et Classification with Deep Convolutional Neural Networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={25},
  year={2012}
}

@article{blelloch1990prefix,
  title={Prefix Sums and Their Applications},
  author={Blelloch, Guy E},
  year={1990},
  publisher={School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA}
}

@inproceedings{martin2018parallelizing,
  title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
  author={Martin, Eric and Cundy, Chris},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022empirical,
  title={An Empirical Analysis of Compute-Optimal Large Language Model Training},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={35},
  pages={30016--30030},
  year={2022}
}

@article{grattafiori2024llama,
  title={The Llama 3 Herd of Models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% Audio

@misc{deepsound,
  author = {DeepSound},
  title = {SampleRNN},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/deepsound-project/samplernn-pytorch}},
}

@inproceedings{mehri2017samplernn,
  IDS={samplernn},
  title={SampleRNN: An Unconditional End-to-End Neural Audio Generation Model},
  author={Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2017}
}

@inproceedings{donahue2019adversarial,
  IDS={wavegan},
  title={Adversarial Audio Synthesis},
  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{kong2021diffwave,
  title={DiffWave: A Versatile Diffusion Model for Audio Synthesis},
  author={Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

% Other audio

@inproceedings{gulati2020conformer,
  title={Conformer: Convolution-augmented Transformer for Speech Recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  booktitle={Proc. INTERSPEECH 2020},
  year={2020}
}

%%% Misc (uncategorized, added to Mamba paper)

@article{ba2016using,
  title={Using Fast Weights to Attend to the Recent Past},
  author={Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={29},
  year={2016}
}

@article{schmidhuber1992learning,
  title={Learning to control fast-weight memories: An alternative to dynamic recurrent networks},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={4},
  number={1},
  pages={131--139},
  year={1992},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{schlag2021linear,
  title={Linear {T}ransformers are Secretly Fast Weight Programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}

@inproceedings{hua2022transformer,
  IDS={flash,gau},
  title={Transformer Quality in Linear Time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={9099--9117},
  year={2022},
  organization={PMLR}
}

@inproceedings{ha2017hypernetworks,
  IDS={hypernetworks},
  title={Hyper{N}etworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V.},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2017}
}

@article{kosma2023time,
  title={Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series},
  author={Kosma, Chrysoula and Nikolentzos, Giannis and Vazirgiannis, Michalis},
  journal={arXiv preprint arXiv:2308.03210},
  year={2023}
}

@article{yang2019condconv,
  title={Cond{C}onv: Conditionally Parameterized Convolutions for Efficient Inference},
  author={Yang, Brandon and Bender, Gabriel and Le, Quoc V and Ngiam, Jiquan},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019}
}

@inproceedings{wang2023selective,
  title={Selective Structured State-Spaces for Long-form Video Understanding},
  author={Wang, Jue and Zhu, Wentao and Wang, Pichao and Yu, Xiang and Liu, Linda and Omar, Mohamed and Hamid, Raffay},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6387--6397},
  year={2023}
}


@article{elhage2021mathematical,
   title={A Mathematical Framework for {T}ransformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@inproceedings{shi2023large,
  title={Large Language Models can be Easily Distracted by Irrelevant Context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={31210--31227},
  year={2023},
  organization={PMLR}
}


@article{touvron2023llama,
  title={Llama: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{su2021roformer,
  title={Roformer: Enhanced {T}ransformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}


@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A Framework for Few-shot Language Model Evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{pile,
  title={The {P}ile: An 800{G}{B} Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A Suite for Analyzing Large Language Models across Training and Scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={The International Conference on Machine Learning (ICML)},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}


@article{chowdhery2022palm,
  title={Pa{L}{M}: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  year={2023},
  volume={24},
  number={240},
  pages={1--113},
  url={http://jmlr.org/papers/v24/22-1144.html}
}

%%% LM Benchmarks
@inproceedings{paperno2016lambada,
  title={The {L}{A}{M}{B}{A}{D}{A} Dataset: Word Prediction Requiring a Broad Discourse Context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  pages={1525--1534},
  year={2016}
}

@inproceedings{zellers2019hellaswag,
  title={Hella{S}wag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An Adversarial {W}inograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2018think,
  title={Think you have Solved Question Answering? Try {A}{R}{C}, the {A}{I}2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{bisk2020piqa,
  title={P{I}{Q}{A}: Reasoning about Physical Commonsense in Natural Language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on Artificial Intelligence},
  volume={34},
  year={2020}
}

@article{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

% Other long context
@article{ding2023longnet,
  title={Long{N}et: Scaling {T}ransformers to 1,000,000,000 Tokens},
  author={Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Wei, Furu},
  journal={arXiv preprint arXiv:2307.02486},
  year={2023}
}

@article{bulatov2023scaling,
  title={Scaling {T}ransformer to 1{M} tokens and Beyond with {R}{M}{T}},
  author={Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S},
  journal={arXiv preprint arXiv:2304.11062},
  year={2023}
}

@article{katsch2023gateloop,
  title={GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling},
  author={Katsch, Tobias},
  journal={arXiv preprint arXiv:2311.01927},
  year={2023}
}

@inproceedings{yang2024gated,
  IDS={gla},
  title={Gated Linear Attention Transformers with Hardware-Efficient Training},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

@inproceedings{yang2024parallelizing,
  title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author={Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@inproceedings{yang2025gated,
  title={Gated Delta Networks: Improving Mamba2 with Delta Rule},
  author={Yang, Songlin and Kautz, Jan and Hatamizadeh, Ali},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2025}
}

@article{shoeybi2019megatron,
  title={Megatron-{LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{black2022gpt,
  title={Gpt-NeoX-20B: An Open-source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{kaul2020linear,
  title={Linear Dynamical Systems as a Core Computational Primitive},
  author={Kaul, Shiva},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16808--16820},
  year={2020}
}

@article{gupta2022simplifying,
  title={Simplifying and Understanding State Space Models with Diagonal Linear {RNN}s},
  author={Gupta, Ankit and Mehta, Harsh and Berant, Jonathan},
  journal={arXiv preprint arXiv:2212.00768},
  year={2022}
}

@article{qin2023hierarchically,
  IDS={hgrn},
  title={Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
  author={Qin, Zhen and Yang, Songlin and Zhong, Yiran},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{qin2024hgrn2,
  IDS={hgrn2},
  title={HGRN2: Gated Linear RNNs with State Expansion},
  author={Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
  journal={arXiv preprint arXiv:2404.07904},
  year={2024}
}

@article{de2024griffin,
  IDS={griffin},
  title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
  journal={arXiv preprint arXiv:2402.19427},
  year={2024}
}

@article{botev2024recurrentgemma,
  IDS={recurrentgemma},
  title={RecurrentGemma: Moving Past Transformers for Efficient Open Language Models},
  author={Botev, Aleksandar and De, Soham and Smith, Samuel L and Fernando, Anushan and Muraru, George-Cristian and Haroun, Ruba and Berrada, Leonard and Pascanu, Razvan and Sessa, Pier Giuseppe and Dadashi, Robert and others},
  journal={arXiv preprint arXiv:2404.07839},
  year={2024}
}

@misc{ali2024hidden,
  title={The Hidden Attention of Mamba Models}, 
  author={Ameen Ali and Itamar Zimerman and Lior Wolf},
  year={2024},
  eprint={2403.01590},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@inproceedings{darcet2024vision,
  title={Vision Transformers Need Registers},
  author={Darcet, Timoth{\'e}e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2024}
}

@inproceedings{xiao2024efficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2024}
}

@inproceedings{zhang2024hedgehog,
  title={The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry},
  author={Zhang, Michael and Bhatia, Kush and Kumbong, Hermann and R{\'e}, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2024}
}

@inproceedings{arora2024zoology,
  IDS={zoology},
  title={Zoology: Measuring and Improving Recall in Efficient Language Models},
  author={Arora, Simran and Eyuboglu, Sabri and Timalsina, Aman and Johnson, Isys and Poli, Michael and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  booktitle={The International Conference on Learning Representations ({ICLR})},
  year={2024}
}

@inproceedings{arora2024simple,
  IDS={based},
  title={Simple Linear Attention Language Models Balance the Recall-Throughput Tradeoff},
  author={Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{qin2023transnormerllm,
  title={TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Luo, Xiao and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2307.14995},
  year={2023}
}

@article{lieber2024jamba,
  IDS={jamba},
  title={Jamba: A Hybrid Transformer-Mamba Language Model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}

@article{glorioso2024zamba,
  IDS={zamba},
  title={Zamba: A Compact 7B SSM Hybrid Model},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  journal={arXiv preprint arXiv:2405.16712},
  year={2024}
}

@inproceedings{ren2025samba,
  title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling},
  author={Ren, Liliang and Liu, Yang and Lu, Yadong and Liang, Chen and Chen, Weizhu and others},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025},
}

@article{blakeman2025nemotron,
  title={Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models},
  author={Blakeman, Aaron and Basant, Aarti and Khattar, Abhinav and Renduchintala, Adithya and Bercovich, Akhiad and Ficek, Aleksander and Bjorlin, Alexis and Taghibakhshi, Ali and Deshmukh, Amala Sanjay and Mahabaleshwarkar, Ameya Sunil and others},
  journal={arXiv preprint arXiv:2504.03624},
  year={2025}
}

@article{liu2025hunyuan,
  title={Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought},
  author={Liu, Ao and Zhou, Botong and Xu, Can and Zhou, Chayse and Zhang, ChenChen and Xu, Chengcheng and Wang, Chenhao and Wu, Decheng and Wu, Dengpeng and Jiao, Dian and others},
  journal={arXiv preprint arXiv:2505.15431},
  year={2025}
}

@inproceedings{jelassi2024repeat,
  title={Repeat After Me: Transformers Are Better Than State Space Models at Copying},
  author={Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M and Malach, Eran},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

@inproceedings{akyurek2024context,
  title={In-Context Language Learning: Architectures and Algorithms},
  author={Aky{\"u}rek, Ekin and Wang, Bailin and Kim, Yoon and Andreas, Jacob},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{grazzi2024mamba,
  title={Is Mamba Capable of In-Context Learning?},
  author={Grazzi, Riccardo and Siems, Julien and Schrodi, Simon and Brox, Thomas and Hutter, Frank},
  journal={arXiv preprint arXiv:2402.03170},
  year={2024}
}

@inproceedings{park2024can,
  title={Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks},
  author={Park, Jongho and Park, Jaeseung and Xiong, Zheyang and Lee, Nayoung and Cho, Jaewoong and Oymak, Samet and Lee, Kangwook and Papailiopoulos, Dimitris},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{hillis1986data,
  title={Data Parallel Algorithms},
  author={Hillis, W Daniel and Steele Jr, Guy L},
  journal={Communications of the ACM},
  volume={29},
  number={12},
  pages={1170--1183},
  year={1986},
  publisher={ACM New York, NY, USA}
}

@article{shleifer2021normformer,
  title={Norm{F}ormer: Improved {T}ransformer Pretraining with Extra Normalization},
  author={Shleifer, Sam and Weston, Jason and Ott, Myle},
  journal={arXiv preprint arXiv:2110.09456},
  year={2021}
}

@article{aksenov2024linear,
  title={Linear Transformers with Learnable Kernel Functions are Better In-Context Models},
  author={Aksenov, Yaroslav and Balagansky, Nikita and Vaina, Sofia Maria Lo Cicero and Shaposhnikov, Boris and Gorbatovski, Alexey and Gavrilov, Daniil},
  journal={arXiv preprint arXiv:2402.10644},
  year={2024}
}

@inproceedings{press2022train,
  IDS={alibi},
  title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author={Press, Ofir and Smith, Noah and Lewis, Mike},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

%%% Test Time Training/Regression

@article{liu2024longhorn,
  title={Longhorn: State Space Models are Amortized Online Learners},
  author={Liu, Bo and Wang, Rui and Wu, Lemeng and Feng, Yihao and Stone, Peter and Liu, Qiang},
  journal={arXiv preprint arXiv:2407.14207},
  year={2024}
}

@article{sun2024learning,
  title={Learning to (Learn at Test Time): RNNs with Expressive Hidden States},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and others},
  journal={arXiv preprint arXiv:2407.04620},
  year={2024}
}

@article{wang2025test,
  title={Test-time Regression: a Unifying Framework for Designing Sequence Models with Associative Memory},
  author={Wang, Ke Alexander and Shi, Jiaxin and Fox, Emily B},
  journal={arXiv preprint arXiv:2501.12352},
  year={2025}
}

@article{von2025mesanet,
  title={MesaNet: Sequence Modeling by Locally Optimal Test-Time Training},
  author={von Oswald, Johannes and Scherrer, Nino and Kobayashi, Seijin and Versari, Luca and Yang, Songlin and Schlegel, Maximilian and Maile, Kaitlin and Schimpf, Yanick and Sieberling, Oliver and Meulemans, Alexander and others},
  journal={arXiv preprint arXiv:2506.05233},
  year={2025}
}
@article{team2024chameleon,
  title={Chameleon: Mixed-Modal Early-Fusion Foundation Models},
  author={Team, Chameleon},
  journal={arXiv preprint arXiv:2405.09818},
  year={2024}
}

@article{shazeer2019fast,
  title={Fast {T}ransformer Decoding: One Write-head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}

% Vision Mambas

@inproceedings{zhu2024vision,
  title={Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{ma2024u,
  title={U-Mamba: Enhancing Long-Range Dependency for Biomedical Image Segmentation},
  author={Ma, Jun and Li, Feifei and Wang, Bo},
  journal={arXiv preprint arXiv:2401.04722},
  year={2024}
}

@article{liu2024vmamba,
  title={VMamba: Visual State Space Model},
  author={Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
  journal={arXiv preprint arXiv:2401.10166},
  year={2024}
}

%%% Genomics Mamba

@inproceedings{schiff2024caduceus,
  title={Caduceus: Bi-directional Equivariant Long-Range DNA Sequence Modeling},
  author={Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

%%% Graph Mamba

@article{wang2024graph,
  title={Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces},
  author={Wang, Chloe and Tsepa, Oleksii and Ma, Jun and Wang, Bo},
  journal={arXiv preprint arXiv:2402.00789},
  year={2024}
}

@article{behrouz2024graph,
  title={Graph Mamba: Towards Learning on Graphs with State Space Models},
  author={Behrouz, Ali and Hashemi, Farnoosh},
  journal={arXiv preprint arXiv:2402.08678},
  year={2024}
}

%%% Formal languages
@inproceedings{merrill2024illusion,
  title={The Illusion of State in State-Space Models},
  author={Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  booktitle={The International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{sarrof2024expressive,
  title={The Expressive Capacity of State Space Models: A Formal Language Perspective},
  author={Sarrof, Yash and Veitsman, Yana and Hahn, Michael},
  journal={arXiv preprint arXiv:2405.17394},
  year={2024}
}

%%% Tokenization

%%% Tokenizer-free methods

@inproceedings{wang2024mambabyte,
  IDS={mambabyte},
  title={MambaByte: Token-free Selective State Space Model},
  author={Wang, Junxiong and Gangavarapu, Tushaar and Yan, Jing Nathan and Rush, Alexander M},
  booktitle={First Conference on Language Modeling (COLM)},
  year={2024}
}

@article{yu2023megabyte,
  IDS={megabyte},
  title={MegaByte: Predicting Million-byte Sequences with Multiscale Transformers},
  author={Yu, Lili and Simig, D{\'a}niel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={78808--78823},
  year={2023}
}

@inproceedings{nawrot2022hierarchical,
  title={Hierarchical Transformers Are More Efficient Language Models},
  author={Nawrot, Piotr and Tworkowski, Szymon and Tyrolski, Micha{\l} and Kaiser, {\L}ukasz and Wu, Yuhuai and Szegedy, Christian and Michalewski, Henryk},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2022},
  pages={1559--1571},
  year={2022}
}

@inproceedings{nawrot2023efficient,
  title={Efficient Transformers with Dynamic Token Pooling},
  author={Nawrot, Piotr and Chorowski, Jan and Lancucki, Adrian and Ponti, Edoardo Maria},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6403--6417},
  year={2023}
}

@article{ho2024block,
  title={Block Transformer: Global-to-local Language Modeling for Fast Inference},
  author={Ho, Namgyu and Bae, Sangmin and Kim, Taehyeon and Jo, Hyunjik and Kim, Yireun and Schuster, Tal and Fisch, Adam and Thorne, James and Yun, Se-Young},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={48740--48783},
  year={2024}
}

@article{xue2022byt5,
  title={Byt5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models},
  author={Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={291--306},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{dai2020funnel,
  title={Funnel-{T}ransformer: Filtering out Sequential Redundancy for Efficient Language Processing},
  author={Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4271--4282},
  year={2020}
}

@article{clark2022canine,
  title={Canine: Pre-training an Efficient Tokenization-free Encoder for Language Representation},
  author={Clark, Jonathan H and Garrette, Dan and Turc, Iulia and Wieting, John},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={73--91},
  year={2022},
}

@inproceedings{godey2022manta,
  title={MANTa: Efficient Gradient-Based Tokenization for End-to-End Robust Language Modeling},
  author={Godey, Nathan and Castagn{\'e}, Roman and De La Clergerie, {\'E}ric Villemonte and Sagot, Beno{\i}t},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={2859--2870},
  year={2022}
}

@inproceedings{kallinimrt5,
  title={MrT5: Dynamic Token Merging for Efficient Byte-level Language Models},
  author={Kallini, Julie and Murty, Shikhar and Manning, Christopher D and Potts, Christopher and Csord{\'a}s, R{\'o}bert},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025}
}

@inproceedings{tay2022charformer,
  title={Charformer: Fast Character Transformers via Gradient-based Subword Tokenization},
  author={Tay, Yi and Tran, Vinh Q and Ruder, Sebastian and Gupta, Jai and Chung, Hyung Won and Bahri, Dara and Qin, Zhen and Baumgartner, Simon and Yu, Cong and Metzler, Donald},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@article{fleshman2023toucan,
  title={Toucan: Token-aware Character Level Language Modeling},
  author={Fleshman, William and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2311.08620},
  year={2023}
}

@article{MAGNET,
  title={Magnet: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-based Tokenization},
  author={Ahia, Orevaoghene and Kumar, Sachin and Gonen, Hila and Hofmann, Valentin and Limisiewicz, Tomasz and Tsvetkov, Yulia and Smith, Noah A},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={47790--47814},
  year={2024}
}

@article{egli2025multiscale,
  title={Multiscale Byte Language Models--A Hierarchical Architecture for Causal Million-Length Sequence Modeling},
  author={Egli, Eric and Manica, Matteo and Born, Jannis},
  journal={arXiv preprint arXiv:2502.14553},
  year={2025}
}

@inproceedings{thawani2023learn,
  title={Learn Your Tokens: Word-Pooled Tokenization for Language Modeling},
  author={Thawani, Avijit and Ghanekar, Saurabh and Zhu, Xiaoyuan and Pujara, Jay},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={9883--9893},
  year={2023}
}

@inproceedings{sreedhar2023local,
  title={Local Byte Fusion for Neural Machine Translation},
  author={Sreedhar, Makesh Narsimhan and Wan, Xiangpeng and Cheng, Yu and Hu, Junjie},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7199--7214},
  year={2023}
}

@article{videau2025bytes,
  IDS={aunet},
  title={From Bytes to Ideas: Language Modeling with Autoregressive U-Nets},
  author={Videau, Mathurin and Idrissi, Badr Youbi and Leite, Alessandro and Schoenauer, Marc and Teytaud, Olivier and Lopez-Paz, David},
  journal={arXiv preprint arXiv:2506.14761},
  year={2025}
}

@misc{EvaByte,
    title = {EvaByte: Efficient Byte-level Language Models at Scale},
    url = {https://hkunlp.github.io/blog/2025/evabyte},
    author = {Lin Zheng and Xueliang Zhao and Guangtao Wang and Chen Wu and David Dong and Angela Wang and Mingran Wang and Yun Du and Haige Bo and Amol Sharma and Bo Li and Kejie Zhang and Changran Hu and Urmish Thakker and Lingpeng Kong},
    year = {2025}
}

@online{sutton2019bitter,
    title = {The Bitter Lesson is coming for Tokenization
},
    url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
    author = {Sutton, Richard},
    year = {2019}
}

@online{peric2025bitter,
  title   = {The Bitter Lesson is coming for Tokenization},
  url     = {https://lucalp.dev/bitter-lesson-tokenization-and-blt},
  author  = {Luca Perić},
  date    = {2025-06-24},
  year    = {2025}
}

%%% BPE paper
@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016}
}

%%% Tokenization theory


@inproceedings{schmidt2024tokenization,
  title={Tokenization Is More Than Compression},
  author={Schmidt, Craig and Reddy, Varshini and Zhang, Haoran and Alameddine, Alec and Uzan, Omri and Pinter, Yuval and Tanner, Chris},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={678--702},
  year={2024}
}

@article{rajaraman2024analysis,
  title={An Analysis of Tokenization: Transformers Under Markov Data},
  author={Rajaraman, Nived and Jiao, Jiantao and Ramchandran, Kannan},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={62503--62556},
  year={2024}
}

% Recent papers

@inproceedings{buitrago2025understanding,
  title={Understanding and Improving Length Generalization in Recurrent Models},
  author={Buitrago, Ricardo and Gu, Albert},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}

@inproceedings{bick2025understanding,
  title={Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism},
  author={Bick, Aviv and Xing, Eric and Gu, Albert},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}

@inproceedings{buitrago2025benefits,
  title={On the Benefits of Memory for Modeling Time-Dependent PDEs},
  author={Buitrago, Ricardo and Marwah, Tanya and Gu, Albert and Risteski, Andrej},
  booktitle={The International Conference on Learning Representations (ICLR)},
  year={2025}
}

%%% Misc

@article{fedus2022switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{chen2025l,
  title={L2M: Mutual Information Scaling Law for Long-Context Language Modeling},
  author={Chen, Zhuo and Jin, Zhuotao and Luo, Di and Soljačić, Marin and others},
  journal={arXiv preprint arXiv:2503.04725},
  year={2025}
}
