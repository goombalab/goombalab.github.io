@article{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}

@article{mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{ssd,
  title={Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={ICML},
  year={2024},
}

@article{mlpmixer,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={NeurIPS},
  year={2021}
}

@article{fnet,
  title={Fnet: Mixing tokens with fourier transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={NAACL},
  year={2022}
}

@article{m2,
  title={Monarch Mixer: A simple sub-quadratic GEMM-based architecture},
  author={Fu, Daniel Y and Arora, Simran and Grogan, Jessica and Johnson, Isys and Eyuboglu, Sabri and Thomas, Armin W and Spector, Benjamin and Poli, Michael and Rudra, Atri and R{\'e}, Christopher},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{bigs,
    title = "Pretraining Without Attention",
    author = "Wang, Junxiong  and
      Yan, Jing  and
      Gu, Albert  and
      Rush, Alexander",
    booktitle = "EMNLP",
    year={2023}
}

@inproceedings{sashimi,
  title={It’s raw! audio generation with state-space models},
  author={Goel, Karan and Gu, Albert and Donahue, Chris and R{\'e}, Christopher},
  booktitle={ICML},
  year={2022},
}

@article{s4,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={ICLR},
  year={2022}
}

@article{s4d,
  title={On the parameterization and initialization of diagonal state space models},
  author={Gu, Albert and Goel, Karan and Gupta, Ankit and R{\'e}, Christopher},
  journal={NeurIPS},
  year={2022}
}

@article{s4nd,
  title={S4nd: Modeling images and videos as multidimensional signals with state spaces},
  author={Nguyen, Eric and Goel, Karan and Gu, Albert and Downs, Gordon and Shah, Preey and Dao, Tri and Baccus, Stephen and R{\'e}, Christopher},
  journal={NeurIPS},
  year={2022}
}

@article{dss,
  title={Diagonal state spaces are as effective as structured state spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{la,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={ICML},
  year={2020},
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={NAACL},
  year={2019}
}

@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={ICLR},
  year={2021}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{tnn,
    title={Toeplitz Neural Network for Sequence Modeling},
    author={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
    booktitle={ICLR},
    year={2023},
}


@article{quasi,
title = {Time and space efficient generators for quasiseparable matrices},
journal = {Journal of Symbolic Computation},
year = {2018},
note = {41th International Symposium on Symbolic and Algebraic Computation (ISSAC’16)},
author = {Clément Pernet and Arne Storjohann},
}

@article{semiseparable,
  title={On a new class of structured matrices},
  author={Eidelman, Yuli and Gohberg, Israel},
  journal={Integral Equations and Operator Theory},
  volume={34},
  number={3},
  pages={293--324},
  year={1999},
  publisher={Springer}
}

@misc{retnet,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{transnormer,
  title={The devil in linear transformer},
  author={Qin, Zhen and Han, Xiaodong and Sun, Weixuan and Li, Dongxu and Kong, Lingpeng and Barnes, Nick and Zhong, Yiran},
  journal={EMNLP},
  year={2022}
}

@inproceedings{mssm,
  author={Yassir Fathullah and Chunyang Wu and Yuan Shangguan and Junteng Jia and Wenhan Xiong and Jay Mahadeokar and Chunxi Liu and Yangyang Shi and Ozlem Kalinli and Mike Seltzer and Mark J. F. Gales},
  title={{Multi-Head State Space Model for Speech Recognition}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={241--245},
  doi={10.21437/Interspeech.2023-1036},
  issn={2308-457X}
}



@inproceedings{hyena,
  author       = {Michael Poli and
                  Stefano Massaroli and
                  Eric Nguyen and
                  Daniel Y. Fu and
                  Tri Dao and
                  Stephen Baccus and
                  Yoshua Bengio and
                  Stefano Ermon and
                  Christopher R{\'{e}}},
  title        = {Hyena Hierarchy: Towards Larger Convolutional Language Models},  
  booktitle={ICML},
  year={2023},
}

@inproceedings{h3,
  author       = {Daniel Y. Fu and
                  Tri Dao and
                  Khaled Kamal Saab and
                  Armin W. Thomas and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=COZDy0WYGg},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/FuDSTRR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glue,
    title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author={Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel},
    booktitle={ICLR},
    year={2019},
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  year={2009},
}

@inproceedings{train_bert,
  title={How to train BERT with an academic budget},
  author={Izsak, Peter and Berchansky, Moshe and Levy, Omer},
  booktitle={EMNLP},
  year={2021}
}

@article{mosaicbert,
  title={MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining},
  author={Portes, Jacob and Trott, Alex and Havens, Sam and King, Daniel and Venigalla, Abhinav and Nadeem, Moin and Sardana, Nikhil and Khudia, Daya and Frankle, Jonathan},
  journal={arXiv preprint arXiv:2312.17482},
  year={2023}
}

@article{c4,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020},
}

@inproceedings{hf_bert,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "EMNLP",
    year = "2020",
}

@article{ema,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  year={1992},
}

@inproceedings{swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={ICML},
  year={2021},
}


@ARTICLE{birnn,
  author={Schuster, M. and Paliwal, K.K.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Bidirectional recurrent neural networks}, 
  year={1997},
  volume={45},
  number={11},
  pages={2673-2681},
  keywords={Recurrent neural networks;Artificial neural networks;Training data;Databases;Probability;Shape;Parameter estimation;Speech recognition;Control systems;Telecommunication control},
  doi={10.1109/78.650093}}


@inproceedings{
gss,
title={Long Range Language Modeling via Gated State Spaces},
author={Harsh Mehta and Ankit Gupta and Ashok Cutkosky and Behnam Neyshabur},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5MkYIYCbva}
}

@article{def_quasi,
  title={Computations with quasiseparable polynomials and matrices},
  author={Bella, Tom and Eidelman, Yuli and Gohberg, Israel and Olshevsky, Vadim},
  journal={Theoretical Computer Science},
  year={2008},
}

@article{GLA,
  title={Gated linear attention transformers with hardware-efficient training},
  author={Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  journal={arXiv preprint arXiv:2312.06635},
  year={2023}
}

@article{zoology,
  title={Zoology: Measuring and improving recall in efficient language models},
  author={Arora, Simran and Eyuboglu, Sabri and Timalsina, Aman and Johnson, Isys and Poli, Michael and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2312.04927},
  year={2023}
}

@article{based,
  title={Simple linear attention language models balance the recall-throughput tradeoff},
  author={Arora, Simran and Eyuboglu, Sabri and Zhang, Michael and Timalsina, Aman and Alberti, Silas and Zinsley, Dylan and Zou, James and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2402.18668},
  year={2024}
}

@article{repeatafterme,
  title={Repeat after me: Transformers are better than state space models at copying},
  author={Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M and Malach, Eran},
  journal={arXiv preprint arXiv:2402.01032},
  year={2024}
}

@article{attention,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{ckconv,
  title={Ckconv: Continuous kernel convolution for sequential data},
  author={Romero, David W and Kuzina, Anna and Bekkers, Erik J and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal={arXiv preprint arXiv:2102.02611},
  year={2021}
}

@article{linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{monarch,
  title={Monarch: Expressive structured matrices for efficient and accurate training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={ICML},
  pages={4690--4721},
  year={2022},
  organization={PMLR}
}

@article{caduceus,
  title={Caduceus: Bi-directional equivariant long-range dna sequence modeling},
  author={Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2403.03234},
  year={2024}
}

@article{vmamba,
  title={Vmamba: Visual state space model},
  author={Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
  journal={arXiv preprint arXiv:2401.10166},
  year={2024}
}

@article{vision_mamba,
  title={Vision mamba: Efficient visual representation learning with bidirectional state space model},
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2401.09417},
  year={2024}
}

@article{kaleidoscope,
  title={Kaleidoscope: An efficient, learnable representation for all structured linear maps},
  author={Dao, Tri and Sohoni, Nimit S and Gu, Albert and Eichhorn, Matthew and Blonder, Amit and Leszczynski, Megan and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2012.14966},
  year={2020}
}

@inproceedings{butterfly,
  title={Learning fast algorithms for linear transforms using butterfly factorizations},
  author={Dao, Tri and Gu, Albert and Eichhorn, Matthew and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International conference on machine learning},
  pages={1517--1527},
  year={2019},
  organization={PMLR}
}

@inproceedings{convnext,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{lru,
  title={Resurrecting recurrent neural networks for long sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={International Conference on Machine Learning},
  pages={26670--26698},
  year={2023},
  organization={PMLR}
}

@inproceedings{quasiseparable,
  title={Exact computations with quasiseparable matrices},
  author={Pernet, Cl{\'e}ment and Signargout, Hippolyte and Villard, Gilles},
  booktitle={Proceedings of the 2023 International Symposium on Symbolic and Algebraic Computation},
  pages={480--489},
  year={2023}
}

@article{btt,
  title={Compute Better Spent: Replacing Dense Layers with Structured Matrices},
  author={Qiu, Shikai and Potapczynski, Andres and Finzi, Marc and Goldblum, Micah and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2406.06248},
  year={2024}
}