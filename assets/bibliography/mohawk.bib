@inproceedings{ssd,
  title={Transformers are {SSM}s: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024},
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{chen2020distilling,
      title={Distilling Knowledge Learned in BERT for Text Generation}, 
      author={Yen-Chun Chen and Zhe Gan and Yu Cheng and Jingzhou Liu and Jingjing Liu},
      year={2020},
      eprint={1911.03829},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{haidar2019textkdgan,
      title={TextKD-GAN: Text Generation using KnowledgeDistillation and Generative Adversarial Networks}, 
      author={Md. Akmal Haidar and Mehdi Rezagholizadeh},
      year={2019},
      eprint={1905.01976},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hahn2019selfknowledge,
      title={Self-Knowledge Distillation in Natural Language Processing}, 
      author={Sangchul Hahn and Heeyoul Choi},
      year={2019},
      eprint={1908.01851},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2021understanding,
      title={Understanding Knowledge Distillation in Non-autoregressive Machine Translation}, 
      author={Chunting Zhou and Graham Neubig and Jiatao Gu},
      year={2021},
      eprint={1911.02727},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tan2019multilingual,
      title={Multilingual Neural Machine Translation with Knowledge Distillation}, 
      author={Xu Tan and Yi Ren and Di He and Tao Qin and Zhou Zhao and Tie-Yan Liu},
      year={2019},
      eprint={1902.10461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hu2018attentionguided,
      title={Attention-Guided Answer Distillation for Machine Reading Comprehension}, 
      author={Minghao Hu and Yuxing Peng and Furu Wei and Zhen Huang and Dongsheng Li and Nan Yang and Ming Zhou},
      year={2018},
      eprint={1808.07644},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yang2019model,
      title={Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System}, 
      author={Ze Yang and Linjun Shou and Ming Gong and Wutao Lin and Daxin Jiang},
      year={2019},
      eprint={1910.08381},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mercat2024linearizing,
      title={Linearizing Large Language Models}, 
      author={Jean Mercat and Igor Vasiljevic and Sedrick Keh and Kushal Arora and Achal Dave and Adrien Gaidon and Thomas Kollar},
      year={2024},
      eprint={2405.06640},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2024hedgehog,
      title={The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry}, 
      author={Michael Zhang and Kush Bhatia and Hermann Kumbong and Christopher Ré},
      year={2024},
      eprint={2402.04347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mehta2022long,
      title={Long Range Language Modeling via Gated State Spaces}, 
      author={Harsh Mehta and Ankit Gupta and Ashok Cutkosky and Behnam Neyshabur},
      year={2022},
      eprint={2206.13947},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ma2023mega,
      title={Mega: Moving Average Equipped Gated Attention}, 
      author={Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
      year={2023},
      eprint={2209.10655},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fu2023hungry,
      title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models}, 
      author={Daniel Y. Fu and Tri Dao and Khaled K. Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
      year={2023},
      eprint={2212.14052},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2023selective,
      title={Selective Structured State-Spaces for Long-Form Video Understanding}, 
      author={Jue Wang and Wentao Zhu and Pichao Wang and Xiang Yu and Linda Liu and Mohamed Omar and Raffay Hamid},
      year={2023},
      eprint={2303.14526},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{sun2023retentive,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{peng2023rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{katharopoulos2020transformers,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gu2023mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2023},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gu2022efficiently,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@phdthesis{gu2023thesis,
  title={Modeling Sequences with Structured State Spaces},
  author={Gu, Albert},
  year={2023},
  school={Stanford University},
  type         = {PhD thesis},
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{qin2023toeplitz,
      title={Toeplitz Neural Network for Sequence Modeling}, 
      author={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
      year={2023},
      eprint={2305.04749},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{leviathan2023fast,
      title={Fast Inference from Transformers via Speculative Decoding}, 
      author={Yaniv Leviathan and Matan Kalman and Yossi Matias},
      year={2023},
      eprint={2211.17192},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{han2021dynamic,
      title={Dynamic Neural Networks: A Survey}, 
      author={Yizeng Han and Gao Huang and Shiji Song and Le Yang and Honghui Wang and Yulin Wang},
      year={2021},
      eprint={2102.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{shazeer2019fast,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Zadeh_2020,
   title={GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference},
   url={http://dx.doi.org/10.1109/MICRO50266.2020.00071},
   DOI={10.1109/micro50266.2020.00071},
   booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
   publisher={IEEE},
   author={Zadeh, Ali Hadi and Edo, Isak and Awad, Omar Mohamed and Moshovos, Andreas},
   year={2020},
   month=oct }


@misc{liu2023ring,
      title={Ring Attention with Blockwise Transformers for Near-Infinite Context}, 
      author={Hao Liu and Matei Zaharia and Pieter Abbeel},
      year={2023},
      eprint={2310.01889},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yang2024gated,
      title={Gated Linear Attention Transformers with Hardware-Efficient Training}, 
      author={Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
      year={2024},
      eprint={2312.06635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{dao2022flashattention,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Guo_2022,
   title={Attention mechanisms in computer vision: A survey},
   volume={8},
   ISSN={2096-0662},
   url={http://dx.doi.org/10.1007/s41095-022-0271-y},
   DOI={10.1007/s41095-022-0271-y},
   number={3},
   journal={Computational Visual Media},
   publisher={Springer Science and Business Media LLC},
   author={Guo, Meng-Hao and Xu, Tian-Xing and Liu, Jiang-Jiang and Liu, Zheng-Ning and Jiang, Peng-Tao and Mu, Tai-Jiang and Zhang, Song-Hai and Martin, Ralph R. and Cheng, Ming-Ming and Hu, Shi-Min},
   year={2022},
   month=mar, pages={331–368} }

@article{zeming_protein,
	abstract = {a.},
	author = {Zeming Lin and Halil Akin and Roshan Rao and Brian Hie and Zhongkai Zhu and Wenting Lu and Nikita Smetanin and Robert Verkuil and Ori Kabeli and Yaniv Shmueli and Allan dos Santos Costa and Maryam Fazel-Zarandi and Tom Sercu and Salvatore Candido and Alexander Rives},
	doi = {10.1126/science.ade2574},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.ade2574},
	journal = {Science},
	number = {6637},
	pages = {1123-1130},
	title = {Evolutionary-scale prediction of atomic-level protein structure with a language model},
	url = {https://www.science.org/doi/abs/10.1126/science.ade2574},
	volume = {379},
	year = {2023},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.ade2574},
	bdsk-url-2 = {https://doi.org/10.1126/science.ade2574}}


@misc{radford2022robust,
      title={Robust Speech Recognition via Large-Scale Weak Supervision}, 
      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
      year={2022},
      eprint={2212.04356},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@misc{gu2022parameterization,
      title={On the Parameterization and Initialization of Diagonal State Space Models}, 
      author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2206.11893},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gupta2022diagonal,
      title={Diagonal State Spaces are as Effective as Structured State Spaces}, 
      author={Ankit Gupta and Albert Gu and Jonathan Berant},
      year={2022},
      eprint={2203.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{smith2023simplified,
      title={Simplified State Space Layers for Sequence Modeling}, 
      author={Jimmy T. H. Smith and Andrew Warrington and Scott W. Linderman},
      year={2023},
      eprint={2208.04933},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhou2023deep,
      title={Deep Latent State Space Models for Time-Series Generation}, 
      author={Linqi Zhou and Michael Poli and Winnie Xu and Stefano Massaroli and Stefano Ermon},
      year={2023},
      eprint={2212.12749},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{io_aware, author = {Aggarwal, Alok and Vitter, Jeffrey,S.}, title = {The input/output complexity of sorting and related problems}, year = {1988}, issue_date = {Sept. 1988}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {31}, number = {9}, issn = {0001-0782}, url = {https://doi.org/10.1145/48529.48535}, doi = {10.1145/48529.48535}, abstract = {a}, journal = {Commun. ACM}, month = {sep}, pages = {1116–1127}, numpages = {12} }

@misc{ma2024umamba,
      title={U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation}, 
      author={Jun Ma and Feifei Li and Bo Wang},
      year={2024},
      eprint={2401.04722},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@misc{zhu2024vision,
      title={Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model}, 
      author={Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},
      year={2024},
      eprint={2401.09417},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2024mambabyte,
      title={MambaByte: Token-free Selective State Space Model}, 
      author={Junxiong Wang and Tushaar Gangavarapu and Jing Nathan Yan and Alexander M. Rush},
      year={2024},
      eprint={2401.13660},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{chandrasekaran2002fast,
  title={Fast stable solver for sequentially semi-separable linear systems of equations},
  author={Chandrasekaran, Shiv and Dewilde, Patrick and Gu, Ming and Pals, T and van der Veen, Alle-Jan},
  booktitle={International Conference on High-Performance Computing},
  pages={545--554},
  year={2002},
  organization={Springer}
}
@misc{niu2024does,
      title={What does the Knowledge Neuron Thesis Have to do with Knowledge?}, 
      author={Jingcheng Niu and Andrew Liu and Zining Zhu and Gerald Penn},
      year={2024},
      eprint={2405.02421},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gunasekar2023textbooks,
      title={Textbooks Are All You Need}, 
      author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio César Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Harkirat Singh Behl and Xin Wang and Sébastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuanzhi Li},
      year={2023},
      eprint={2306.11644},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{beck2024xlstm,
      title={xLSTM: Extended Long Short-Term Memory}, 
      author={Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      year={2024},
      eprint={2405.04517},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{qin2024hgrn2,
      title={HGRN2: Gated Linear RNNs with State Expansion}, 
      author={Zhen Qin and Songlin Yang and Weixuan Sun and Xuyang Shen and Dong Li and Weigao Sun and Yiran Zhong},
      year={2024},
      eprint={2404.07904},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%%% Distillation
@article{liang2023homodistil,
  title={Homodistil: Homotopic task-agnostic distillation of pre-trained transformers},
  author={Liang, Chen and Jiang, Haoming and Li, Zheng and Tang, Xianfeng and Yin, Bin and Zhao, Tuo},
  journal={arXiv preprint arXiv:2302.09632},
  year={2023}
}

@article{wang2020minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020}
}

@article{jha2023large,
  title={Large Language Model Distillation Doesn't Need a Teacher},
  author={Jha, Ananya Harsh and Groeneveld, Dirk and Strubell, Emma and Beltagy, Iz},
  journal={arXiv preprint arXiv:2305.14864},
  year={2023}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{kasai2021finetuning,
  title={Finetuning pretrained transformers into rnns},
  author={Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A},
  journal={arXiv preprint arXiv:2103.13076},
  year={2021}
}

@inproceedings{paperno2016lambada,
  title={The {L}{A}{M}{B}{A}{D}{A} Dataset: Word Prediction Requiring a Broad Discourse Context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  pages={1525--1534},
  year={2016}
}

@inproceedings{zellers2019hellaswag,
  title={Hella{S}wag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An Adversarial {W}inograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2018think,
  title={Think you have Solved Question Answering? Try {A}{R}{C}, the {A}{I}2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{bisk2020piqa,
  title={P{I}{Q}{A}: Reasoning about Physical Commonsense in Natural Language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@misc{jamba2024,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
      year={2024},
      eprint={2403.19887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19887}, 
}

@misc{PoinTramba,
      title={PoinTramba: A Hybrid Transformer-Mamba Framework for Point Cloud Analysis}, 
      author={Zicheng Wang and Zhenghao Chen and Yiming Wu and Zhen Zhao and Luping Zhou and Dong Xu},
      year={2024},
      eprint={2405.15463},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.15463}, 
}

@misc{Samba,
      title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}, 
      author={Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen},
      year={2024},
      eprint={2406.07522},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07522}, 
}

@misc{C4,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{MambaVision,
      title={MambaVision: A Hybrid Mamba-Transformer Vision Backbone}, 
      author={Ali Hatamizadeh and Jan Kautz},
      year={2024},
      eprint={2407.08083},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.08083}, 
}

@article{BTSurvery,
	author = {Serkan Gugercin and Athanasios C. Antoulas},
	doi = {10.1080/00207170410001713448},
	eprint = {https://doi.org/10.1080/00207170410001713448},
	journal = {International Journal of Control},
	number = {8},
	pages = {748--766},
	publisher = {Taylor \& Francis},
	title = {A Survey of Model Reduction by Balanced Truncation and Some New Results},
	volume = {77},
	year = {2004},
}

@ARTICLE{TVBTSurvery,
  author={Sandberg, H. and Rantzer, A.},
  journal={IEEE Transactions on Automatic Control}, 
  title={Balanced truncation of linear time-varying systems}, 
  year={2004},
  volume={49},
  number={2},
  pages={217-229},
  keywords={Time varying systems;Linear systems;Reduced order systems;Riccati equations;Nonlinear systems;Linear approximation;Councils;Sufficient conditions;Asymptotic stability;Robust control},
  doi={10.1109/TAC.2003.822862}
}

@book{Dewilde1998,
  title={Time-Varying Systems and Computations},
  author={Patrick Dewilde and Alle-Jan Veen},
  year={1998},
  publisher={Springer New York, NY},
  edition={1},
  isbn={978-0-7923-8189-1},
  isbn={978-1-4419-5045-1},
  isbn={978-1-4757-2817-0},
  doi={https://doi.org/10.1007/978-1-4757-2817-0},
  url={https://doi.org/10.1007/978-1-4757-2817-0},
  pages={XIV, 460},
  series={Springer Book Archive},
  note={Springer Science+Business Media Dordrecht 1998},
  keywords={Systems Theory, Control, Electrical Engineering, Linear and Multilinear Algebras, Matrix Theory, Signal, Image and Speech Processing}
}

@misc{hwang2024hydrabidirectionalstatespace,
      title={Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers}, 
      author={Sukjun Hwang and Aakash Lahoti and Tri Dao and Albert Gu},
      year={2024},
      eprint={2407.09941},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{MELCHIOR201472,
	author = {Samuel A. Melchior and Paul {Van Dooren} and Kyle A. Gallivan},
	doi = {https://doi.org/10.1016/j.apnum.2013.10.007},
	issn = {0168-9274},
	journal = {Applied Numerical Mathematics},
	keywords = {Model reduction, Time-varying systems, State-space model},
	pages = {72-81},
	title = {Model reduction of linear time-varying systems over finite horizons},
	url = {https://www.sciencedirect.com/science/article/pii/S0168927413001414},
	volume = {77},
	year = {2014},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0168927413001414},
	bdsk-url-2 = {https://doi.org/10.1016/j.apnum.2013.10.007}}


@article{Dewilde1993,
	author = {Dewilde, P. and van der Veen, A.J.},
	date = {1993/03/01},
	date-added = {2024-07-31 19:19:19 -0700},
	date-modified = {2024-07-31 19:19:19 -0700},
	doi = {10.1007/BF01322544},
	id = {Dewilde1993},
	isbn = {1420-8989},
	journal = {Integral Equations and Operator Theory},
	number = {1},
	pages = {1--45},
	title = {On the Hankel-norm approximation of upper-triangular operators and matrices},
	url = {https://doi.org/10.1007/BF01322544},
	volume = {17},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1007/BF01322544}}


@article{VANDERVEEN19941145,
	author = {van der Veen, A.J. and Dewilde, P.},
	doi = {https://doi.org/10.1016/0024-3795(94)90383-2},
	issn = {0024-3795},
	journal = {Linear Algebra and its Applications},
	pages = {1145-1201},
	title = {On low-complexity approximation of matrices},
	url = {https://www.sciencedirect.com/science/article/pii/0024379594903832},
	volume = {205-206},
	year = {1994},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0024379594903832},
	bdsk-url-2 = {https://doi.org/10.1016/0024-3795(94)90383-2}}
