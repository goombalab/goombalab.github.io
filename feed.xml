<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://goombalab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://goombalab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-02T02:48:25+00:00</updated><id>https://goombalab.github.io/feed.xml</id><title type="html">Goomba Lab</title><subtitle>Homepage of the Goomba AI Lab @ CMU MLD. # A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Cross-Architecture Distillation Part I - The MOHAWK Framework</title><link href="https://goombalab.github.io/blog/2024/distillation-part1-mohawk/" rel="alternate" type="text/html" title="Cross-Architecture Distillation Part I - The MOHAWK Framework"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/distillation-part1-mohawk</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/distillation-part1-mohawk/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2408.10189">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba">Code</a>]</p> <ol> <li>Part I - MOHAWK</li> <li><a href="/blog/2024/distillation-part2-phi-mamba/">Part II - Phi-Mamba</a></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>We start off by summarizing some important aspects from <d-cite key="ssd"></d-cite>, specifically the sequence transformation/mixer viewpoint and the Mamba-2 SSM variant.</p> <p><strong>Definition:</strong> A <em>sequence transformation/mixer</em> refers to a parameterized map on sequences $Y = f_{\theta}(X)$ where $X, Y \in \mathbb{R}^{(T, P)}$ and $\theta$ is an arbitrary collection of parameters. $T$ represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_t, Y_t \in \mathbb{R}^P$.</p> <p>In layman’s terms, <em>sequence mixers</em> aggregate tokens across various time steps. This ability to learn temporal interactions and information forms the foundation of modern deep sequence models, like Transformers.</p> <p><strong>Definition:</strong> <em>Matrix mixers</em> are a specific type of sequence mixers that can be represented as $Y = MX$ for matrix $M \in \mathbb{R}^{(T,T)}$.</p> <p>Examples of <em>matrix mixers</em> which fall under this definition include vanilla self-attention, where $M = \text{Softmax}(\mathbf{Q}\mathbf{K}^\top)$ <d-cite key="vaswani2023attention"></d-cite>, linear attention <d-cite key="katharopoulos2020transformers"></d-cite>, and Toeplitz matrices <d-cite key="qin2023toeplitz"></d-cite>.</p> <h3 id="mamba-2">Mamba-2</h3> <p>Mamba-2 <d-cite key="ssd"></d-cite> is a recent variant of Structured State Space Models (SSMs) <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> which can be viewed as a matrix mixer that can be applied onto an input sequence in subquadratic time due to structured matrix multiplication. Mamba-2 is a time-varying SSM, defined as</p> \[\begin{aligned} h_{t+1} &amp;= A_t h_t + B_t x_t \\ y_t &amp;= C_t h_t \end{aligned}\] <p>where $B_t$ and $C_t$, like in Mamba-1 <d-cite key="gu2023mamba"></d-cite>, are input-dependent projections, but $A_t$ is the identity matrix $I$ multiplied by a scalar $\alpha_t$. Importantly, Mamba-2 identified the <em>Structured State Space Duality (SSD)</em> connection which found that specific variants of SSMs can be viewed as a form of causal linear attention <d-cite key="katharopoulos2020transformers"></d-cite>.</p> <p>Formally, the Mamba-2 SSD matrix mixer can be represented as</p> \[\begin{equation} \label{eq:ssd-matrix-mixer} \begin{aligned} \begin{bmatrix} \alpha_{1} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{2:1} &amp; \alpha_{2} &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{3:1} &amp; \alpha_{3:2} &amp; \alpha_{3} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \alpha_{n:1} &amp; \alpha_{n:2} &amp; \alpha_{n:3} &amp; \cdots &amp; \alpha_{n} \end{bmatrix} \circ (C \cdot B^\top) \cdot X \end{aligned} \end{equation}\] <p>where $\alpha_{t:i} = \alpha_{t-1} \cdot \alpha_{t-2} \cdots \alpha_{i}$.</p> <p>From this representation, one can see that Mamba-2 can be viewed as causal linear attention with a learnable causal mask!</p> <h2 id="mohawk-method">MOHAWK Method</h2> <p>Inspired by the <em>matrix mixer</em> viewpoint which provides a common lense for viewing the key components of various architectures, we introduce the <strong>MOHAWK</strong> framework for cross-architectural distillation, which is composed of three stages:</p> <ol> <li><strong>M</strong>atrix <strong>O</strong>rientation</li> <li><strong>H</strong>idden-State <strong>A</strong>lignment</li> <li><strong>W</strong>eight-Transfer and <strong>K</strong>nowledge Distillation</li> </ol> <p>These three sequential stages distill the student model from the bottom up, steadily increasing the number of components distilled into at each stage until the end student model has been distilled end-to-end. We find that this multi-stage process is much more effective than traditional knowledge distillation.</p> <p>Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba –based on the Phi-1.5 model <d-cite key="gunasekar2023textbooks"></d-cite>– along with the specifics of its distillation process.</p> <p>For clarity, we refer to the term <em>block</em> as a repeating component that forms the backbone of the end-to-end model. <em>Blocks</em> are composed of layers, for instance the Llama block is composed of a self-attention layer followed by a MLP layer. <em>Layers</em> can be composed of numerous subcomponents, like the self-attention layer, which encompasses the projections and the self-attention mechanism, and the Mamba layer, which includes the projections, convolution, and SSM mixer, etc.</p> <h3 id="stage-1-matrix-orientation">Stage 1: Matrix Orientation</h3> <p>We begin the first stage of MOHAWK by matching the matrix mixer of both the student and teacher. Prior to directly aligning the matrix mixers themselves, we first adjust the <em>matrix mixer layer</em> to be analogous to that of the teacher’s, i.e., structurally both layers are the same except the matrix mixer component. We then minimize the distance between the matrix mixer of the teacher and student layers, which can be expressed as the following equation:</p> <p>\(\begin{equation} \label{eq:matrix-orientation-minimization} \min_{\mathbf{\phi}} \|\mathrm{TeacherMixer}(\mathbf{u}) - \mathrm{StudentMixer}_{\boldsymbol{\phi}}(\mathbf{u})\|_F \end{equation}\) where $\phi$ represents the parameters in the layer and $\mathbf{u}$ is the shared input derived from the teacher model. The stage ensures that the student can closely approximate the teacher’s matrix mixer layer which sets a strong foundation for teacher matching in subsequent stages of the MOHAWK process.</p> <p>For Phi-Mamba: Because the student model uses the Mamba-2 mixer, we initialize the convolution to identity and discarded the nonlinear activation after the convolution to ensure the components upstream of the matrix mixers roughly equivalent to the self-attention layer. The loss calculate was between the self-attention matrix of the teacher and the “unraveled” SSM matrix as shown in Equation \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="stage-2-hidden-state-alignment">Stage 2: Hidden-State Alignment</h3> <p>After optimizing Equation \eqref{eq:matrix-orientation-minimization} in Stage 1, Stage 2 proceeds to match the outputs of the student and teacher blocks.</p> <p>\(\begin{equation} \label{eq:hidden-state-minimization} \min_{\mathbf{\phi}} \|\mathrm{AttnBlock}(\mathbf{u}) - \mathrm{StudentMixerBlock}_{\boldsymbol{\phi}}(\mathbf{u})\|_2 \end{equation}\) where once again the inputs are the same. Like Stage 1, Stage 2 can be run in parallel. We find that the distance between the layer outputs is strongly correlated with the student model’s ability to recover the teacher model’s knowledge.</p> <p>For Phi-Mamba: To keep the block architectures as similar as possible, we initialized the Mamba-2 gate to be a value of 1 to simulate Phi’s lack of gating and removed the norm prior to the output projection.</p> <h3 id="stage-3-weight-transfer-and-knowledge-distillation">Stage 3: Weight-Transfer and Knowledge Distillation</h3> <p>The final stage aims to fine-tune the entire student model to match the performance of the teacher. This stage is critical for mending the potential discrepancies between post-Stage 2 blocks. We also initialize information dense components of the student model, in particular the MLPs, embedding, and LM head, before fine-tuning the student end-to-end. Given the weight transfer of critical architectural components, the overall block structure of the student mirror that of the teacher model, e.g., our student model has the MLPs and matrix mixer layers in parallel. Finally, we use knowledge distillation loss <d-cite key="hinton2015distilling"></d-cite> to encourage the student to imitate the teacher’s distribution:</p> \[\begin{equation} \min_{\mathbf{\phi}} \mathbf{\mathcal{L}}_{\mathrm{CE}}\big(\mathrm{Teacher}(\mathbf{x}), \mathrm{Student}_{\boldsymbol{\phi}} (\mathbf{x})\big) \end{equation}\] <p>For Phi-Mamba: We create a new Phi-Mamba block that has the same parallel MLP-matrix mixer layer structure as the original Phi-1.5 block. We copy over the MLP and norm weights, token embeddings, and language model head and pre-head norm as it has been hypothesized that much of a model’s information is stored in these components. We also find that the MLPs can be frozen after the transfer with only a slight decrease in performance but reduce the number of trainable parameters by more than half!</p> <h2 id="approximating-self-attention">Approximating Self-Attention</h2> <p>With the MOHAWK method we can now distill from any quadratic self-attention model to any model that utilizes a <em>matrix mixer</em> for sequential modeling. But, a caveat is that the performance of the student model is inherently constrained by the expressivity of its matrix mixer. So why did we decide to use the Mamba-2 mixer instead of an alternative like linear attention or gated convolution? In this next section, we will empirically explore Mamba-2’s ability to approximate the self-attention matrix $\text{Softmax}(QK^\top)$ and compare it to some other popular sub-quadratic matrix mixer families. We describe a couple of them below.</p> <h3 id="linear-attention-and-ssd">Linear Attention and SSD</h3> <p>When describing linear attention matrices, we can utilize the fact that both $Q$ and $K$ are token-dependent projections of some input $x \in \mathbb{R}^{d_{in}}$ onto $\mathbb{R}^{d_{out}}$, and therefore the rank of $Q$ and $K$ are bounded by $\min{ { d_{in}, d_{out} } }$ For multi-head linear attention, $d_{out}$, which corresponds to the head dimension, is typically a small value (e.g., $64$ and $128$ for Phi-1.5 and Llama2-7b-Chat respectively). Thus, we approximate linear attention matrix mixers using causal low-rank matrices $\mathbf{L \circ Q K}^\top$, where $\mathbf{L}$ is a lower-triangular causal mask of 1s, and $\mathbf{Q}$, $\mathbf{K}$ are in $\mathbb{R}^{n \times d}$ with $d \ll n$.</p> <p>For the multi-head Mamba-2 matrix family, we utilize the state space dual (SSD) layer in a manner similar to the previous linear attention class, but imbuing the causal matrix $\mathbf{L}$ with an $n$-degree rolling multiplicative structure for $\mathrm{SSD}$. This can be seen as a more expressive mask that generalizes the lower-triangular, ones-only causal mask \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="general-semi-separable-and-toeplitz">General Semi-separable and Toeplitz</h3> <p>To approximate the general class of semi-separable matrices (abbreviated as “SSM” in the following table), we utilize <em>balanced truncation</em>. This method is used in the field of time-invariant Dynamical System model reduction <d-cite key="BTSurvery"></d-cite> and has been modified for use in time-varying systems <d-cite key="TVBTSurvery"></d-cite>. Similarly, for the family of Toeplitz matrices, which represent a convolution operation, we apply a causal mask, the same one used for causal low-rank matrices, on top a Toeplitz matrix.</p> <h3 id="empirical-approximation">Empirical Approximation</h3> <p>To empirically validate the expressiveness of the four aforementioned families, we sample 1,000 attention matrices, each consisting of 512 tokens, from the Llama2-7B-Chat <d-cite key="touvron2023llama"></d-cite> model on four different datasets. One attention head, and its respective attention matrix, from each layer was chosen at random. Both (causal) low-rank (LR) and SSD matrix families were approximated with 10,000 steps of gradient descent per sample. SSM and Toeplitz were both calculated without using gradient descent using balanced truncation and a simple heuristic respectively. We calculate the Frobenius distance between each “ground truth” self-attention matrix and the approximated matrix of each family.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-480.webp 480w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-800.webp 800w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx.png" width="100%" height="auto" title="Matrix Approximation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Self Attention matrix approximation by structured matrix mixers</figcaption> </figure> <p>Given the previous table’s experiment was conducted in a very controlled setting, we further explore the ability of the various families’ abilities to approximate the self-attention matrix within a language model. We replace the self-attention matrix mixers of a Phi-1.5 model with either input-dependent Toeplitz, causal low-rank, or SSD (our Mamba-2 variant) matrix mixers, and ran the second and third stages of our MOHAWK procedure for 1B tokens each.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl.png" width="100%" height="auto" title="Matrix Structure Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Evaluations of various structured matrices on downstream tasks</figcaption> </figure> <p>We find that there is a constant correlation between the self attention approximation abilities (measured via projection distances) of a matrix family and the downstream performance metrics (accuracy) of the matrix mixer integrated into an end-to-end language model. This finding that more expressive matrix mixers lead to more effective models is echoed in <d-cite key="hwang2024hydrabidirectionalstatespace"></d-cite>.</p> <h2 id="next-up">Next Up</h2> <p>The <a href="/blog/2024/distillation-part2-phi-mamba/">following section</a> will cover MOHAWK in action, distilling our final Phi-Mamba and Hybrid-Phi-Mamba models, and explore the training laws regarding each stage of MOHAWK.</p>]]></content><author><name>Aviv Bick*</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws</title><link href="https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba/" rel="alternate" type="text/html" title="Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2408.10189">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba">Code</a>]</p> <ol> <li><a href="/blog/2024/distillation-part1-mohawk/">Part I - MOHAWK</a></li> <li>Part II - Phi-Mamba</li> </ol> <p>In <a href="/blog/2024/distillation-part1-mohawk/">Part I</a> of this series, we covered important terminology, the Mamba-2 architecture, and the MOHAWK architecture. We also demonstrated Mamba-2’s ability to match the self-attention matrix of Transformers, which influenced our choice to use it as the student model for validating our MOHAWK method.</p> <p>In this section, we will explore the training laws regarding each of the three stages of MOHAWK and empirically validate the importance of all stages. We use the cumulative insights gained to then distill a <strong>fully subquadratic Mamba model using only 3B tokens</strong> - less than 1% of many of the other models’ token budget - while being <strong>competitive with many of the current state-of-the-art open-source subquadratic models</strong>! We also distill a strong Mamba-Attention hybrid.</p> <h2 id="final-results">Final Results</h2> <p>We empirically validate the MOHAWK framework by distilling the pretrained Phi-1.5 model into a 1.5B Mamba variant, dubbed Phi-Mamba. Our final model was distilled with <strong>only 3B tokens</strong>, with a 80M/160M/2.76B token split among Stage 1/2/3, from the C4 dataset with a context length of 2048. The choices for these token splits were influenced by our verification of the importance of all three stages and training laws that determined, given a fixed token budget, how to allocate resources, which we detail in the following sections.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance.png" width="100%" height="auto" title="Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance of Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <h2 id="importance-of-each-mohawk-stage">Importance of Each MOHAWK Stage</h2> <p>A brief recap of the three stages of MOHAWK are</p> <p>1) <strong>Matrix Orientation</strong>: matches the matrix mixer of each respective block.</p> <p>2) <strong>Hidden-State Alignment</strong>: independently compares the block output given the same input across all layers of the student model.</p> <p>3) <strong>Weight-Transfer and Knowledge Distillation</strong>: performs knowledge distillation of logits from teacher to student and copies over crucial weights from the teacher model.</p> <p><strong>Each stage plays a crucial role</strong> as shown in our ablations below. All the runs were performed with a fixed total token count.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl.png" width="100%" height="auto" title="MOHAWK Ablations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Effects of various MOHAWK stage ablations on downstream performance</figcaption> </figure> <p>As expected, Stage 3’s end-to-end alignment is important as the <strong>previous stages only match the block outputs</strong>, leaving the blocks disjoint if the hidden state cannot be completely matched, as shown with both the Phi-Mamba and Hybrid-Phi-Mamba trained on Stage 3 outperform their counterparts trained with Stage 2. Of course, student models that have more mixing layers similar to the teacher may see a diminished impact of Stage 3 as the layers may be aligned more with only Stage 2.</p> <p>The addition of a Stage 2 initialization provides additional synergy, <strong>boosting performance significantly compared to Stage 3 only</strong>. We also note that the effects of adding Stage 2 is more pronounced in cases where the student architecture is less similar to the teacher architecture, e.g., the improvement for Phi-Mamba which has zero attention layers is larger than Hybrid-Phi-Mamba which has four.</p> <p>Stage 1 also provides a good in downstream performance. For example, only with the addition of Stage 1 on top of Stage 2 and 3 can a Phi-to-Phi distillation <strong>recover the original teacher Phi’s overall performance</strong>. And, we see in the two other architectures that performance gains can also be observed.</p> <h2 id="training-laws-for-mohawk">Training Laws for MOHAWK</h2> <p>We aimed to evaluate the impact the preceding stage had on the current stage’s performance.</p> <p>For the Stage 2 + 3 pair, we trained Phi-Mamba instances from scratch using Stage 2 to various checkpoints. These checkpoints were then used to initialize Phi-Mamba instances that were trained using Stage 3 to different total budgets. The figure below shows that given an adequate training budget, <strong>student models initialized from Stage 2 outperform students trained only with Stage 3</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage23-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage23.png" width="100%" height="auto" title="Stage 2 + 3 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Training Laws for Stage 2 and 3 of MOHAWK</figcaption> </figure> <p>Given the previous finding, we then analyze how matrix mixer matching (Stage 1) can impact the student’s ability to match the overall mixer block with the teacher (Stage 2). Similar to before, we train numerous Phi-Mamba models using Stage 1 and use them as initializations for Stage 2 and compare them against each other and also a Stage 2 only model. Here, we find that <strong>even a small budget allocated to Stage 1 can help the subsequent stage</strong> perform better than random initialization.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage12-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage12.png" width="100%" height="auto" title="Stage 1 + 2 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Training Laws for Stage 1 and 2 of MOHAWK</figcaption> </figure> <h3 id="training-the-final-phi-mamba-model">Training the Final Phi-Mamba Model</h3> <p>Using the insights gained in the training laws above, we finalized our training regime given a fixed budget of 3B tokens. Stage 1 was allocated 80M due to the strong performance on matrix distance and hidden state distance. Stage 2 was trained for 160M tokens given the seeming saturation of both hidden state distance and perplexity when compared to the other initialization states, e.g., 10M, 20M, 40M, etc. We train Stage 3 to reach 3B tokens in total, but reduced the learning rate of the last stage to alleviate training instabilities. We hypothesize that this is due to the Stage 1 + 2 initialization’s Mamba component being quite similar to that of the teacher model, so a large learning rate coupled with disconnect between blocks, which are addressed in Stage 3, can cause training instabilities.</p> <h2 id="hybrid-phi-mamba-model">Hybrid Phi-Mamba Model</h2> <p>There has been a growing body of work that combines both Attention and SSM mechanisms, leading to improved performance over either one used by itself <d-cite key="Samba"></d-cite><d-cite key="jamba2024"></d-cite><d-cite key="MambaVision"></d-cite>. Although incorporating Attention layers does make the model quadratic, limiting their number allows us to mitigate the efficiency drawbacks while increasing expressivity and performance!</p> <p>Thus, we distill the Phi-1.5 model into a Mamba-Attention hybrid model that maintains only four quadratic Attention layers. The remaining layers use the Mamba-2 layer variant also used in our Phi-Mamba model. Trained with 5B tokens using the MOHAWK method, our model achieves an average score of $66.0$ on downstream metrics, <strong>outperforming Phi-Mamba</strong>’s $65.1$ and <strong>approaching Phi-1.5</strong>’s $67.2$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance.png" width="100%" height="auto" title="Hybrid-Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance of Hybrid-Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <p>Our Hybrid-Phi-Mamba model is <strong>performs comparably</strong> to strong Attention-Mamba hybrids at the 1.5B range <strong>while using less Attention layers</strong> than Samba (12) and Mamba-SWA-MLP (18). We find that interleaving the Attention layers with the Mamba layers resulted in the strongest model, an observation that was also seen in <d-cite key="Samba"></d-cite>. We also find that increasing the number of Attention layers improved performance.</p>]]></content><author><name>Aviv Bick*</name></author><summary type="html"><![CDATA[[Paper] [Code]]]></summary></entry><entry><title type="html">Hydra Part I - Matrix Mixer Framework</title><link href="https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer/" rel="alternate" type="text/html" title="Hydra Part I - Matrix Mixer Framework"/><published>2024-07-16T00:00:00+00:00</published><updated>2024-07-16T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2407.09941">Paper</a>] [<a href="https://github.com/goombalab/hydra">Code</a>]</p> <ol> <li>Part I - Matrix Mixer Framework</li> <li><a href="/blog/2024/hydra-part2-model/">Part II - Hydra: The Model</a></li> </ol> <p>Attention mechanisms<d-footnote>In this work, Attention<d-cite key="attention"></d-cite> exclusively refers to Self-Attention<d-cite key="transformer"></d-cite></d-footnote> have taken center stage in the world of sequence mixing, celebrated for their significant flexibility and performance. However, this power comes with a price: high computational and memory demands. Despite these challenges, attention has become the go-to solution for many applications.</p> <p>In modern state-of-the-art models, architectural designs typically split into two main components: the sequence mixer and the channel mixer. To illustrate, let’s look at the Transformer encoder architecture. It consists of two key elements: Multi-Head Attention and a Feed-Forward Network (FFN). The Multi-Head Attention serves as the sequence mixer, efficiently managing interactions across the input sequence. Meanwhile, the FFN acts as the channel mixer, processing information within each sequence element.</p> <p>Take a glance at the figure below to see this architecture in action. You’ll notice how these components work together to create the robust models we rely on today.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_mixer_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/matrix_mixer_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In our work, we study the large and important class of sequence mixers that can be represented as basic matrix multiplications: $\textbf{Y} = \textbf{M}\textbf{X}$. We call this approach <strong><em>the matrix mixer framework</em></strong>. This framework includes diverse and important classes of sequence models such as Attention, convolutions<d-cite key="ckconv"></d-cite><d-cite key="tnn"></d-cite>, and state-space models<d-cite key="s4"></d-cite><d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite>. For example, the typical self-attention mechanism, $\textbf{Y} = \text{softmax}(\textbf{Q}\textbf{K}^T)\textbf{V}$, can be seen as a special case where the matrix $\textbf{M}$ is defined as $\text{softmax}(\textbf{Q}\textbf{K}^T)$.</p> <p>Viewing sequence mixers through this lens has a significant advantage: designing new sequence mixers becomes a matter of finding the optimal matrix $\textbf{M}$. This perspective opens up a systematic way to explore and innovate in the field of sequence modeling.</p> <p>So, now the question is, what is a good $\textbf{M}$? Key desiderata for such a matrix would include:</p> <ul> <li>Efficiency: We want sub-quadratic matrix multiplication and parameterization to ensure our models run swiftly and handle long sequences with ease.</li> <li>Performance: The matrix mixer should match the high standards of Attention mechanisms in modeling diverse sequence data across various modalities.</li> <li>Flexibility: The solution should work well with sequences of different lengths (+ capable of both causal and bidirectional sequence modeling, which we will tackle in <a href="/blog/2024/hydra-part2-model/">Part II</a>)</li> </ul> <p>Check out the table below to see how various sequence mixers measure up. While several models like MLP-Mixer<d-cite key="mlpmixer"></d-cite>, FNet<d-cite key="fnet"></d-cite>, TNN<d-cite key="tnn"></d-cite>, LA<d-cite key="la"></d-cite>, and M2<d-cite key="m2"></d-cite> have been introduced, none of them fully meet all our criteria.</p> <table> <thead> <tr> <th> </th> <th>Sub-quadratic</th> <th>Performance</th> <th>Flexibility</th> </tr> </thead> <tbody> <tr> <td>MLP-Mixer</td> <td>😭</td> <td>😭</td> <td>😭</td> </tr> <tr> <td>FNet</td> <td>🤗</td> <td>😭</td> <td>🤗</td> </tr> <tr> <td>TNN</td> <td>🤗</td> <td>😭</td> <td>🤗</td> </tr> <tr> <td>LA</td> <td>🤗</td> <td>😭</td> <td>🤗</td> </tr> <tr> <td>M2</td> <td>🤗</td> <td>😭</td> <td>😭</td> </tr> <tr> <td>Transformer</td> <td>😭</td> <td>🤗</td> <td>🤗</td> </tr> </tbody> </table> <p>As you can see, each of these models has its strengths and weaknesses, but none perfectly hit all the marks. This gap highlights the need for another approach in developing sequence mixers.</p> <blockquote> <p><strong>So, is it even possible to meet all three key criteria?</strong></p> </blockquote> <p>We believe the answer lies in examining <strong><em>the structures</em></strong> of the mixer matrix $\textbf{M}$. Our work begins with an in-depth theoretical and empirical analysis of various sequence mixers using the matrix mixer framework. We then extend this idea, offering a systematic approach to designing new sequence mixers. By fully leveraging this framework, we have developed <strong>multiple</strong> novel architectures, including a new bidirectional mixer named <strong><em>Hydra</em></strong>.</p> <p>Let’s dive into more details, which is outlined as follows:</p> <ul> <li>We study and formalize the matrix mixer framework, introducing new theoretical concepts about structures of $\textbf{M}$ that can capture such desiderata.</li> <li>Guided by the properties of different matrix classes, we introduce a series of sequence models with strong and predictable performance.</li> <li>We provide careful systematic studies on these matrix classes, comparing empirical performances by varying only the matrix mixer</li> </ul> <h2 id="formalization-of-the-matrix-mixer-framework">Formalization of the Matrix Mixer Framework</h2> <p>We begin by further formalizing our matrix mixer framework. While this framework can be applied to multi-head architectures, we will focus on the single-headed scenario here for simplicity.</p> <p>In essence, a sequence mixer transforms an input $\textbf{X} \in \mathbb{R}^{L \times C}$ into an output $\textbf{Y} \in \mathbb{R}^{L \times C}$, where $L$ is the sequence length and $C$ is the number of channels.</p> <ol> <li>Input preprocessing function: Denoted as $f_X \colon \mathbb{R}^{L \times C} \rightarrow \mathbb{R}^{L \times D}$, this function handles common data transformations before the mixing process.</li> <li>Matrix construction function: Denoted as $f_{\mathcal{M}} \colon \mathbb{R}^{L \times C} \times \Theta \rightarrow \mathcal{M} \subseteq \mathbb{R}^{L \times L}$, this function maps input data to mixer matrices. Here, $\Theta$ represents the space of learnable parameters, and $\mathcal{M}$ represents the class of mixer matrices.</li> </ol> <p>Given these functions, we denote the mixer matrix as $\textbf{M} = f_{\mathcal{M}}(\textbf{X}, \theta)$. The matrix mixer framework is then defined by the equation: \(\textbf{Y} = \textbf{M} (f_X(\textbf{X})).\)</p> <p>Using this framework, we are now playing a game of finding the optimal $\textbf{M}$ that satisfies all three requirements: efficiency, performance, and flexibility! This systematic approach allows us to analyze the characteristics of different sequence mixers and formalize the properties needed to meet our criteria.</p> <p>Let’s break down these objectives step-by-step and explore which matrices work best in achieving them.</p> <h2 id="solution-for-sub-quadratic-complexity-structured-matrices">Solution for Sub-quadratic Complexity: Structured Matrices</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_classes_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_classes_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_classes_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/matrix_classes_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To meet our first key requirement – sub-quadratic matrix multiplication – we can focus on a special type of matrices known as <strong>structured matrices</strong>. For a general matrix $\textbf{M}$, matrix multiplication typically incurs a computational cost of $O(L^2)$. However, structured matrices, with their compressed representation, allow us to perform these operations much more efficiently, achieving sub-quadratic complexity. We refer to sequence mixers using these matrices as <strong><em>structured matrix mixers</em></strong>.</p> <p>Structured matrices provide a broad array of options for our matrix mixer $\mathcal{M}$, as illustrated in the figure above. By leveraging these matrices, we can significantly reduce computational overhead while maintaining an efficient parameter count.</p> <p>All previous versions of sub-quadratic sequence mixers fit within the matrix mixer framework. This categorization by the class of mixer matrices helps us systematically analyze and understand the strengths and weaknesses of different approaches.</p> <details><summary>Notations</summary> <p>Think of bold capital letters like $\textbf{X}$ as matrices, bold small letters like $\textbf{x}$ as vectors, and regular small letters like $x$ as scalars. When we talk about elements in a matrix, we’ll use subscripts. So, if we have a matrix $\textbf{X} \in \mathbb{R}^{M \times N}$, the element in the $i$-th row and $j$-th column is $x_{ij}$. If we’re looking at the whole $i$-th row, it’s $\textbf{x}_i$.</p> </details> <table> <thead> <tr> <th>Matrix Structure $\mathcal{M}$</th> <th>Formulation (\(𝑚_{ij}\))</th> <th>Complexity</th> <th>Method Instantiations</th> </tr> </thead> <tbody> <tr> <td>Dense</td> <td>$m_{ij}$</td> <td>$O(L^2)$</td> <td>MLP-Mixer<d-cite key="mlpmixer"></d-cite></td> </tr> <tr> <td>Dense (Softmax Attention)</td> <td>$\text{softmax}_j(q^T_i k_j)$</td> <td>$O(L^2)$</td> <td>Transformer<d-cite key="transformer"></d-cite></td> </tr> <tr> <td>Low-rank (Linear Attention)</td> <td>$q^T_i k_j$</td> <td>$O(L)$</td> <td>Linear Attention<d-cite key="la"></d-cite>, Linformer<d-cite key="linformer"></d-cite></td> </tr> <tr> <td>Butterfly</td> <td>Refer to <d-cite key="kaleidoscope"></d-cite><d-cite key="monarch"></d-cite></td> <td>$O(L \log L)$</td> <td>Kaleidoscope<d-cite key="kaleidoscope"></d-cite>, Monarch<d-cite key="monarch"></d-cite></td> </tr> <tr> <td>Toeplitz (Convolution)</td> <td>$m_{j-i}$</td> <td>$O(L \log L)$</td> <td>S4<d-cite key="s4"></d-cite>, H3<d-cite key="h3"></d-cite>, TNN<d-cite key="tnn"></d-cite>, CKConv<d-cite key="ckconv"></d-cite></td> </tr> <tr> <td>Discrete Fourier Transform</td> <td>$w^{ij}$</td> <td>$O(L \log^2 L)$</td> <td>FNet<d-cite key="fnet"></d-cite></td> </tr> <tr> <td>Semiseparable</td> <td>\(\textbf{c}^T_i \textbf{A}^{\times}_{i:j} \textbf{b}_j \mathbb{1}_{\{i \geq j\}}\)</td> <td>$O(L)$</td> <td>Mamba (S6, SSD) <d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite></td> </tr> </tbody> </table> <p>As shown in the table above, using structured matrices (all but the dense variants) as the mixer matrix directly leads to sub-quadratic computational complexity.</p> <h2 id="solution-for-all-desiderata-sequence-aligned-matrices">Solution for All Desiderata: Sequence Aligned Matrices</h2> <p>So, can we simply choose any structured matrix as our sequence mixer matrix and expect it to meet all our requirements for efficiency, performance, and flexibility? Unfortunately, not all structured matrix mixers are up to the task. This begs the question: Is there a class of mixer matrices that can satisfy all three requirements? Fortunately, the answer is yes!</p> <p>We introduce a special subset of structured matrices called <strong><em>Sequence Aligned Matrices (SAM)</em></strong>. SAMs are designed to achieve efficiency, high performance, and flexibility all at once.</p> <h4 id="what-are-sequence-aligned-matrices-sam">What are Sequence Aligned Matrices (SAM)?</h4> <p>In simple terms, SAMs ensure that the parameters for every submatrix $\textbf{M}[: i+1, : i+1]$ are only functions of the tokens up to index $i$. Here is a formal definition of SAM.</p> <details><summary>Formal definition of Sequence Alignment</summary> <p><strong>Definition</strong> <em>(Sequence Aligned Matrices)</em> Let $L$ be the sequence length and let $\textbf{M} \in \mathbb{R}^{L \times L}$ denote a matrix with a parameter set $\mathcal{P}$. Then, we say that $\textbf{M}$ is a Sequence Aligned Matrix if there exists a partition $\Pi$ of $\hat{\mathcal{P}} \subseteq \mathcal{P}$, and $\hat{\mathcal{P}} \neq \phi$, such that for all sets $\mathcal{E} \in \Pi$, there exists a bijective map $f_{\mathcal{E}} : [L] \rightarrow \mathcal{E}$, and, for each $i \in [L]$, the sub-matrix $\textbf{M}[:i+1,:i+1]$ is composed solely from the parameters in the subset $\cup_{\mathcal{E}, k \le i} f_{\mathcal{E}}(k) \subseteq \mathcal{P}$.</p> </details> <h4 id="properties-of-sam">Properties of SAM</h4> <p>SAM matrices come with two crucial properties that make them stand out:</p> <ul> <li><strong>Data Dependency</strong>: SAM matrices are dynamically generated from the input data. This means they adapt in real-time based on the information they process.</li> <li><strong>Extendability</strong>: SAM matrices can handle inputs of arbitrary lengths, making them versatile for various applications.</li> </ul> <p>Take, for instance, the Attention mechanism in Transformers. It’s a perfect example of a SAM matrix: the Query-Key-Value components are all dynamically projected from the input data, and the mechanism itself adapts seamlessly to different sequence lengths.</p> <p>These two properties are not just nice-to-haves; they are essential for the flexibility and performance of modern models. Our experimental results strongly highlight the necessity of SAM, showing that SAM-based mixer matrices significantly enhance the performance of models.</p> <h3 id="sam-variations">SAM Variations</h3> <p>Let’s dive into a series of new SAM-based models we developed: <em>Toeplitz, Cauchy, Vandermonde, and quasiseparable</em> sequence mixers. By making these mixer matrices SAM, we achieved significant improvements. To make this explanation easier, we’ll assume that Query-Key-Value are projected from an input sequence.</p> <h4 id="cauchy-code">Cauchy <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/cauchy.py">(Code)</a></h4> <p>We begin with our Cauchy variant, as it shares a significant similarity with the Attention mechanism: the norm of $m_{ij}$ represents the magnitude of correlations between the $i$-th and $j$-th tokens. Following the definition of Cauchy matrices, our SAM Cauchy mixer works as follows:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d} \frac{1}{(q_{id} - k_{jd} + c)} \space, \end{equation}\] <p>where $\textbf{Q}, \textbf{K} \in \mathbb{R}^{L \times D}$, and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected matrices from $\textbf{X}$, and $c$ is a trainable constant that stabilizes training by preventing divide-by-zero errors.</p> <h4 id="vandermonde-code">Vandermonde <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/vandermonde.py">(Code)</a></h4> <p>Recall the definition of Vandermonde matrices: $m_{rs} = (m_r)^s$. Due to the exponential values, this can lead to instability during training. Therefore, we use the formulation $q_{rs} = \mathfrak{R}(e^{i \cdot r \cdot q_s})$ and $k_{rs} = \mathfrak{R}(e^{i \cdot s \cdot k_r})$ for $\textbf{Q}$ and $\textbf{K}$. This technique, taking the real part of complex numbers, is commonly used in SSMs. Under the same setting as our SAM Cauchy mixer, our SAM Vandermonde mixer $\textbf{M}$ is parameterized as:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d}(\cos(2 \pi q_{id}^j) - \cos(2 \pi k_{jd}^i)) \space, \end{equation}\] <p>where the cosine function comes from <a href="https://en.wikipedia.org/wiki/Euler's_formula">Euler’s formula</a>.</p> <h4 id="toeplitz-code">Toeplitz <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/toeplitz.py">(Code)</a></h4> <p>A Toeplitz matrix mixer is inherently a convolution between weights $\textbf{w} \in \mathbb{R}^{2L-1}$ and an input sequence $\textbf{V} \in \mathbb{R}^{L \times C}$. Usually, a general convolution adopts input-independent $\textbf{w}$, which does not satisfy the definition of SAM. Therefore, we extend our Toeplitz matrix mixer to be SAM as follows:</p> \[\begin{equation} \textbf{Y} = \mathcal{F}^{-1}(\mathcal{F}_\textbf{w} \odot \mathcal{F}_\textbf{V}), \qquad \qquad \textbf{w}_{i} = \begin{cases} q_{i-L+1} &amp; \text{if } i \geq L \\ k_{L-i+1} &amp; \text{if } i \lt L \\ \end{cases} \space , \end{equation}\] <p>where the convolution is implemented using FFT $\mathcal{F}$, and $\textbf{q}, \textbf{k} \in \mathbb{R}^{L}$ and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected from $\textbf{X}$.</p> <h4 id="quasiseparable-code">Quasiseparable <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/quasiseparable.py">(Code)</a></h4> <blockquote class="block-tip"> <p><strong>This variant has a separate name, Hydra. Stay tuned for <a href="/blog/2024/hydra-part2-model/">Part II</a> 🤭</strong></p> </blockquote> <h2 id="impact-of-sam-parameterization">Impact of SAM Parameterization</h2> <p>Now, we validate that the SAM matrix mixers are better than non-SAM mixers. To prove this claim, we conducted strictly controlled systematic albations where the only variable was the mixer matrix. Check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixer.py">our efforts</a> for a comprehensive and fair comparison!</p> <table> <tr> <td style="font-weight:bold;">Structure</td> <td style="text-align:center;">Data Dependent</td> <td style="text-align:center;"># Params</td> <td style="text-align:center;">GLUE Avg</td> <td style="text-align:center;">Δ</td> </tr> <tr> <td style="font-weight:bold;">Dense</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.7</td> <td style="text-align:center;"></td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Toeplitz</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">75.8</td> <td rowspan="2" style="text-align:center;">+1.9</td> </tr> <tr> <td style="text-align:center;">✅</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">77.7</td> </tr> <tr> <td style="font-weight:bold;">DFT</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.7</td> <td rowspan="3" style="text-align:center;">+5.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Vandermonde</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">70.8</td> </tr> <tr> <td style="text-align:center;">✅</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">76.0</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Cauchy</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.2</td> <td rowspan="2" style="text-align:center;">+4.0</td> </tr> <tr> <td style="text-align:center;">✅</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Low-rank</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.9</td> <td rowspan="2" style="text-align:center;">+3.5</td> </tr> <tr> <td style="text-align:center;">✅</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.4</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Attention</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.9</td> <td rowspan="2" style="text-align:center;">+6.9</td> </tr> <tr> <td style="text-align:center;">✅</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.8</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Quasiseparable</td> <td style="text-align:center;">❌</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">75.1</td> <td rowspan="2" style="text-align:center;">+4.6</td> </tr> <tr> <td style="text-align:center;">✅</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">79.7</td> </tr> </table> <p>The results in the table above clearly demonstrate the importance of SAM. Regardless of the matrix class, incorporating the SAM property always leads to a significant performance boost. Additionally, our SAM-based Toeplitz, Cauchy, and low-rank mixers perform remarkably well, with quasiseparable mixers even surpassing Attention. These findings underscore the immense potential of structured matrix mixers as efficient yet powerful sequence mixers.</p> <h2 id="next-up">Next Up</h2> <p>Curious about the quasiseparable matrix mixer? In <a href="">the next part</a>, we’ll introduce Hydra, our bidirectional extension of SSMs that not only surpasses Attention but also achieves sub-quadratic complexity. Stay tuned!</p>]]></content><author><name>Sukjun Hwang*</name></author><summary type="html"><![CDATA[[Paper] [Code]]]></summary></entry><entry><title type="html">Hydra Part II - The Model</title><link href="https://goombalab.github.io/blog/2024/hydra-part2-model/" rel="alternate" type="text/html" title="Hydra Part II - The Model"/><published>2024-07-16T00:00:00+00:00</published><updated>2024-07-16T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/hydra-part2-model</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/hydra-part2-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/logo_trans-480.webp 480w,/assets/img/2024-07-16-hydra/logo_trans-800.webp 800w,/assets/img/2024-07-16-hydra/logo_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/logo_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2407.09941">Paper</a>] [<a href="https://github.com/goombalab/hydra">Code</a>]</p> <ol> <li><a href="/blog/2024/hydra-part1-matrix-mixer/">Part I - Matrix Mixer Framework</a></li> <li>Part II - Hydra: The Model</li> </ol> <p>In our previous post, we systematically compared various sequence models with different mixer matrices, and the quasiseparable SAM mixer emerged as the top performer. So, what exactly is it?</p> <h2 id="recap-ssms-are-semiseparable-matrix-mixers">Recap: SSMs Are Semiseparable Matrix Mixers</h2> <p>Before diving into the details of quasiseparable SAM mixers, let’s briefly revisit some key findings from <a href="https://arxiv.org/abs/2405.21060">Mamba-2</a><d-cite key="ssd"></d-cite>. Recently, Mamba-2 has shown that the mixer matrices of SSMs are inherently parametrized to one of the fundamental structured matrix classes – semiseparable matrices.</p> <blockquote> <p><strong>Defintion</strong> of Semiseparable Matrices <br/> A lower triangular matrix $\textbf{M}$ is $N$-semiseparable iff any submatrix from the lower triangle (on or below the diagonal) has a rank of at most $N$. See (a) in the figure below.</p> </blockquote> <p>So why are SSMs semiseparable matrix mixers? Using our previously defined matrix mixer framework, we can represent SSMs as follows:</p> \[\begin{align} \textbf{y}_t &amp;= \sum^{t}_{s=0} \textbf{C}^T_t \left(\prod_{k=s+1}^{i} \textbf{A}_{k}\right) \textbf{B}_s \textbf{x}_s \\ \\ \textbf{Y} &amp;= \text{SSM}(\textbf{A}, \textbf{B}, \textbf{C})(\textbf{X}) = \textbf{M} \textbf{X} \space ,\\ \\ m_{ij} &amp; = \textbf{c}^T_i \textbf{A}_i \cdots \textbf{A}_{j+1} \textbf{b}_j \end{align}\] <p>where each matrix $\textbf{A}_i \in \mathbb{R}^{N \times N}$ and vector $\textbf{c}_i, \textbf{b}_i \in \mathbb{R}^{N \times 1}$. This decomposition shows that SSMs are indeed semiseparable mixers. [If you are not familiar with this concept, we recommend checking out this <a href="/blog/2024/mamba2-part2-theory/">blog post</a> for a great explanation.]</p> <p>Semiseparable matrices are an excellent choice for mixer matrices – they are sub-quadratic, performant, and can be extended to handle sequences of various lengths. However, there’s one significant limitation: due to their definition, the upper right triangle of semiseparable matrices is filled with zeros, making them inevitably causal. This limitation makes SSMs incapable of <strong>bidirectional sequence processing</strong>.</p> <p>Why is bidirectionality important? Bidirectional processing is crucial for several reasons. One major reason is its importance in handling multiple modalities, such as processing 2D images. Without bidirectionality, models can’t fully leverage information from both past and future contexts within a sequence, which is essential for comprehensive data analysis across various applications.</p> <p>A straightforward way to make SSMs bidirectional is to use two separate SSMs: one for forward sequence processing and one for reverse sequence processing. There are several approaches to combine their outputs, such as adding, multiplying, or concatenating them <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite><d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite>. While these heuristics can work, they lack a principled design philosophy, leading to different heuristics being used for different tasks without a systematic approach.</p> <p>But what if we could use the matrix mixer framework to systematically derive the optimal $\textbf{M}$? Absolutely, we can! In addition to the three desiderata we discussed previously – sub-quadratic complexity, extendability, and high-performance – let’s add one more requirement: <strong>bidirectionality</strong>. For the mixer matrix to achieve bidirectionality, it must feature upper triangular components. So, how should we fill them?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/semiquasi_trans-480.webp 480w,/assets/img/2024-07-16-hydra/semiquasi_trans-800.webp 800w,/assets/img/2024-07-16-hydra/semiquasi_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/semiquasi_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="structured-matrix-of-our-choice-quasiseparable-matrices">Structured Matrix of Our Choice: Quasiseparable Matrices</h2> <p>For our bidirectional sequence mixer, we choose quasiseparable matrices. So, what makes quasiseparable matrices stand out? Let’s start by looking at their definition.</p> <blockquote> <p><strong>Defintion</strong> of Quasiseparable Matrices by the Rank Characterization. <br/> A matrix $\textbf{M}$ is $N$-quasiseparable iff any submatrix from either the strictly upper or lower triangle (off from the diagonal) has a rank of at most $N$. See (b) in the figure above.</p> </blockquote> <p>At first glance, this definition might seem similar to that of semiseparable matrices. To clarify, let’s highlight the key differences between quasiseparable and semiseparable matrices:</p> <table> <thead> <tr> <th> </th> <th><strong>Semiseparable</strong></th> <th><strong>Quasiseparable</strong></th> </tr> </thead> <tbody> <tr> <td>(I)</td> <td>any submatrix from <em>the lower triangle</em></td> <td>any submatrix from either the strictly <em>upper or lower triangle</em></td> </tr> <tr> <td>(II)</td> <td><em>on or below</em> the diagonal</td> <td><em>off</em> from the diagonal</td> </tr> </tbody> </table> <h3 id="quasiseparable-matrices-supset-semiseparable-and-low-rank-matrices">Quasiseparable Matrices $\supset$ Semiseparable and Low-Rank Matrices</h3> <p>Although the differences between quasiseparable and semiseparable matrices might seem subtle, they lead to significant improvements in expressivity. According to difference <strong>(I)</strong>, semiseparable matrices zero out the upper triangular elements, while quasiseparable matrices extend to include these elements, enabling bidirectionality. Consequently, semiseparable matrices can only generalize mixers that use causal low-rank matrices, such as Linear Attention, whereas quasiseparable matrices generalize typical low-rank matrices. Moreover, both differences <strong>(I)</strong> and <strong>(II)</strong> mean that quasiseparable matrices not only generalize but also extend semiseparable matrices.</p> <ul> <li><strong><em>Quasiseparable matrices generalize low-rank matrices.</em></strong></li> <li><strong><em>Quasiseparable matrices generalize and extend semiseparable matrices.</em></strong></li> </ul> <h3 id="quasiseparable-matrices-supset-two-separate-ssms">Quasiseparable Matrices $\supset$ Two Separate SSMs</h3> <p>We now understand that for bidirectional processing scenarios, quasiseparable mixers are indeed better than semiseparable matrices. But what makes quasiseparable mixers superior to the bidirectional extensions using two separate SSMs?</p> <p>Heuristic variants that use the Hadamard product and concatenation <d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite> are difficult to analyze systematically within the matrix mixer framework. Moreover, concatenation variants double the number of output channels, necessitating additional parameters for reducing the number of channels.</p> <p>In contrast, addition-based variants <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite> can be formulated using the matrix mixer framework, as shown in (c) of the figure above, which resembles quasiseparable matrices in (d). However, difference <strong>(II)</strong> highlights that the diagonals of semiseparable matrices are also constrained by the rank characterization, and consequently, so are the diagonals of addition-based extensions. Quasiseparable matrices, on the other hand, do not have this constraint on the diagonals, allowing them to be complete free parameters. This flexibility makes quasiseparable matrices more mathematically expressive than addition-based bidirectional extensions.</p> <ul> <li><strong><em>Quasiseparable matrices are strictly more expressive than mixer matrices of addition-based bidirectional SSMs.</em></strong></li> </ul> <p>This property of complete freedom in the diagonals of quasiseparable matrices is more evident in another definition of quasiseparable matrices:</p> <blockquote> <p>A matrix $\textbf{M}$ is $N$-quasiseparable if each element $m_{ij}$ satisfies:</p> \[\begin{equation} m_{ij} = \begin{cases} \overrightarrow{\textbf{c}^{T}_{i}} \overrightarrow{\textbf{A}_i} \cdots \overrightarrow{\textbf{A}_{j+1}} \overrightarrow{\textbf{b}_{j}}, &amp; \text{if } i &gt; j \\ \delta_{i}, &amp; \text{if } i = j \\ \overleftarrow{\textbf{c}^{T}_{i}} \overleftarrow{\textbf{A}_{i}} \cdots \overleftarrow{\textbf{A}_{j-1}} \overleftarrow{\textbf{b}_{j}}, &amp; \text{if } i &lt; j\\ \end{cases},\\ \end{equation}\] <p>where each $\delta_i$ is a scalar, $\textbf{b}_i, \textbf{c}_i \in \mathbb{R}^{N \times 1}$, and $\textbf{A}_i \in \mathbb{R}^{N \times N}$.</p> </blockquote> <p>These are the actual results we obtained for the C4 and GLUE benchmark, along with the validation loss curve. Supported by these theoretical claims, our Hydra model, which uses a quasiseparable mixer matrix, indeed has shown superior performance to previous heuristic bidirectional extensions!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/bidirectionality_trans-480.webp 480w,/assets/img/2024-07-16-hydra/bidirectionality_trans-800.webp 800w,/assets/img/2024-07-16-hydra/bidirectionality_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/bidirectionality_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hydra-our-main-bidirectional-sequence-mixer">Hydra: Our Main Bidirectional Sequence Mixer</h2> <h3 id="implementation">Implementation</h3> <p>Now that we’ve confirmed quasiseparable matrices as the go-to mixer matrices, we fully leverage them to propose the two-headed Mamba – <strong><em>Hydra</em></strong>. Take a look at part (d) in the figure above, which illustrates the mixer matrix of Hydra, and also notice it’s also our previosly defined SAM! Utilizing an SSM, which is a semiseparable mixer, we can implement Hydra with the following formula: \(QS(\textbf{X}) = \texttt{shift}(SS(\textbf{X})) + \texttt{flip}(\texttt{shift}(SS(\texttt{flip}(\textbf{X})))) + \textbf{DX},\) where $\textbf{X}$ is the input sequence, $\texttt{flip}(\cdot)$ denotes a function that reverses the input, $\texttt{shift}(\cdot)$ denotes a right-shift function, and $\textbf{D} = \text{diag}(\delta_1, \cdots, \delta_L)$ represents the diagonal elements of $QS$. Here, $QS(\cdot)$ and $SS(\cdot)$ are the mixer matrix of Hydra and an SSM, respectively.</p> <p>Among the various iterations of SSMs, we adopt the latest one – SSD from Mamba-2. Since SSMs are sub-quadratic, this simple implementation maintains the sub-quadratic cost. Compared to heuristic extensions that use two separate SSMs for bidirectionality, Hydra shares the input processing function $f_X$ for forward and reverse sequence processing, which nearly halves the number of parameters.</p> <p>You can check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py">the actual code</a>. To sum up:</p> <ul> <li>Hydra’s matrix mixer is meticulously parameterized to be a quasiseparable matrix with enhanced expressivity through shift operations.</li> <li>Hydra is sub-quadratic and super easy to implement using existing SSM implementations like Mamba.</li> <li>Hydra greatly reduces parameter counts compared to bidirectional extensions using two SSMs.</li> </ul> <h3 id="performance">Performance</h3> <p>We have seen that Hydra outperforms heuristic bidirectional extensions of SSMs, but how does it compare to state-of-the-art methods? Surprisingly, Hydra surpasses all previous models, including Transformer-based models such as BERT and ViT. When matched for the number of parameters, Hydra consistently shows the best performance across both NLP and Vision domains, highlighting its versatility.</p> <table> <tr> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #4dabf7">NLP</td> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #69db7c">Vision</td> </tr> <tr> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">GLUE Avg</td> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">Top-1 (%)</td> </tr> <tr> <td style="font-weight:bold;">BERT<d-cite key="bert"></d-cite></td> <td>110M</td> <td>83.5</td> <td style="font-weight:bold;">ViT-B<d-cite key="vit"></d-cite></td> <td>87M</td> <td>78.8</td> </tr> <tr> <td style="font-weight:bold;">MLP-Mixer<d-cite key="mlpmixer"></d-cite></td> <td>112M</td> <td>77.5</td> <td style="font-weight:bold;">S4-ViT-B<d-cite key="s4"></d-cite><d-cite key="s4d"></d-cite></td> <td>89M</td> <td>79.4</td> </tr> <tr> <td style="font-weight:bold;">FNet<d-cite key="fnet"></d-cite></td> <td>112M</td> <td>75.8</td> <td style="font-weight:bold;">Hyena-ViT-B<d-cite key="hyena"></d-cite></td> <td>88M</td> <td>78.4</td> </tr> <tr> <td style="font-weight:bold;">M2<d-cite key="m2"></d-cite></td> <td>116M</td> <td>80.9</td> <td style="font-weight:bold;">Mamba-ViT-B<d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite></td> <td>89M</td> <td>79.1</td> </tr> <tr> <td style="background-color: #f783ac; font-weight:bold;">Hydra</td> <td>112M</td> <td>84.3</td> <td style="background-color: #f783ac; font-weight:bold;">Hydra-ViT-B</td> <td>91M</td> <td>81.0</td> </tr> </table> <p>On the GLUE benchmark, Hydra outperforms BERT by 0.8 points. On ImageNet-1K, Hydra improves by 2.2 points over ViT. These results underscore Hydra’s capability to set new standards in both natural language processing and image classification tasks!</p> <h2 id="epilogue">Epilogue</h2> <p>Lately, the demand for large-scale computation has never been higher. Since the emergence of Mamba, interests in structured matrices has surged, and now is their time to shine. Structured matrices offer an exciting approach to efficient and powerful input processing, similar to how M2 improved over MLP-Mixer.</p> <p>In recent years, we’ve seen numerous groundbreaking works showcasing promising results using structured matrices like Mamba. If the community strives together, just as we have spent about seven years investigating and improving Transformers, we believe there is enormous potential for further advancements through systematic exploration of different structured matrices, along with better optimized training settings (which have been fine-tuned for Transformers).</p> <p>A big shout-out to the recent <a href="https://arxiv.org/abs/2406.06248">BTT</a><d-cite key="btt"></d-cite> work, which systematically explores structured matrices for effective channel mixers. We were very excited to see this kind of systematic investigation, which is crucial for the continued advancement of better architectures.</p>]]></content><author><name>Sukjun Hwang*</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part I - The Model</title><link href="https://goombalab.github.io/blog/2024/mamba2-part1-model/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part I - The Model"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part1-model</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part1-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2405.21060">Paper</a>] [<a href="https://github.com/state-spaces/mamba">Code</a>]</p> <p><strong>This series is cross-posted at <a href="https://tridao.me/blog/2024/mamba2-part1-model/">tridao.me</a></strong></p> <ol> <li>Part I - The Model</li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>Since the release of <a href="https://arxiv.org/abs/2312.00752">Mamba</a> 6 months ago, we’ve been pleasantly surprised by the overwhelming <a href="https://github.com/AvivBick/awesome-ssm-ml">community response</a>. It’s been incredibly gratifying to see the line of research on efficient sequence models we’ve been pursuing for years really resonate with the machine learning community and take off more than we could have anticipated. We’ve seen an enormous amount of exciting follow-up work, from direct applications (e.g. vision <d-cite key="zhu2024vision"></d-cite><d-cite key="ma2024u"></d-cite><d-cite key="liu2024vmamba"></d-cite>, genomics <d-cite key="schiff2024caduceus"></d-cite>, graphs <d-cite key="wang2024graph"></d-cite><d-cite key="behrouz2024graph"></d-cite>, and more) to understanding (e.g. on recall abilities <d-cite key="jelassi2024repeat"></d-cite>, in-context learning<d-cite key="akyurek2024context"></d-cite> <d-cite key="grazzi2024mamba"></d-cite> <d-cite key="park2024can"></d-cite>, and formal language expressivity <d-cite key="merrill2024illusion"></d-cite><d-cite key="sarrof2024expressive"></d-cite>), and an enormous number of <a href="https://jackcook.com/2024/02/23/mamba.html">online</a> <a href="https://srush.github.io/annotated-mamba/hard.html">blogs</a>, <a href="https://www.youtube.com/watch?v=dVH1dRoMPBc">tutorials</a>, <a href="https://www.youtube.com/watch?v=8Q_tqwpTpVU">and</a> <a href="https://www.youtube.com/watch?v=N6Piou4oYx8">videos</a>. We couldn’t be more excited about the direction of this research!</p> <p>Yet despite its potential so far, we weren’t completely satisfied with the first version of Mamba…</p> <h3 id="problem-1-understanding">Problem 1 (Understanding)</h3> <p>From a conceptual standpoint, one of the reasons we found SSMs so fascinating is how they just feel <em>fundamental</em>. One way this is exemplified is how they have rich ties to many major paradigms of sequence models. As developed in our earlier works on structured SSMs <d-cite key="gu2021combining"></d-cite><d-cite key="gu2023thesis"></d-cite>, they seem to capture the essence of continuous, convolutional, and recurrent sequence models – all wrapped up in a simple and elegant model.</p> <p>But of course, aside from these, there’s another major sequence model paradigm: variants of the ubiquitous <strong>attention</strong> mechanism<d-cite key="bahdanau2015neural"></d-cite><d-cite key="vaswani2017attention"></d-cite>. SSMs always felt somewhat disjoint from attention, and we’ve tried for a while to understand their relationship better.</p> <blockquote> <p>Question 1: <strong>What are the conceptual connections between state space models and attention?</strong> Can we combine them?</p> </blockquote> <h3 id="problem-2-efficiency">Problem 2 (Efficiency)</h3> <p>From a computational standpoint, despite the work that went into making Mamba fast (in particular, its hardware-aware selective scan implementation) it’s still much less hardware-efficient than mechanisms such as attention. The missing piece is that modern accelerators such as GPUs and TPUs are <em>highly</em> specialized for matrix multiplications. While this isn’t a problem for inference, which is bottlenecked by somewhat different considerations, this can be a big deal during training time.</p> <blockquote> <p>Question 2: <strong>Can we speed up the training of Mamba models by recasting them as matrix multiplications?</strong></p> </blockquote> <p>These are the main questions that Mamba-2 – in particular, its new state space model variant – tries to address.</p> <h2 id="the-ssd-model">The SSD Model</h2> <p>The main point of the Mamba-2 paper is what we call <strong>structured state space duality</strong> (SSD), which refers to several things:</p> <ol> <li>The <strong>SSD model</strong> refers to a specific standalone layer, like attention or an SSM, that can be incorporated into deep neural networks</li> <li>The <strong>SSD framework</strong> is a general framework for reasoning about this model (and many more theoretical connections)</li> <li>The <strong>SSD algorithm</strong> is an algorithm for computing SSD layers much more efficiently than previous SSMs</li> </ol> <p>The main SSD model or “state space dual model” itself really isn’t so complicated! In this first part of a series of blog posts, we’ll provide a self-contained description of the SSD layer (and Mamba-2) in isolation and how it compares to related models, particularly Mamba-1.</p> <p>In the next parts of this series, we’ll describe the general framework and theoretical connections, which aren’t necessary to actually use Mamba-2.</p> <h3 id="the-linear-ssm-mode">The Linear (SSM) Mode</h3> <p>SSD starts from the same set of equations as Mamba:</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} h_t \end{aligned}\] <p>\begin{equation} \label{eq:ssm} (\text{Selective state space model (SSM)}) \end{equation}</p> <p>To recap, a <strong>structured state space model (SSM)</strong> <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> defines a map from $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$. Think of $x_t$ and $y_t$ as being scalars, and the hidden state $h_t$ as an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the <em>state size, state dimension, or state expansion factor</em>.</p> <p>A <em>selective</em> state space model allows the $(A, B, C)$ SSM parameters to vary across time <d-cite key="gu2023mamba"></d-cite>. We’ll think of them as tensors with shapes $A \in \mathbb{R}^\mathtt{(T, N, N)}$, $B \in \mathbb{R}^\mathtt{(T, N)}$, and $C \in \mathbb{R}^\mathtt{(T, N)}$ respectively.<d-footnote>As with Mamba-1, we take everything over the reals $\mathbb{R}$, although complex variants as with other structured SSMs like the S4 lineage <d-cite key="gu2022efficiently"></d-cite> are also possible.</d-footnote></p> <p>Structured SSMs require $A$ to have structure to be efficiently computable, such as the most commonly used diagonal structure <d-cite key="gu2022parameterization"></d-cite><d-cite key="gupta2022diagonal"></d-cite><d-cite key="smith2023s5"></d-cite><d-cite key="gupta2022simplifying"></d-cite>. In this case $A$ has shape $\mathtt{(T, N)}$ where only the diagonal elements of the $\mathtt{N} \times \mathtt{N}$ matrices are stored.</p> <h4 id="ssd-scalar-structured-ssm">SSD: Scalar Structured SSM</h4> <p>The original Mamba (or more precisely its core “S6” layer) is exactly a selective SSM with diagonal structure.</p> <p><strong>The SSD layer of Mamba-2 makes only one small modification</strong>: it restricts the diagonal $A$ even further to a <em>scalar times identity</em> structure; in other words the diagonal elements of $A$ must all be the same value. In this case $A$ can be represented with shape just $\mathtt{(T)}$ and one can also identify $A_t$ as just a scalar (and so we’ll sometimes denote it $a_t$).</p> <h4 id="multihead-ssms">Multihead SSMs</h4> <p>Equation \eqref{eq:ssm} is defined only for a single dimensional input $x \in \mathbb{R}^\mathtt{T}$. If $X \in \mathbb{R}^\mathtt{(T, P)}$ has $\mathtt{P}$ separate channels, we can use the same dynamics (i.e. the same SSM $(A, B, C)$) independently for each channel. This can be interpreted as a <em>single head</em> of the SSM model.</p> <p>Here, we think of $X$ as a tensor of shape $\mathtt{(T, P)}$ where $\mathtt{T}$ is the sequence (time) dimension and $\mathtt{P}$ is the “head dimension”.<d-footnote>Normally there's an additional batch dimension $\mathtt{B}$ when implementing these models, which we'll ignore throughout this presentation.</d-footnote></p> <p>Multiple heads can be constructed completely independently; for the remainder of this post, we assume that we’re working with a single head. Note that these heads are exactly analogous to how heads in multi-head attention models work, and in Mamba-2 we also choose similar dimensions as modern Transformers, e.g. $\mathtt{P} = 64$ or $\mathtt{P}=128$. (To scale to larger model widths $\mathtt{D} = \mathtt{d\_model}$, we keep this fixed and increase the number of independent heads.)</p> <p>We can notate the general (selective) state space model as \begin{equation} \label{eq:ssm-transformation} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T,…)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>Some axes of variation include</p> <ol> <li>The structure on $A$, which affects its parameter shape: <ul> <li><code class="language-plaintext highlighter-rouge">... = (N,N)</code> for general (unstructured) SSMs</li> <li><code class="language-plaintext highlighter-rouge">... = (N)</code> for diagonal SSMs (or other structures, such as diagonal-plus-low-rank <d-cite key="gu2022efficiently"></d-cite>)</li> <li><code class="language-plaintext highlighter-rouge">... = ()</code> for scalar SSMs (i.e. SSD)</li> </ul> </li> <li>The state dimension $\mathtt{N}$ (i.e. <code class="language-plaintext highlighter-rouge">d_state</code>)</li> <li>The head dimension $\mathtt{P}$ (i.e. <code class="language-plaintext highlighter-rouge">d_head</code>)</li> </ol> <p>There are other axes of variation of structured SSMs (e.g. time-invariance vs. selectivity, SISO vs. MIMO<d-cite key="smith2023s5"></d-cite>, real vs. complex, etc.), but we’re highlighting these so that we can contrast Mamba-2 to Mamba-1 in just a second…</p> <h3 id="the-quadratic-attention-mode">The Quadratic (Attention) Mode</h3> <p>But first, let’s switch tacks and forget about state space models for a moment. Given the same tensors above with the same shapes $(A^\mathtt{(T)}, B^\mathtt{(T, N)}, C^\mathtt{(T, N)})$, let’s define a different object.</p> <p>First, we’ll define the following matrix (don’t worry, we’ll explain more and give it a name in Part II of this series!)</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix} .\] <p>Then, let’s define the following matrix</p> <p>\begin{equation} \label{eq:ssd-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Finally, $M$ encodes a <em>sequence transformation</em> $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$ mapping a 1D input to a 1D output—just as in equation \eqref{eq:ssm}—through basic matrix multiplication $y = Mx$.</p> <p>What’s special about this? Well, you may notice that it looks very similar to an attention computation. In fact, if all $a_t = 1$, then $L$ is simply the lower-triangular <em>causal mask</em> and \eqref{eq:ssd-attention} is equivalent to <strong>causal linear attention</strong> <d-cite key="katharopoulos2020transformers"></d-cite>:</p> \[Y = (L \circ Q K^\top) V\] <p>This is exactly the same as equation \eqref{eq:ssd-attention} if we rename $(C, B, X) \mapsto (Q, K, V)$!</p> <h2 id="state-space-duality">State Space Duality</h2> <p>The so-called “duality” refers to the fact that the two models defined in equations \eqref{eq:ssm} (for the scalar-identity structured $A_t$ case) and \eqref{eq:ssd-attention} are actually <em>exactly the same model</em>, which we can view as a particular function</p> \[(A^\mathtt{(T)}, B^\mathtt{(T, N)}, C^\mathtt{(T, N)}, X^\mathtt{(T, P)}) \mapsto Y^\mathtt{(T, P)}\] <p>In the general <em>SSD Framework</em> (Part II of this series), we’ll show this equivalence in two completely different ways, both of which are actually much more general and each quite illuminating.</p> <p>If you take our word for it, though, then SSD is relatively simple to contrast in relation to either SSMs or attention.</p> <h3 id="ssd-vs-state-space-models">SSD vs. State Space Models</h3> <p>Compared to previous SSMs, SSD is pretty much the same as the core layer of Mamba but with even more structure on the recurrent $A$ matrices.</p> <ol> <li>Mamba-1 (S6) uses diagonal structure on $A$, while Mamba-2 (SSD) uses scalar-times-identity structure on $A$.</li> <li>Mamba-1 has a head dimension of $\mathtt{P}=1$ (i.e. all channels are completely independently controlled by separate SSMs), while Mamba-2 uses a head dimension of $\mathtt{P}&gt;1$ (something like $\mathtt{P}=64$ by default).</li> </ol> <p>In particular, this can be viewed as weight-tied in two ways:</p> <ul> <li>By restricting the diagonal structure of $A$ to scalar-times-identity, the recurrence dynamics are shared across all $\mathtt{N}$ elements of the state space.</li> <li>These dynamics are also shared across all $\mathtt{P}$ channels of a given head.</li> </ul> <p>In other words, a single SSM head has total state size $\mathtt{P} \times \mathtt{N}$, which are each governed by separate scalar recurrences in Mamba-1 but are controlled by a single shared recurrence in Mamba-2.</p> <p>Why make these restrictions? The main motivation is efficiency: these changes are necessary to be able to view the model in its [<a href="#the-quadratic-attention-mode">dual attention form</a>], which allows matrix multiplications to be used.</p> <blockquote class="block-tip"> <h4 id="the-bottom-line-mamba-1-vs-mamba-2">The Bottom Line: Mamba-1 vs. Mamba-2</h4> <p>Compared to Mamba-1, Mamba-2 allows <strong>much larger state dimensions</strong> (from <code class="language-plaintext highlighter-rouge">N=16</code> in Mamba-1 to <code class="language-plaintext highlighter-rouge">N=64</code> to <code class="language-plaintext highlighter-rouge">N=256</code> or even higher in Mamba-2) while simultaneously being <strong>much faster during training</strong>.</p> </blockquote> <p>But can this hurt us? There’s some intuition to believe that it shouldn’t. One of the main reasons for the selectivity (e.g. $A$ that depends on the input $X$) introduced in Mamba is to let the SSM be able to control whether to remember or ignore particular pieces of information; for example, if a filler “um” is encountered in a text transcript. But if such information should be ignored, then the entire state can ignore it together, and so it should be okay if the state’s dynamics are shared across all features.</p> <p>Empirically, we haven’t found evidence that the restricted expressivity of Mamba-2 might hurt, but the jury’s still out! From one perspective, Mamba-2 isn’t <em>strictly</em> better than Mamba-1: while it’s a dramatic improvement from a <em>training</em> perspective, Mamba-1 might be better from a pure <em>inference</em> perspective. Since inference speed of SSMs is entirely governed by the state dimension, if one wants to maximize performance for a target inference efficiency (i.e. for a particular state size $\mathtt{N}$), then the increased expressivity of Mamba-1 might be better. We haven’t fully analyzed the (theoretical or empirical) tradeoffs here, and think this would be a cool direction for the community to dig in more!</p> <h3 id="ssd-vs-attention">SSD vs. Attention</h3> <p>Compared, to standard (self-)attention, SSD also only has two differences:</p> <ol> <li>The softmax normalization is dropped.</li> <li>A separate elementwise mask matrix is applied multiplicatively.</li> </ol> <p>The first difference can be interpreted as what reduces the effective state size of the model from linear to constant, and improves its efficiency from quadratic to linear.</p> <p>The second difference is what distinguishes SSD from standard linear attention. One way to think of the mask is as <strong>input-dependent relative positional encodings</strong>. Because of the mask $L$ in \eqref{eq:ssd-attention}, the standard attention score $\langle Q_i, K_j \rangle$ is attenuated by a weight</p> \[a_{i:j}^\times = a_i \cdots a_{j+1}\] <p>which can be interpreted as a “discount factor” based on how far apart the positions $i$ and $j$ are. (This interpretation was concurrently espoused by Tobias Katsch’s <a href="https://arxiv.org/abs/2311.01927">GateLoop</a> paper<d-cite key="katsch2023gateloop"></d-cite>.) In its attention form, this input-dependent positional mask can be interpreted as the key factor that encodes the “selectivity” of Mamba!</p> <h2 id="best-of-both-worlds">Best of Both Worlds</h2> <p>So why do we care that there are two views of this model? Well, first of all, it’s extremely mathematically interesting, as we’ll cover in <a href="/blog/2024/mamba2-part2-theory/">Part II</a>, and we hope will inspire future directions. But there are immediate practical benefits too!</p> <h3 id="efficiency-the-ssm-and-attention-modes">Efficiency: the SSM and Attention Modes</h3> <p>The SSM \eqref{eq:ssm} and attention \eqref{eq:ssd-attention} modes represent two different ways of computing the same function, so let’s contrast them.</p> <p>First, remember that one main reason why SSMs are interesting to begin with is because computing \eqref{eq:ssm} as a recurrence requires maintaining a <em>constant-size state</em> (size $\mathtt{N}$ per channel) and scales <em>linearly in the sequence length</em> $\mathtt{T}$. The downside is that the raw FLOPs don’t reflect actual speed in practice because of hardware considerations…</p> <p>On the other hand, computing this sequence transformation $y = Mx$ through equation \eqref{eq:ssd-attention} takes quadratic time in the sequence length, because we’re materializing this $\mathtt{T} \times \mathtt{T}$ matrix. But it can be fast in practice because it only uses matrix multiplications, which are extremely optimized on GPUs and TPUs.</p> <h3 id="efficiency-the-ssd-mode">Efficiency: the SSD Mode</h3> <p>So if there are two equivalent ways of computing the same model, when should we use one mode or the other? During inference, there’s no trade-off: the SSM mode is designed for fast autoregressive inference. But what about training? Here there’s a tension between FLOPs and hardware efficiency where the attention mode uses more FLOPs, but uses them more efficiently through matrix multiplications.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_algorithm-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_algorithm.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It turns out we can get the best of both worlds by combining the algorithms! There are two equivalent interpretations of this “state space dual” algorithm, either as</p> <ol> <li>A block decomposition of a particular structured matrix that defines the SSD “token-mixing” sequence transformation.</li> <li>A “chunkwise” algorithm that splits the sequence into segments, computes the quadratic attention form on each segment, and adjusts the result by passing the SSM states between segments.</li> </ol> <p>We’ll leave the details of this algorithm to <a href="/blog/2024/mamba2-part3-algorithm/">Part III</a> (or Section 6 of the <a href="https://arxiv.org/abs/2405.21060">full paper</a>), as it requires a bit of machinery from the theory to derive. But we do emphasize that the implementation of this algorithm isn’t too complicated – a minimal implementation that we provide is only ~30 lines of PyTorch!</p> <p>The benefits of the SSD algorithm is that it preserves the same efficient FLOP counts as SSMs (compared to quadratic attention), and also dramatically speeds up training compared to general state space models by utilizing matmuls.</p> <table> <thead> <tr> <th> </th> <th>Attention</th> <th>SSM</th> <th>SSD</th> </tr> </thead> <tbody> <tr> <td>State size</td> <td>$\mathrm{T}$</td> <td>$\mathbf{N}$</td> <td>$\mathbf{N}$</td> </tr> <tr> <td>Training FLOPs</td> <td>$\mathrm{T}^2\mathrm{N}$</td> <td>$\mathbf{TN^2}$</td> <td>$\mathbf{TN^2}$</td> </tr> <tr> <td>Inference FLOPs</td> <td>$\mathrm{T}\mathrm{N}$</td> <td>$\mathbf{N^2}$</td> <td>$\mathbf{N^2}$</td> </tr> <tr> <td>(Naive) memory</td> <td>$\mathrm{T}^2$</td> <td>$\mathrm{TN}^2$</td> <td>$\mathbf{TN}$</td> </tr> <tr> <td>Matrix multiplications?</td> <td>:heavy_check_mark:</td> <td>:x:</td> <td>:heavy_check_mark:</td> </tr> </tbody> </table> <h2 id="the-mamba-2-architecture">The Mamba-2 Architecture</h2> <p>Although the core contribution of Mamba-2 is the new SSD layer and theory, we also make some small changes to Mamba’s neural network architecture.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/architecture_2-480.webp 480w,/assets/img/2024-05-31-mamba-2/architecture_2-800.webp 800w,/assets/img/2024-05-31-mamba-2/architecture_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/architecture_2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The main change is producing the $(A, B, C)$ SSM parameters in parallel with the $X$ input, instead of sequentially. This is partly motivated by the connections to attention; but more pragmatically, it’s simpler and more amenable to scaling techniques such as tensor parallelism, which will be discussed in Part IV of this series!</p> <p>There are some other small differences which are covered in more detail in the paper. However, we do want to emphasize that these architectural changes aren’t really the main point of the model.</p> <h3 id="language-modeling">Language Modeling</h3> <p>In terms of empirical results, we didn’t test Mamba-2 as extensively as Mamba-1, but believe it should generally be on par or better across the board. Our full language model results use the same protocol as Mamba, and found slightly better scaling at Chinchilla laws <d-cite key="hoffmann2022empirical"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-480.webp 480w,/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-800.webp 800w,/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/pile_8k_mamba2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fully trained models on the Pile dataset<d-cite key="pile"></d-cite> and the standard zero-shot downstream evaluations show similar trends. We emphasize that even when the performance is comparable, Mamba-2 is <em>much</em> faster to train than Mamba-1!</p> <h3 id="synthetic-language-modeling-mqar">Synthetic Language Modeling: MQAR</h3> <p>More interestingly, we highlight the one synthetic task we tried. Since the original Mamba paper, which investigated synthetics such as Synthetic Copying and Induction Heads, many follow-up works have begun investigating harder associative recall tasks. The <strong>multi-query associative recall (MQAR)</strong> task introduced by the Zoology and Based <d-cite key="arora2024zoology"></d-cite><d-cite key="arora2024simple"></d-cite> line of work has become a de facto standard.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mqar-480.webp 480w,/assets/img/2024-05-31-mamba-2/mqar-800.webp 800w,/assets/img/2024-05-31-mamba-2/mqar-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mqar.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We ran a version of this task that’s much harder than the one usually reported in the literature, and found that Mamba-2 is substantially better than Mamba-1. One reason for the improved performance is the much larger state size (up to $16\times$ larger than Mamba-1 here), which was one of the primary motivations of Mamba-2 in the first place.</p> <p>Interestingly, Mamba-2 also appears to be noticeably better than Mamba-1 on this particular task even when the state size is controlled. We’re not quite sure why to be honest, and it would be great to ablate the other aspects of the model to investigate… for example, could it be possible that the [<a href="#ssd-vs-state-space-models">restricted structure of SSD</a>] is actually <em>helpful</em> here?</p> <h2 id="next-up">Next Up</h2> <p>In <a href="/blog/2024/mamba2-part2-theory/">the next part of this series</a>, we’ll go more into the full SSD framework, including how to prove the claimed “duality” of the SSD layer, and strong generalizations of it.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part II - The Theory</title><link href="https://goombalab.github.io/blog/2024/mamba2-part2-theory/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part II - The Theory"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part2-theory</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part2-theory/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li>Part II - The Theory</li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>In <a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series, we defined the state space dual (SSD) <em>model</em>. In isolation, this model is relatively simple to define, and we claimed that it can be computed either as an SSM recurrence or with an attention-like pattern. If you just want to use the model, feel free to skip this post!</p> <p>In this post, we’ll dive into the theory behind the model. We’ll derive the SSD “duality” in two completely separate ways, one starting from the SSM perspective and one from the attention perspective. Each method is actually much more broad than the SSD model itself, and the union of these two strong generalizations is what we call the SSD <em>framework</em>. This framework provides a rich body of connections between state space models, attention, and structured matrices. While the SSD model can be viewed as a specific instantiation of each prong of the framework, the SSD framework is much more general opens up many directions for future work.</p> <h4 id="the-state-space-duality-framework">The State Space Duality framework</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_venn-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_venn-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_venn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_venn.png" width="100%" height="auto" title="Structured State Space Duality" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SSD Framework (red, blue): State space models (i.e. semiseparable matrices) and structured masked attention encapsulate large classes of efficient sequence models. Their intersection is the SSD model (purple).</figcaption> </figure> <p>For each of the two parts of this framework, we’ll</p> <ol> <li>Define the general concepts</li> <li>Show how the SSD model is an instantiation, and prove the duality</li> <li>Suggest future directions for how the framework can be used</li> </ol> <p>Note that this theory is <em>not necessary</em> to use the SSD model itself; this part of the series can be safely skipped for the practitioner that just wants to use SSD (Mamba-2).</p> <h2 id="recap-the-ssd-model">Recap: The SSD Model</h2> <p><a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series introduced the SSD layer, which is defined as a selective SSM</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} y_t \end{aligned}\] <p>\begin{equation} \label{eq:ssm} (\text{Selective state space model (SSM)}) \end{equation}</p> <p>with scalar-identity structure on $A$.</p> <p>More formally, we view it as a <em>sequence transformation</em> $X \mapsto Y$</p> <p>\begin{equation} \label{eq:ssm-transformation} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>The dual attention-like form of the SSD layer is</p> <p>\begin{equation} \label{eq:ssd-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Now let’s see how to prove this!</p> <h2 id="ssd-framework-1-structured-matrix-transformations">SSD Framework 1: Structured Matrix Transformations</h2> <p>The first framing of the duality will be from an SSM-centric perspective, where we’ll prove the duality through the framework of <strong>matrix sequence transformations</strong> or “matrix mixers”.</p> <h3 id="matrix-transformations">Matrix Transformations</h3> <p>The idea is that many sequence models, i.e. <em>sequence transformations</em> $X \in \mathbb{R}^\mathtt{(T,P)} \mapsto Y \in \mathbb{R}^\mathtt{(T,P)}$, can be written in the form of a single matrix multiplication $Y = M(X) \cdot X$ where $M$ is a matrix which can itself depend on $X$. We call this a <em>matrix sequence transformation</em>, or matrix transformation for short. In the literature sequence transformations have also been referred to as “sequence mixers” or “token mixers”, and matrix sequence transformations as “matrix mixers”. There are many examples of these, which are distinguished by the structure of the $M$ matrix. The de facto example is self-attention itself, where $M = \mathsf{softmax}(QK^\top)$ is the attention matrix. Other examples include MLP-Mixer<d-cite key="tolstikhin2021mlp"></d-cite>, FNet<d-cite key="lee2021fnet"></d-cite>, and Monarch Mixer<d-cite key="dao2022monarch"></d-cite><d-cite key="fu2024monarch"></d-cite>.</p> <p>Why do we care about these types of models?</p> <blockquote> <p>Writing a sequence model as a matrix transformation provides a powerful tool to understand the structure and characteristics of the model.</p> </blockquote> <p>And although general non-linear RNNs such as LSTMs <em>cannot</em> be written as matrix mixers, state space models can! In fact, this is pretty easy to see by just unrolling the definition of the SSM recurrence. The upshot is that the SSM \eqref{eq:ssm-transformation} can be written as a matrix transformation</p> \[Y = \mathsf{SSM}(A, B, C)(X) = MX\] <p>where $M_{ij} = 0$ for $i &lt; j$ (i.e. it’s lower triangular) and otherwise \begin{equation} \label{eq:semiseparable} M_{ij} = C_i^\top A_{i:j}^\times B_j := C_i^\top A_i \dots A_{j+1} B_j \end{equation}</p> <p>Drawing it out, this matrix looks like</p> \[\begin{bmatrix} C_0^\top B_0 &amp; \\ C_1^\top A_1 B_0 &amp; C_1^\top B_1 &amp; \\ C_2^\top A_2A_1 B_0 &amp; C_2^\top A_2 B_1 &amp; C_2^\top B_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_1 B_0 &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_2 B_1 &amp; \dots &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1} B_{\mathtt{T}-2} &amp; C_\mathtt{T}^\top B_{\mathtt{T}-1} \\ \end{bmatrix}\] <p>\begin{equation} \label{eq:ssm-matrix} (\text{Matrix Transformation Representation of State Space Models}) \end{equation}</p> <h3 id="semiseparable-matrices">Semiseparable Matrices</h3> <p>This type of matrix in fact has a name: it’s called a (triangular) <strong>semiseparable matrix</strong>, and has been studied in other fields of engineering and computational linear algebra<d-cite key="vandebril2005bibliography"></d-cite>. These matrices are (IMO) quite fundamental and beautiful, and the full paper talks about more of their properties. For example, an alternative characterization of semiseparable matrices is their <em>structured rank property</em>, which says that every submatrix contained in the lower-triangular portion is low rank.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/semiseparable-480.webp 480w,/assets/img/2024-05-31-mamba-2/semiseparable-800.webp 800w,/assets/img/2024-05-31-mamba-2/semiseparable-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/semiseparable.png" width="100%" height="auto" title="State Space Models are Semiseparable Matrices" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">All submatrices contained on-and-below the diagonal of a semiseparable matrix are low-rank.</figcaption> </figure> <p>For our purposes, we’ll care about this form mainly for the algorithmic considerations. One of the central messages of this SSD paper is that:</p> <blockquote class="block-tip"> <h4 id="takeaway-computing-ssms-through-matrix-multiplication">Takeaway: Computing SSMs Through Matrix Multiplication</h4> <p>All algorithms for computing state space models can be viewed as structured matrix multiplication algorithms on semiseparable matrices.</p> </blockquote> <p>Let’s see an easy instantiation of this, focusing on our main objective!</p> <h3 id="deriving-the-duality-ssm-to-attention">Deriving the Duality: SSM to Attention</h3> <p>To show that equation \eqref{eq:ssd-attention} follows from equation \eqref{eq:ssm} (in the case of the SSD model, i.e. scalar SSM), we directly use the matrix form of the state space model \eqref{eq:semiseparable}. Because the $A_t$ are all scalars in this case, they can be factored out of the entries</p> \[C_i^\top A_{i:j}^\times B_j = A_{i:j}^\times \cdot (C_i^\top B_j)\] <p>which directly implies equation \eqref{eq:ssd-attention}.</p> <p>In summary:</p> <blockquote class="block-tip"> <h4 id="duality-representation-1-ssm">Duality Representation 1 (SSM)</h4> <p>The duality for the SSD model can be seen as two <strong>different matrix multiplication algorithms</strong> on the semiseparable matrix.</p> </blockquote> <ul> <li>The linear form is a <em>structured matrix multiplication algorithm</em> that computes the outputs $Y_0, Y_1, \dots$ sequentially, leveraging the structure of the semiseparable matrix.</li> <li>The quadratic form is the <em>naive matrix multiplication algorithm</em> that materializes the full matrix.</li> </ul> <h3 id="going-beyond-the-ssd-layer-1">Going Beyond the SSD Layer 1</h3> <p>The power of the semiseparable matrix representation applies to <em>all</em> state space models, with various downstream implications.</p> <h4 id="algorithms">Algorithms</h4> <p>Algorithmically, the Mamba-2 paper explores several consequences, such as:</p> <ol> <li>The above duality result for the SSD model, i.e. a scalar-identity structured SSM.</li> <li>New asymptotic efficiency results for state space models (<a href="https://arxiv.org/abs/2405.21060">Theorem 3.7</a>), which follow from applying known results from the semiseparable matrix literature <d-cite key="pernet2016computing"></d-cite><d-cite key="pernet2018time"></d-cite><d-cite key="pernet2023exact"></d-cite>.</li> <li>A more general hybrid algorithm that can be viewed as combining both the linear and quadratic forms to get the best of both worlds. This can be derived as a new matrix multiplication algorithm utilizing <em>block decompositions</em> of the semiseparable matrix. This is the subject of Part III of this blog series!</li> </ol> <h4 id="understanding">Understanding</h4> <p>Conceptually, the matrix transformation viewpoint helps provide a unifying view of sequence models. Some example downstream ideas include</p> <ul> <li><strong>New sequence models</strong>: Restricting ourselves to matrix transformations reduces the problem of developing new sequence models to that of finding structured matrix classes with target properties. In ongoing work by my students, we study this point of view, and use it to derive the most natural bidirectional extension of Mamba (coming very soon!).</li> <li><strong>Expressivity</strong>: Looking at the matrix transformation representation can help us understand what different models can represent from a linear algebraic perspective. In another ongoing work, we use this as a tool to study which subquadratic models are the most amenable to being distilled from Transformers.</li> <li><strong>Interpretability</strong>: A concurrent work <d-cite key="ali2024hidden"></d-cite> derived the matrix formulation of SSMs and use it to probe the internal representations of Mamba models.</li> </ul> <p>We’re excited to see what algorithmic and conceptual ideas from the structured matrix literature can be applied to further improve state space models!</p> <h2 id="ssd-framework-2-structured-attention">SSD Framework 2: Structured Attention</h2> <p>The second framing of the duality is from an attention-centric perspective, where we’ll prove the duality through the framework of <strong>tensor contractions</strong>.</p> <p>Note that this is entirely independent of the previous [<a href="#ssd-framework-1-structured-matrix-transformations">matrix transformation viewpoint</a>].</p> <h3 id="warm-up-kernel-attention">Warm-up: Kernel Attention</h3> <p>For our purposes, we’ll define attention as a function</p> \[(Q^\mathtt{(T,N)}, K^\mathtt{(S,N)} , V^\mathtt{(S,P)} ) \mapsto Y^\mathtt{(T,P)}\] <p>given by the pairwise matrix multiplications</p> \[Y = (QK^\top) \cdot V\] <details><summary>On Dimensions</summary> <p>Think of $\mathtt{P} = \mathtt{N}$ as the head dimension; technically speaking, in attention the $V$ head dimension $\mathtt{P}$ can differ from the $QK$ head dimension $\mathtt{N}$. Think of $\mathtt{T}$ as the <em>target</em> sequence dimension and $\mathtt{S}$ as the <em>source</em> sequence dimension. Giving these two axes different names will make the math more clear and also covers more general forms of attention such as cross-attention, where the source and target are separate sequences with different lengths. However, for our purposes we’ll assume the self-attention setting where $\mathtt{S}=\mathtt{T}$.</p> </details> <details><summary>Why can we assume this form?</summary> <p>The usual form of attention $Y = f(QK^\top) \cdot V$ (e.g. where $f$ is the softmax function) can, for essentially all functions $f$<d-footnote>And up to some additional massaging such as row-wise normalization, which is easy to handle</d-footnote>, be written as $Y = \psi(Q)\psi(K)^\top \cdot V$ for some appropriate feature map $\psi$ (which may be infinite dimensional). In this case, we can simply redefine $Q \leftarrow \psi(Q)$ and define $\mathtt{N}$ to be the <strong>feature dimension</strong> of the attention kernel to begin with. Softmax attention, for example, can be represented with a particular infinite-dimensional feature map ($\mathtt{N}=\infty$) which represents the exponential kernel.</p> </details> <p>We’ll restrict ourselves to the case when $\psi$ is finite, which is sometimes called <strong>kernel attention</strong>. Many, many variants have been proposed before!<d-cite key="katharopoulos2020transformers"></d-cite><d-cite key="peng2021random"></d-cite><d-cite key="choromanski2021rethinking"></d-cite><d-cite key="qin2022cosformer"></d-cite><d-cite key="zheng2022linear"></d-cite><d-cite key="wang2020linformer"></d-cite><d-cite key="xiong2021nystromformer"></d-cite></p> <p>Why do we care about this formulation? When the sequence length $\mathtt{T}$ grows and the feature dimension $\mathtt{N}$ is small—commonly, in the regime when $\psi$ is simple such as an elementwise transform and so $\mathtt{N}$ is constant—then the cost of attention can be reduced from quadratic in $\mathtt{T}$ to linear. This follows from simply computing the matrix multiplications in a different order</p> \[Y = Q \cdot (K^\top V)\] <p>This is a somewhat “folklore” interpretation of linear attention.<d-footnote>At least, one lineage of efficient attention; other varieties exist, such as those based on sparsity or hashing. We reserve the term "linear attention" to those related to Katharopoulos et al.<d-cite key="katharopoulos2020transformers"></d-cite>, or more broadly low-rank attention.</d-footnote></p> <blockquote> <p>The most common way of linearizing attention is usually viewed as a consequence of the <strong>associativity of matrix multiplication</strong></p> </blockquote> <h3 id="causal-linear-attention">(Causal) Linear Attention</h3> <p>However, once the basic kernel attention is slightly modified, we can no longer use the associativity of matrix multiplication directly.</p> <p>The seminal <strong>Linear Attention (LA)</strong> framework of Katharopoulos et al. <d-cite key="katharopoulos2020transformers"></d-cite> shows that it can still be extended to the important case of incorporating causality into attention, for autoregressive settings such as language modeling.</p> <p>Let’s be a lot more explicit about how it works. The quadratic form of <strong>causal linear attention</strong> is \begin{equation} \label{eq:quadratic-kernel-attention} Y = (L \circ QK^\top) \cdot V \end{equation} where</p> \[L = \begin{bmatrix} 1 \\ \vdots &amp; \ddots \\ 1 &amp; \dots &amp; 1 \end{bmatrix}\] <p>is the <strong>causal mask</strong> matrix.</p> <p>The issue is: once the $L$ mask is incorporated into \eqref{eq:quadratic-kernel-attention}, we can no longer directly apply matrix associativity! This is the problem that the original Linear Attention paper addresses. What they show is that \eqref{eq:quadratic-kernel-attention} is equivalent to a different form which avoids materializing the quadratic $QK^\top$ attention matrix and has linear time complexity</p> \[Y = Q \cdot \mathsf{cumsum}(K^\top V)\] <p>As far as we’re aware this wasn’t explicitly proved in the paper, although it isn’t too hard to write out the summation to show it.</p> <p>What we’ll do is prove this equivalence in essentially one line, while revealing <em>exactly</em> where the “linear” part of Linear Attention comes from, and how to strongly generalize it.</p> <p>Spoiler alert:</p> <blockquote class="block-tip"> <h4 id="where-does-the-cumsum-in-linear-attention-come-from">Where does the cumsum in Linear Attention come from?</h4> <p>The appearance of the <em>cumulative sum</em> in linear attention is exactly equivalent to the fact that the causal mask $L$, as a matrix multiplication, encodes cumulative sums:</p> \[y = L \cdot x \iff y = \mathsf{cumsum}(x)\] </blockquote> <h3 id="a-tensor-contraction-proof-of-linear-attention">A Tensor Contraction Proof of Linear Attention</h3> <p>Let’s write out the quadratic form of linear attention \eqref{eq:quadratic-kernel-attention} very explicitly in <strong>tensor contraction</strong> or <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a> notation, with shape annotations:</p> \[\begin{aligned} G &amp;= \mathsf{contract}(\mathtt{TN, SN} \to \mathtt{TS})(Q, K) \\ M &amp;= \mathsf{contract}(\mathtt{TS, TS} \to \mathtt{TS})(G, L) \\ Y &amp;= \mathsf{contract}(\mathtt{TS, SP} \to \mathtt{TP})(M, V) \end{aligned}\] <p>\begin{equation} \label{eq:sma-quad} (\text{Structured Masked Attention - Quadratic Form}) \end{equation}</p> <p>With this notation, we can notice that this sequence of contractions can be written as a <em>single four-way contraction</em></p> <p>\begin{equation} \label{eq:sma} y = \mathsf{contract}(\mathtt{TN},\mathtt{SN},\mathtt{SP},\mathtt{TS} \to \mathtt{TP})(Q, K, V, L) . \end{equation}</p> <p>And finally, it can be computed with any other contraction ordering. In particular, we can perform pairwise reductions on the order $V, K, L, Q$ instead of $Q, K, L, V$</p> \[\begin{aligned} Z &amp;= \mathsf{contract}(\mathtt{SP},\mathtt{SN} \to \mathtt{SPN})(V, K) \\ H &amp;= \mathsf{contract}(\mathtt{TS},\mathtt{SPN} \to \mathtt{TPN})(L, Z) \\ Y &amp;= \mathsf{contract}(\mathtt{TN},\mathtt{TPN} \to \mathtt{TP})(Q, H) \end{aligned}\] <p>\begin{equation} \label{eq:sma-lin} (\text{Structured Masked Attention - Linear Form}) \end{equation}</p> <p>Now the key observation is that the second line of \eqref{eq:sma-lin} is simply a matrix multiplication by $L$, which can be computed with a cumulative sum.</p> <p>That’s the entire proof of linear attention! The beauty of it is that we didn’t have to write out a single summation, which was abstracted out into a tensor contraction combined with the structure of $L$.</p> <p>This immediately proves our claim about the <a href="#where-does-the-cumsum-in-linear-attention-come-from">cumsum in linear attention</a>. Moreover, this immediately reveals that the efficiency of linear attention can be made much more general…</p> <h3 id="structured-masked-attention">Structured Masked Attention</h3> <p>The critical observation is that in order for \eqref{eq:sma-lin} to be fast, all that is necessary is for $L$ to be <em>any structured matrix</em> – in other words any matrix that has subquadratic matrix-vector multiplication.</p> <p>This immediately motivates one of the main prongs of the SSD framework, which can be seen as a strong generation of LA.</p> <blockquote class="block-tip"> <h4 id="definition-structured-masked-attention">Definition: Structured Masked Attention</h4> <p><strong>Structured masked attention (SMA)</strong> is defined as the <em>four-way tensor contraction</em> \eqref{eq:sma} using an attention mask $L$ that is a structured matrix.</p> </blockquote> <blockquote class="block-tip"> <h4 id="duality-representation-2-sma">Duality Representation 2 (SMA)</h4> <p>SMA has <strong>dual quadratic and linear</strong><d-footnote>Assuming that the structured matrix $L$ has linear time matrix-vector multiplication</d-footnote> <strong>modes</strong> which are simply <em>two different pairwise reduction orders</em> \eqref{eq:sma-quad} and \eqref{eq:sma-lin}.</p> </blockquote> <p>Finally, let’s just connect this back to the commonly held view of linear attention as matrix multiplication associativity.</p> <blockquote> <p>Although it is commonly believed that incorporating attention masks $L$ prevents matrix multiplication reordering, it turns out to still be compatible. In particular, <strong>associativity of matrix multiplication</strong> is a special case of <strong>tensor contraction reduction orders</strong>; although the former no longer applies, the latter can integrate the attention mask $L$.</p> </blockquote> <p>Next, let’s look at some consequences of the structured attention framework.</p> <h3 id="deriving-the-duality-attention-to-ssm">Deriving the Duality: Attention to SSM</h3> <p>Recall that the SSD model is defined as either a scalar-identity SSM in equation \eqref{eq:ssm}, or through the attention-like form in equation \eqref{eq:ssd-attention}.</p> <p>To show the equivalence of these forms, we simply recognize that \eqref{eq:ssd-attention} is a special case of structured masked attention where the mask matrix is</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix} .\] <p>\begin{equation} \label{eq:1-ss} (\text{1-semiseparable (1-SS) matrix}) \end{equation}</p> <p>We call this a <strong>1-semiseparable (1-SS) matrix</strong>, for reasons that are explained in more detail in the Mamba-2 paper.</p> <p>Thus, we can also say that the SSD model is <strong>1-semiseparable masked attention</strong> or <strong>1-SS SMA</strong>.</p> <p>To prove that this can be written as an SSM, we simply appeal to the SMA framework, which says that the dual form of this model can be computed through matrix multiplication by $L$. So how fast is that? It’s not too hard to see that multiplication $y = Lx$ can be computed in linear time through a scalar recurrence:</p> \[\begin{aligned} y_0 &amp;= x_0 \\ y_1 &amp;= a_1 x_0 + a_1 \\ y_2 &amp;= a_2a_1 x_0 + a_2 x_1 + x_2 = a_2 y_1 + x_2 \\ \vdots &amp; \qquad \vdots \end{aligned}\] <p>This corresponds exactly to the original SSM recurrence!</p> <p>(In fact, multiplication by 1-SS matrices $L$ can be computed in a <em>lot</em> more ways, which we compile in the full paper! Alternative algorithms can reveal more insights: for example, the associative scan algorithm used by S5 <d-cite key="smith2023s5"></d-cite> and Mamba can also be shown to be a structured matrix multiplication algorithm on 1-SS matrices.)</p> <h3 id="going-beyond-the-ssd-layer-2">Going Beyond the SSD Layer 2</h3> <p>Structured masked attention not only helps define the SSD model and prove its duality, but it is a much broader framework of efficient attention models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/sma-480.webp 480w,/assets/img/2024-05-31-mamba-2/sma-800.webp 800w,/assets/img/2024-05-31-mamba-2/sma-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/sma.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prior examples include the original linear attention as well as the recent Retentive Network (RetNet) model<d-cite key="sun2023retentive"></d-cite>. These can be viewed as direct special cases of SSD. But beyond SSD, we can define classes of efficient attention by replacing the mask $L$ with <em>any structured matrix</em>. As a suggestion, we think that Toeplitz or Fourier structured attention may be interesting to consider because they might encode different forms of positional information.</p> <p>Additionally, other forms of structure can be incorporated into the $L$ mask. For example, another extension my students are developing is viewing SSD (and recurrences in general) as an algorithm operating on <em>directed line graphs</em>, and generalizing it to incorporate arbitrary graph structures.</p> <h2 id="state-space-duality">State Space Duality</h2> <p>We’ll end this post with a brief recap of what we’ve covered.</p> <p>The <strong>SSD framework</strong> consists of the two broad approaches covered in this post, which is summarized by the two areas of the [<a href="#the-state-space-duality-framework">Venn diagram</a>]:</p> <ol> <li>Viewing state space models through [<a href="#ssd-framework-1-structured-matrix-transformations">structured matrix transformations</a>]</li> <li>Generalizing linear attention through [<a href="#ssd-framework-2-structured-attention">tensor contractions</a>]</li> </ol> <p>The [<a href="#recap-the-ssd-model">SSD layer</a>] is a particular model which is the purple intersection in the figure, which can be viewed as an instance of either part of the SSD framework, and in particular has dual quadratic and linear forms that can be derived from either representation.</p> <table> <thead> <tr> <th><em>SSD Framework</em></th> <th>Structured SSMs</th> <th>Structured Attention</th> </tr> </thead> <tbody> <tr> <td>The main representation is…</td> <td>Structured matrix \eqref{eq:ssm-matrix} <br/> sequence transformations</td> <td>The 4-way \eqref{eq:sma} <br/> tensor contraction</td> </tr> <tr> <td>This generalizes…</td> <td>State space models</td> <td>Linear attention</td> </tr> <tr> <td>The SSD model is <br/> an instantiation as…</td> <td>Scalar state space model <br/> ($A_t$ is a scalar-identity matrix)</td> <td>1-semiseparable masked attention <br/> ($L$ mask is a 1-SS matrix)</td> </tr> <tr> <td>The linear-quadratic duality is <br/> revealed through…</td> <td>Structured matrix <br/> multiplication algorithms</td> <td>Tensor contraction <br/> reduction orderings</td> </tr> </tbody> </table> <h2 id="next-up">Next Up</h2> <p>In <a href="/blog/2024/mamba2-part3-algorithm/">the next part of this series</a>, we’ll see how to use some of the SSD framework (in particular, the <a href="#takeaway-computing-ssms">structured matrix algorithm</a> point of view) to derive the more efficient hybrid SSD algorithm that leverages both of the dual forms.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part III - The Algorithm</title><link href="https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part III - The Algorithm"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part3-algorithm</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li>Part III - The Algorithm</li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>The theoretical framework of structured state space duality (see <a href="/blog/2024/mamba2-part1-model/">Part I</a> and <a href="/blog/2024/mamba2-part2-theory/">Part II</a> of this series) connects SSMs and (linear) attention through structured matrices. As mentioned in Part I, this connection allows us to derive new algorithms for selective SSMs that are faster than the parallel associative scan in Mamba-1 by leveraging matrix multiplication as a primitive. Moreover, the connection can bring system optimizations (e.g. tensor parallelism, sequence parallelism, variable sequence length) originally developed for Transformer to SSM-land.</p> <h2 id="the-ssd-algorithm">The SSD Algorithm</h2> <p>Even though we already developed optimized scans implementations for Mamba-1, we were limited to small state expansion (typically $\mathtt{N}=16$) as the algorithm and implementation did not use tensor cores (specialized hardware units that perform matrix multiplication). Typically matrix multiplication (matmul) FLOPs are much faster (up to 16x) than non-matmul FLOPs: the A100 GPU has 312 TFLOPS of BF16 matmul but only 19 TFLOPS of FP32 arithmetics, and the H100 has 989 TFLOPS of BF16 matmul but only 67 TFLOPS of FP32 arithmetics. One of our primary goals with Mamba-2 is to <strong>leverage tensor cores to speed up the SSM</strong>.</p> <p>To recap, after tying parameters and introducing the head structure, the SSM in Mamba-1 turns into SSD, a more restrictive form that has an attention-like formulation. And as SSD connects SSMs and structured matrices, we saw in Part II that efficient algorithms to compute SSMs correspond directly to different decompositions of the “token-mixing” or “sequence-mixing” matrix $M$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_algorithm-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_algorithm.png" width="100%" height="auto" title="SSD Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can therefore create new algorithms to compute SSMs simply by looking for alternative ways to multiply this matrix, for example by decomposing it in various ways. A simple block decomposition of this matrix, with carefully chosen block sizes, turns out to get all the advantages of both the linear-recurrent and quadratic-attention dual forms of SSD. This leads to the SSD algorithm, which has 4 steps. There are two completely different interpretations of this algorithm!</p> <h3 id="ssd-algorithm-block-matrix-decomposition">SSD Algorithm: Block Matrix Decomposition</h3> <p>We first partition the SSM (semiseparable) matrix into blocks of size $\mathtt{Q} \times \mathtt{Q}$. Then, we use the properties of semiseparable matrices to factorize each off-diagonal block, which is low rank.</p> <ol> <li>(<em>Orange</em>) Each diagonal block is a smaller semiseparable matrix; we can compute this multiplication however we like; in particular, using the quadratic (attention-like) form of SSD.</li> <li>(<em>Green</em>) There are only $\mathtt{T} / \mathtt{Q}$ total different green blocks because many of them are shared. These can be computed with a batched matmul.</li> <li>(<em>Yellow</em>) Notice that the yellow terms themselves form a 1-semiseparable matrix; in other words, this step is equivalently to an SSM scan (on some modified $A$ factors)!</li> <li>(<em>Blue</em>) Similar to green, these can be computed with a batched matmul.</li> </ol> <h3 id="ssd-algorithm-chunking-and-state-passing">SSD Algorithm: Chunking and State Passing</h3> <p>An alternative interpretation of the algorithm involves reasoning about how the SSM operates on the actual sequence. We first split the sequence of input into blocks (or chunks) of size $\mathtt{Q}$. The steps then have the interpretation</p> <ol> <li><strong>Intra-chunk outputs</strong>: compute the local output of each chunk (<em>what is the output per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Chunk states</strong>: compute the final state of each chunk (<em>what is the final state per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Pass states</strong>: compute a recurrence on all of the chunks’ final states – using any desired algorithm, e.g. parallel or sequential scan (<em>what is the actual final state per chunk taking into account all previous inputs?</em>)</li> <li><strong>Output states</strong>: for each chunk, given its true initial state (computed in Step 3), compute the contribution to the output just from the initial state</li> </ol> <p>Either way, we see that most of the algorithm (Step 1, 2, and 4) leverages matmuls (and hence tensor cores), and also can be computed completely in parallel! Only Step 3 requires a scan, but it operates on a much shorter sequence and usually only takes a small fraction of the time of the full algorithm.</p> <h3 id="special-cases">Special Cases</h3> <p>We note that special cases of this algorithm have been seen before. In particular RetNet<d-cite key="sun2023retentive"></d-cite>, which we showed in Part II to be a special case of SSD, mention a “chunkwise” algorithm which computes the quadratic form on a chunk of the input one-at-a-time and passes the final state to the next chunk. This turns out to be essentially equivalent to the SSD algorithm specialized to a restricted case (i.e. a decay matrix mask $L$). Our derivation comes from a different direction—the block matrix decomposition—which also makes it more obvious how to parallelize this algorithm and make it really fast in practice.</p> <p>Other forms of “chunkwise” recurrences have recently become popular, such as in <a href="https://arxiv.org/abs/2312.06635">Gated Linear Attention (GLA)</a><d-cite key="yang2024gated"></d-cite>.</p> <h2 id="the-code">The Code</h2> <p>In the “Minimal SSD” code that we provide in the paper and the <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py">code release</a>, we delineate each of these four steps. As promised, this algorithm is not only faster but also much easier to implement than the original selective scan of Mamba, coming in at just around 25 lines of code!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>

<span class="k">def</span> <span class="nf">ssd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">block_len</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">initial_states</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">A</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">C</span><span class="p">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_len</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># Rearrange into blocks/chunks
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (c l) ... -&gt; b c l ...</span><span class="sh">"</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">block_len</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)]</span>

    <span class="n">A</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h -&gt; b h c l</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">A_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 1. Compute the output for each intra-chunk (diagonal blocks)
</span>    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="n">Y_diag</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bcshn,bhcls,bcshp-&gt;bclhp</span><span class="sh">"</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 2. Compute the state for each intra-chunk
</span>    <span class="c1"># (right term of low-rank factorization of off-diagonal blocks; B terms)
</span>    <span class="n">decay_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">((</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">A_cumsum</span><span class="p">))</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bhcl,bclhp-&gt;bchpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">decay_states</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
</span>    <span class="c1"># (middle term of factorization of off-diag blocks; A terms)
</span>    <span class="k">if</span> <span class="n">initial_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">initial_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">states</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">initial_states</span><span class="p">,</span> <span class="n">states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">decay_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))))</span>
    <span class="n">new_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhzc,bchpn-&gt;bzhpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">decay_chunk</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># 4. Compute state -&gt; output conversion per chunk
</span>    <span class="c1"># (left term of low-rank factorization of off-diagonal blocks; C terms)
</span>    <span class="n">state_decay_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">)</span>
    <span class="n">Y_off</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bclhn,bchpn,bhcl-&gt;bclhp</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">state_decay_out</span><span class="p">)</span>

    <span class="c1"># Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">Y_diag</span><span class="o">+</span><span class="n">Y_off</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h p -&gt; b (c l) h p</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">final_state</span>
</code></pre></div></div> <h2 id="the-details">The Details</h2> <p>Let’s talk about a couple of additional details in the implementation (these don’t even appear in the full paper, so pay attention!) that unpack some of the choices in this reference code.</p> <h3 id="the-ssm-scan">The SSM Scan</h3> <p>In the above code, we utilized the connection between scalar SSM recurrences</p> \[h_{t+1} = A_t h_t + B_t x_t\] <p>and matrix multiplication by 1-semiseparable matrices</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix}\] <p>which we covered in Part II (and Section 3.2.2 of the paper). In this minimal implementation, we compute Step 3 of the algorithm, which is computing a scalar SSM by <em>any</em> algorithm of our choice, by explicitly materializing a 1-SS matrix and doing dense matrix multiplication.</p> <p>We use this version for several reasons:</p> <ol> <li>Code-wise, it’s simpler to materialize and multiply by this matrix than to actually implement a parallel associative scan</li> <li>Because of the block decomposition of the SSM matrix, the sequence length $\mathtt{T}$ is reduced by a factor of $\approx 100$ – so doing the scan in time $O(\mathtt{T}^2)$ instead of $O(\mathtt{T})$ isn’t too bad</li> <li>We have to materialize a 1-SS matrix anyways for Step 1 of the algorithm (the diagonal blocks), so might as well reuse the code ¯\_(ツ)_/¯</li> </ol> <p>While this example code is simpler and reasonably efficient on GPU (and probably TPU as well!), it’s no longer truly linear at long sequences. Our more optimized Triton implementation does replace the 1-SS multiplication in Step 3 with an actual associative scan.</p> <h3 id="stability">Stability</h3> <h4 id="attempt-1-ratios-of-cumprods">Attempt 1: Ratios of cumprods</h4> <p>The first naive attempt may be to notice that the entries of this matrix are cumulative products</p> \[a_{i:j}^\times = a_i \times \cdots \times a_{j-1} = \frac{a_{i:\mathtt{T}}^\times}{a_{j:\mathtt{T}}^\times}\] <p>However, this runs into severe numerical issues because these products can get really tiny (imagine $a_t \approx 0.9$ and powering it up for a sequence length $\mathtt{T}$ in the thousands!)</p> <h4 id="fix-1-the-segment-sum-segsum-operation">Fix 1: The Segment Sum (<code class="language-plaintext highlighter-rouge">segsum</code>) Operation</h4> <p>The second attempt would be to do all of this in log-space, because all the $a_t$ are positive; so the products become additions, and instead of <code class="language-plaintext highlighter-rouge">cumprod</code>s to deal with we have <code class="language-plaintext highlighter-rouge">cumsum</code>s instead. Then in order to compute the 1-SS matrix, we just have to compute the sums $\log a_i + \dots + \log a_{j-1}$ for every <em>segment</em> $[i:j]$. We call this the <strong>segment sum (segsum)</strong> primitive, analogous to cumulative sum (cumsum).</p> <h4 id="attempt-2-differences-of-cumsums">Attempt 2: Differences of cumsums</h4> <p>The obvious way to do this again is using the same idea as above, but in log space</p> \[a_{i:j}^\times = \exp\left( \log a_i + \cdots + \log a_{j-1} \right) = \left( (\log a)_{i:\mathtt{T}}^+ - (\log a)_{j:\mathtt{T}}^+ \right)\] <p>where we compute a single cumulative sum of $a$ along the time axis, and then compute all pairwise differences. In code, we can do this with</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum_unstable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>
</code></pre></div></div> <p>(and then the 1-semiseparable matrix is just the exponential of this output).</p> <p>Sums/differences are a lot more stable than products/quotients, so this should work – right?</p> <h4 id="fix-2-remove-all-subtractions">Fix 2: Remove All Subtractions</h4> <p>Unfortunately, it turns out this still doesn’t work. The values of this 1-SS matrix roughly represent the SSM dynamics, which are very sensitive to these values of $a_t$, so we have to be very precise. And even in log space, these cumsums can be fairly large, which runs into <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation">catastrophic cancellation</a> when subtracted. So we really have to find a way to compute this matrix with only additions, while still vectorizing everything…</p> <h4 id="attempt-3-stable-segsum">Attempt 3: Stable Segsum</h4> <p>This leads to the helper function in the reference SSD code. Instead of computing a single cumsum and then subtracting, we find a way to use a batch of independent cumsums that immediately produces the right answer without subtraction.</p> <p>These details do matter! Without the right implementation of these primitives, the basic SSD algorithm produces NaNs immediately during training (even with FP32).</p> <h3 id="discretization">Discretization</h3> <p>This lineage of structured state space models developed from <a href="https://arxiv.org/abs/2111.00396">S4</a> and <a href="https://arxiv.org/abs/2110.13985">its</a> <a href="https://arxiv.org/abs/2008.07669">predecessors</a> which were viewed as continuous-time systems.<d-cite key="gu2023thesis"></d-cite><d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2021combining"></d-cite><d-cite key="gu2020hippo"></d-cite></p> <p>In Mamba, however, we don’t really view the SSM as continuous anymore. In fact, as mentioned in the Discussion (Section 5) of the <a href="https://arxiv.org/abs/2312.00752">original paper</a>, Mamba trades off with S4 on modeling different types of data:</p> <ul> <li>S4 is a continuous-time model that excels at modeling continuous data, e.g. perceptual signals such as audio waveforms and pixel-level vision.</li> <li>Mamba is a discrete-time model that excels at modeling discrete data, e.g. tokenized data such as language.</li> </ul> <p>However, the parameterization of Mamba still used the same discretization step as in prior structured SSMs, where there is another parameter $\Delta$ being modeled. We do this because the discretization step has other side effects such as properly normalizing the activations <d-cite key="gu2023train"></d-cite><d-cite key="orvieto2023resurrecting"></d-cite> which is important for performance.</p> <p>The initializations and parameterizations from the previous <a href="https://arxiv.org/abs/2206.12037">theory on structured SSMs</a> still work out-of-the-box, so why fix what’s not broken?</p> <p>Despite this, we’re pretty sure that the discretization step isn’t really necessary for Mamba. In the Mamba-2 paper, we chose to work directly with the “discrete parameters” $A$ and $B$, which in all previous structured SSM papers (including Mamba-1) were denoted $(\bar{A}, \bar{B})$ and defined through an additional transformation</p> \[\begin{align*} \bar{A} &amp;= \exp(e^{\Delta A}) \\ \bar{B} &amp;= (\exp(e^{\Delta A}) - I) A^{-1} B \end{align*}\] <p>This doesn’t pose any problems: to use the continuous SSM parameterization, simply transform the parameters through the above formulas before plugging into the SSD code above.</p> <p>In the full Mamba-2 code, we also kept the same parameterization and discretization step as in Mamba—again, why fix what’s not broken?—but hypothesize that “discrete-centric” variants (such as the <em>gamma normalization</em> of <a href="https://arxiv.org/abs/2303.06349">LRU</a><d-cite key="orvieto2023resurrecting"></d-cite> and <a href="https://arxiv.org/abs/2402.19427">Griffin</a><d-cite key="de2024griffin"></d-cite>) should work equally well.</p> <blockquote class="block-tip"> <h4 id="is-discretization-necessary">Is Discretization Necessary?</h4> <p>It’s useful for other structured SSMs, but perhaps not needed for Mamba. But it’s just a simple invertible transformation, so use either discrete or continuous parameterizations as you like!</p> </blockquote> <h2 id="whats-next">What’s Next</h2> <p>In the <a href="/blog/2024/mamba2-part4-systems/">final part of this series</a>, we’ll continue talking about the implementation of Mamba-2, but on a more macroscopic level; about the entire neural network, instead of just details of the core SSD layer.</p> <p>We’ll also talk about the actual speed of the algorithm covered in this post.</p>]]></content><author><name>Tri Dao</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part IV - The Systems</title><link href="https://goombalab.github.io/blog/2024/mamba2-part4-systems/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part IV - The Systems"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part4-systems</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part4-systems/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li>Part IV - The Systems</li> </ol> <p>Transformers have benefited from 7 years of systems optimization from the whole research community and large companies. The SSD framework draws connections between SSMs and attention, and allows us to implement many of these optimizations for models like Mamba-2 as well. We focus on tensor parallel and sequence parallel for large-scale training, as well as variable-length sequences for efficient finetuning and inference.</p> <h2 id="systems-and-scaling-optimizations">Systems and Scaling Optimizations</h2> <h3 id="tensor-parallelism">Tensor Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_tp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_tp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_tp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_tp.png" width="100%" height="auto" title="Mamba-2 Tensor Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>One difficulty with large-scaling training of Mamba-1 using tensor parallelism (TP) is that it requires 2 all-reduces per layer, compared to just 1 all-reduce per attention or MLP layer in Transformer. This is because some of the SSM parameters are functions of the inner activations, not of the input to the layer. In Mamba-2, with the “parallel projection” structure, all SSM parameters are functions of the input to the layer, and we can easily apply TP to the input projection: We split the input projection and output projection matrices into 2, 4, 8 shards, depending on the TP degree. We use a grouped norm with number of groups divisible by the TP degree, so that normalization is done separately per GPU. These changes result in 1 all-reduce per layer, instead of 2.</p> <h3 id="sequence-parallelism">Sequence Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_cp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_cp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_cp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_cp.png" width="100%" height="auto" title="Mamba-2 Sequence Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When training on very long sequence length, we might need to split along the sequence length and assign different parts to different devices. There are two main forms of sequence parallelism (SP): For the residual and normalization operation: this replaces the all-reduce in TP with a reduce-scatter, residual + normalization, then all-gather. Since Mamba-2 uses the same residual and normalization structure as Transformer, this form of SP applies directly with no modification. For the attention or SSM operation, aka context parallelism (CP). For attention, one could use Ring attention to split it up along the sequence dimension. For Mamba-2, the SSD framework comes to our help once again: using the same block decomposition, we can have each GPU computing its local output and its final states, then pass the states between GPUs (using send/receive communication primitives), before updating the final output of each GPU.</p> <h3 id="variable-length">Variable Length</h3> <p>For finetuning and inference, in the same batch we often have sequences of different lengths. For Transformer, one would usually pad so all sequences have the same length (wasting computation), or implement attention specifically for variable length sequences with careful load-balancing. With SSM, we can simply treat the whole batch as a long “sequence”, and avoid passing the states between different sequences in the batch by setting the state transition $A_t$ to 0 for tokens at the end of each sequence.</p> <h2 id="results">Results</h2> <p>How well do these optimizations work? The faster SSD algorithm allows us to increase the state dimension ($\mathtt{N}=64$ or $128$ compared to $\mathtt{N}=16$ in Mamba-1). Even though technically Mamba-2 is more restricted than Mamba-1 for the same $\mathtt{N}$, the larger state dimensions generally improve model quality. Here we show results for models trained on 300B tokens on the Pile, with Mamba-2 outperforming Mamba-1 and Pythia.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_lm_downstream-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/blog_lm_downstream.png" width="100%" height="auto" title="Downstream Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Standard downstream evaluations for open source models trained on the Pile</figcaption> </figure> <p>What about <strong>hybrid models</strong>? We have seen from recent and concurrent work (such as <a href="https://arxiv.org/abs/2403.19887">Jamba</a> and <a href="https://arxiv.org/abs/2405.16712">Zamba</a>) that combining Mamba layers with attention layers can improve over pure Transformer or Mamba. We validate at 2.7B parameters and 300B tokens scale that a hybrid model with just 6 attention blocks (and 58 SSD blocks) outperforms 64 SSD blocks, as well as our standard Transformer++ baseline (32 gated MLP and 32 attention blocks).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_hybrid-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_hybrid-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_hybrid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/blog_hybrid.png" width="100%" height="auto" title="Downstream Evaluations for Hybrid Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Downstream evaluations for hybrid Mamba/attention models</figcaption> </figure> <p>We also validated that the SSD algorithm is significantly faster than the selective scan algorithm from Mamba-1 for the same state dimension, and scales much better computationally to larger state dimensions. Getting those tensor cores to go brrr is the key!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate.png" width="100%" height="auto" title="Mamba-2 Efficiency Benchmarks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Efficiency benchmarks on sequence length 2K</figcaption> </figure> <h2 id="future-directions">Future Directions</h2> <p>With SSD, we have connected (linear) attention and SSMs, allowing us to design faster algorithms and implement systems optimizations for SSMs. There are still tons of exciting directions that we (and hopefully the community) want to tackle:</p> <ul> <li><strong>Understanding</strong>: hybrid models with a few (4-6) attention layers perform very well, even better than pure Mamba(-2) or Transformer++. What are these attention layers doing? Can they be replaced with another mechanism?</li> <li><strong>Training optimizations</strong>: though SSD might be faster than attention, Mamba-2 as a whole might still be slower than Transformers at short (e.g. 2K) sequence length, since the MLP layers in Transformers are very hardware-friendly. Our implementation of SSD does not specifically take advantage of new features on H100 GPUs, and we look forward to future optimizations that could make SSMs faster to train than Transformers for large-scale pretraining at 2-4K sequence length.</li> <li><strong>Inference optimizations</strong>: there’s a whole suite of optimizations tailored to Transformers, in particular handling the KV cache (quantization, speculative decoding). How would the inference landscape change if model states (e.g. SSM states) no longer scale with context length, and KV cache is no longer the bottleneck?</li> </ul>]]></content><author><name>Tri Dao</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry></feed>