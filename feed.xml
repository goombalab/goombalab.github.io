<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://goombalab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://goombalab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-11T16:54:15+00:00</updated><id>https://goombalab.github.io/feed.xml</id><title type="html">Goomba Lab</title><subtitle>Homepage of the Goomba AI Lab @ CMU MLD. </subtitle><entry><title type="html">H-Nets - the Future</title><link href="https://goombalab.github.io/blog/2025/hnet-future/" rel="alternate" type="text/html" title="H-Nets - the Future"/><published>2025-07-11T00:00:00+00:00</published><updated>2025-07-11T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2025/hnet-future</id><content type="html" xml:base="https://goombalab.github.io/blog/2025/hnet-future/"><![CDATA[<p>This post is part of a two-part series.</p> <ol> <li><a href="/blog/2025/hnet-past/">H-Nets: the Past</a></li> <li>H-Nets: the Future</li> </ol> <p>In this post, I‚Äôm going to try to convince you why H-Nets are fundamental and important. There was only so much content that could make it to the paper, and I think there are a lot of downstream consequences and interesting technical connections that we didn‚Äôt cover. Much of this will be based on deeper (but mostly unvalidated) intuitions I have and speculative implications about H-Nets. For fun, I‚Äôll formulate several concrete hypotheses and predictions about the future of this research direction ‚Äì these are personal takes that aren‚Äôt meant to be calibrated or taken super seriously, but does crystallize what I think might be some important directions for the field.</p> <h2 id="direct-applications">Direct Applications</h2> <p>Let me first say that by ‚ÄúH-Net‚Äù, I don‚Äôt necessarily mean our <em>exact</em> model, but the concept we introduced and our general definition of H-Nets: end-to-end hierarchical networks with data-dependent, dynamic segmentation strategies.</p> <h3 id="alternative-languages-and-modalities">Alternative languages and modalities</h3> <p>The biggest use case of H-Net is, of course, languages and modalities that don‚Äôt have obvious syntactic boundaries for meaningful chunking of the data. (In contrast, the white spaces of English are heavily used in modern LM pipelines, namely through being <a href="https://huggingface.co/learn/llm-course/en/chapter6/4">hacked into the tokenization pipeline</a>.) This means they have much weaker tokenizers that should strongly benefit from using H-Net, which ideally would discover better chunking boundaries.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/bpb_chinese_code-480.webp 480w,/assets/img/2025-07-11-hnet/bpb_chinese_code-800.webp 800w,/assets/img/2025-07-11-hnet/bpb_chinese_code-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/bpb_chinese_code.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Indeed, the most striking results in our paper are for non-English languages. We showed that on both <strong>Chinese and code</strong>, H-Net scales much, <em>much</em> better than the standard tokenized language model. I imagine these results should hold for many other distributions, including <strong>math</strong> (as shown in the <a href="https://arxiv.org/abs/2404.14408">SpaceByte</a> paper <d-cite key="slagle2024spacebyte"></d-cite>) as well as <strong>most human languages</strong>. The linguists have done a much better job than us of explaining <a href="https://arxiv.org/abs/2103.06874">how broken tokenization is for certain languages</a> <d-cite key="clark2022canine"></d-cite>.</p> <p>Other types of sequential data such as <strong>scientific modalities</strong> are hungry for new architectures because the standard Transformer isn‚Äôt enough (as touched on in my <a href="/blog/2025/tradeoffs/#so-what-happens-without-tokenization">previous blog post</a>).</p> <p>Out of all the experiments we did in this paper, the DNA results seemed perhaps the most striking in terms of the improvement over standard architectures.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/bpb_dna-480.webp 480w,/assets/img/2025-07-11-hnet/bpb_dna-800.webp 800w,/assets/img/2025-07-11-hnet/bpb_dna-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/bpb_dna.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I think this most directly validates the power of hierarchical modeling, as the baselines here are also operating on single base pair (non-tokenized) resolution. On data that isn‚Äôt pre-compressed by tokenizers, applying an end-to-end model that tries to learn hierarchical patterns from data seems like a free win!<d-footnote>I actually suspect that DNA modeling could benefit empirically from a BPE tokenizer just like other languages. I'm not quite sure why I haven't seen anyone use this; it will certainly still lead to a valid DNA language model. I guess it just seems too weird to the biologists? Well, another point against tokenizers.</d-footnote></p> <p>I‚Äôll also mention that we didn‚Äôt spend too much time on DNA and never tried iterating the H-Net to 2-stages. It might just get better for free, just like for language?</p> <blockquote class="block-tip"> <h4 id="hypothesis">Hypothesis</h4> <p>H-Nets will immediately be adopted for sequence modalities with unusual characteristics, like <strong>tail languages and genomics</strong>.</p> </blockquote> <p>A final type of application that H-Net could unlock is allowing ‚Äútokenization‚Äù of <strong>continuous-valued sequences</strong>. The concept of tokenization as applied to language models is predicated on discrete vocabularies ‚Äì the process involves explicitly constructing new vocab words from existing ones. So it‚Äôs not applicable directly to non-discrete data like audio and video, even though those modalities seem like they should benefit from a form of dynamic compression (in time) as well! On the other hand, H-Nets are applicable to continuous-valued input sequences and can in principle chunk them in time, ideally grouping meaningful units like the role of tokenization in language. This can potentially be used to perform alignment problems (e.g. audio $\to$ phoneme) or just compress away redundancies (e.g. silence spans of audio or motionless video).</p> <blockquote class="block-tip"> <h4 id="hypothesis-1">Hypothesis</h4> <p>H-Nets will be useful for <strong>audio and video</strong>, but may require more research.</p> </blockquote> <h3 id="multimodal-alignment">Multimodal alignment</h3> <p>Speaking of different modalities, so far I‚Äôve only talked about training models on a single modality at a time. The power of dynamic chunking will be even more important when moving to multimodal models that fuse different streams of data together.</p> <p>In particular, multimodal streams with temporal mismatch (e.g. text and audio) are difficult to synchronize. For example, language is currently tokenized into subwords, while audio is tokenized as raw waveforms or downsampled codecs, making them difficult to model jointly. Learned chunking mechanisms provide a path to fuse multimodal streams ‚Äúticking‚Äù at different rates, unlocking native <em>multi-channel</em> multimodal models operating at a higher abstraction level and enabling better transfer, reasoning, and understanding across modalities.</p> <blockquote class="block-tip"> <h4 id="hypothesis-2">Hypothesis</h4> <p>H-Nets will unlock <strong>multi-channel multimodal models</strong> with temporal fusion.</p> </blockquote> <h3 id="language-and-reasoning">Language (and reasoning)</h3> <p>Of course, the big question is how good this model <em>really</em> is for core language modeling, which is still at the heart of the most mainstream AI progress today. I‚Äôm personally very bullish on H-Nets ‚Äì I wouldn‚Äôt have worked in this direction if not ‚Äì but as always with architectural research, there are significant unknowns, risks, and barriers to adoption.</p> <p>The main reasons I think they‚Äôll improve language modeling has been laid out in the paper, but in a nutshell, the goal of H-Nets is to <em>compress data into semantic units</em> and <strong>operate over higher levels of abstraction</strong>. It‚Äôs currently not known to what extent they can successfully do this right now (but I think the scaling behavior we showed is evidence for it), but if possible, it should just allow for stronger models that are more capable of reasoning intelligently.</p> <blockquote class="block-tip"> <h4 id="hypothesis-3">Hypothesis</h4> <p>H-Nets will have increased <strong>language modeling</strong> and <strong>reasoning</strong> abilities.</p> </blockquote> <p>We didn‚Äôt formally run out true scaling laws in the paper, which would require sweeping over many more model sizes and compute horizons. But as I mentioned in the previous part, I have some reasons to believe that they will have better scaling (i.e. shift the scaling law curve by a non-trivial amount). I started writing this up but got tired üòÜ but maybe I‚Äôll follow this up with another blog post.</p> <p>Given the importance of language and it being our main motivation for this model, the rest of this post will focus exclusively on language modeling intuitions.</p> <h2 id="efficiency-and-engineering-and-a-connection-to-speculative-decoding">Efficiency and Engineering (and a Connection to Speculative Decoding)</h2> <p>One of the first things that gets asked about any new architecture is how efficient it is. Much of my prior architecture research has specifically focused on being <a href="https://arxiv.org/abs/2111.00396">more</a> <a href="https://arxiv.org/abs/2312.00752">efficient</a> than the status quo. In the H-Net paper, we basically didn‚Äôt touch on efficiency, so what can we say about this?</p> <h3 id="the-efficiency---quality-pareto-frontier">The efficiency ‚Üî quality Pareto frontier</h3> <p>Well, the simple reason why we didn‚Äôt is because the intuition behind our method (chunking) is more obviously connected to model <em>quality</em> rather than <em>efficiency</em>, so we focused on those aspects. But of course, what really matters is the <em>interaction</em> between efficiency and quality. This is generally monotone, leading to an entire Pareto frontier of performance tradeoffs. At a superficial level, I expect that the quality gains of H-Nets would directly translate to efficiency gains as well.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/efficiency_quality-480.webp 480w,/assets/img/2025-07-11-hnet/efficiency_quality-800.webp 800w,/assets/img/2025-07-11-hnet/efficiency_quality-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/efficiency_quality.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">I realized this figure probably contains zero information content, but <br/> since I already drew it for some reason I'm not gonna waste it.</figcaption> </figure> <p>But there can often be more nuance to this with architecture research because of qualitative differences between models.<d-footnote>For instance, the efficiency ‚Üî quality tradeoff of Mamba vs. Transformers isn't actually so clear-cut, as discussed in my previous blog post on the tradeoffs of SSMs and Transformers.</d-footnote> What people want to know is whether there are <strong>qualitative structural characteristics</strong> in the architecture that directly relate to its efficiency. (This is what made SSMs so appealing, I suppose, as these characteristics ‚Äì the constant state size ‚Äì are very intuitive.)</p> <p>This topic is a lot more subtle for H-Nets, but there are a few intriguing connections to highlight.</p> <h3 id="speculative-decoding">Speculative decoding</h3> <p>Let‚Äôs think about <strong>speculative decoding (specdec)</strong>, which is by now a universally used technique for LLM inference <d-cite key="leviathan2023fast"></d-cite><d-cite key="chen2023accelerating"></d-cite><d-cite key="xia2023speculative"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/speculative-480.webp 480w,/assets/img/2025-07-11-hnet/speculative-800.webp 800w,/assets/img/2025-07-11-hnet/speculative-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/speculative.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The speculative decoding process of stepping a small model on every token and a large model on every few tokens strongly resembles the H-Net decoding process. [<a href="https://arxiv.org/abs/2203.16487">Source</a>]</figcaption> </figure> <h4 id="speculative-decoding-resembles-an-h-net">Speculative decoding resembles an H-Net</h4> <p>Without getting too in the weeds, speculative decoding consists of a <em>large model</em> (usually called the ‚Äúverification model‚Äù) that we want to sample from, and a <em>small model</em> (usually called the ‚Äúdraft model‚Äù) that‚Äôll help us sample from it faster. The decoding process basically looks like this:</p> <ol> <li>On every autoregressive step, the <strong>draft model</strong> will take a step to generate a new token.</li> <li>Every few steps, the <strong>verification model</strong> will verify the small model‚Äôs sequence of tokens, accepting as many of them as it can.</li> </ol> <p>At a high level, specdec improves generation speed by letting the large model only do a forward pass every few tokens. But this is incredibly similar to the decoding process of an H-Net!</p> <ol> <li>The H-Net <strong>encoder/decoder networks</strong> take a step on every token.</li> <li>The H-Net <strong>main network</strong> takes a step every few tokens (on every <em>chunk</em>).</li> </ol> <h4 id="speculative-decoding--h-net-is-redundant">Speculative decoding + H-Net is redundant</h4> <p>One can take this a step further and ask: what happens if we combine speculative decoding to try to speed up an H-Net? In the <em>ideal</em> case of speculative decoding, what might happen is:</p> <ol> <li>The small model (an auxiliary draft model) takes a few steps, say $k$, and proposes a number of tokens.</li> <li>The large model (H-Net) does a forward pass on these $k$ tokens. Suppose that $k$ lines up with the next chunk: then this amounts to <ul> <li>$1$ parallel pass (over $k$ tokens) of the H-Net‚Äôs encoder/decoder networks, and</li> <li>just $1$ standard decoding step of the H-Net‚Äôs main network.</li> </ul> </li> </ol> <p>Further suppose that the draft model has a similar network structure and size as the H-Net‚Äôs encoder/decoders. Then this is almost the same as the vanilla decoding process of an H-Net above, just with the sequential steps of the encoder/decoder swapped for sequential steps of an auxiliary draft model!</p> <p>So this application of speculative decoding was pretty much a no-op in terms of improving efficiency.</p> <p>Why is this? One way to interpret this is that the speculative decoding process is already <em>baked into</em> the H-Net structure, and once we move to H-Nets, we might not need inference techniques like specdec anymore.</p> <h4 id="entropy-strikes-again">Entropy strikes again</h4> <p>And there‚Äôs one final conceptual connection! Speculative decoding works because there‚Äôs some form of redundancy in token sequences that makes some of them easier to predict than others; the speedup happens exactly when there are small sequences of easy-to-predict tokens. Or put another way, when there are local strings of low-entropy or low-information tokens. But this is exactly one of the heuristics for how to think about dynamic hierarchical networks (as discussed in the <a href="/blog/2025/hnet-past/#information-based-chunking">previous post</a>) ‚Äì they segment on surprising tokens, or more generally on some notion of information content in the data.</p> <p>All in all, there are a lot of striking similarities between speculative decoding and H-Nets! My hypothesis is that: <strong>the H-Net structure implicitly subsumes the speculative decoding process</strong>.</p> <p>What are the implications of this? Well, the obvious practical one is that despite seeming more complicated than standard models, H-Nets might not actually be much more complicated than modern LLM <em>pipelines</em> used in practice.</p> <blockquote class="block-tip"> <h4 id="hypothesis-4">Hypothesis</h4> <p>H-Net inference engineering will have a <strong>similar complexity</strong> to current LLM inference pipelines.<br/> <br/> Furthermore, inference tricks will become marginally less and less useful (and ideally not be necessary at all), as they become <strong>subsumed by end-to-end models</strong> that incorporate the underlying concepts in a more natural way.</p> </blockquote> <p>But to me, there‚Äôs a more important conceptual implication.</p> <ul> <li>Much of architecture efficiency optimization consists of asking: <strong>Given this model, how can I make it faster?</strong></li> <li>One can flip this on its head and ask: <strong>Given that this model <em>could</em> be sped up, what does that imply about the original model?</strong> The way to think about this is that <em>the very fact that standard LLMs can be sped up</em> through tricks like speculative decoding means that <em>the original models have redundancies</em> and could be made more powerful to begin with.</li> </ul> <p>The H-Net structure is exactly the way to smooth out those redundancies, baking (something akin to) the speculative decoding process directly into the model, while <strong>leveraging parameters and compute</strong> more effectively and <strong>training everything end-to-end</strong>. In other words, the structure of the H-Net preserves the same characteristics of the <em>inference-optimized</em> standard LM, but with a better <em>training objective</em>.<d-footnote>Just to unpack a bit more: intuitively, the reason this should lead to a stronger model is because the main network (analogous to the target verification model in specdec) is trained directly on *chunk-level* modeling, the way they would be used at inference, instead of the specdec pipeline of being trained on a more granular (*token-level*) objective and being used in a different way at inference.</d-footnote></p> <p>Thus, what I predict is that with optimized inference implementations for H-Nets, then for any target inference budget, an H-Net would be a stronger model than our current standard LLMs.</p> <h3 id="engineering-challenges">Engineering challenges</h3> <p>Okay, a lot of what I‚Äôve talked about so far (and the way it‚Äôs implicitly described in the paper) is about <em>theoretical</em> efficiency; we considered FLOPs, not wall-clock time. A very important question is: is the theoretical efficiency realizable in practice?</p> <h4 id="training">Training</h4> <p>Training is more difficult than normal because sequences are dynamically subsampled, which causes load balance issues among other edge cases. Sukjun spent a while engineering our pipeline to be reasonably efficient by incorporating dynamic packing and such. Our current implementation is still a bit slower than isotropic models during training, but I expect to have substantial room for improvement. There has been a lot of work on mixture-of-experts (MoE) in the last few years, and I expect a lot of general ideas will transfer to H-Nets.</p> <h4 id="inference">Inference</h4> <p>Inference has largely been discussed in relation to speculative decoding; I think it‚Äôs going to take some work, but don‚Äôt see any fundamental barriers.</p> <p>Overall, engineering for H-Nets will be a substantial but surmountable problem for their adoption at scale.</p> <blockquote class="block-tip"> <h4 id="hypothesis-5">Hypothesis</h4> <p>H-Nets will require non-trivial <strong>research and infrastructure</strong> work for both <strong>training and inference</strong>; on the order of what was needed for tokenizers, mixture-of-experts, and speculative decoding pipelines.<br/> <br/> This effort will be worth the tradeoff to achieve higher quality and less brittle end-to-end models.</p> </blockquote> <h2 id="revival-of-architecture-research">Revival of Architecture Research</h2> <p>As multi-component pipelines are consolidated into end-to-end models, previous parts of the pipeline that required dedicated treatment will transform into added complexity in the model instead. Architectures will become somewhat more sophisticated and require new considerations and research. Here are a couple of such considerations.</p> <h3 id="hierarchical-sequence-models">Hierarchical sequence models</h3> <p>As I described in <a href="/blog/2025/hnet-past/#hierarchical-rnns">my own journey through sequence models</a>, hierarchy is far from new.</p> <p>There have been a few recent works that investigate hierarchical structures inside novel sequence model layers (i.e. variants of attention or SSMs). For example:</p> <ul> <li><a href="https://arxiv.org/abs/2502.11089">Native Sparse Attention (NSA)</a> <d-cite key="yuan2025native"></d-cite> is a recent sparse attention model that performs a 2-stage process of aggregating information inside local blocks.</li> <li><a href="https://arxiv.org/abs/2506.04761">Log-linear attention</a> <d-cite key="guo2025log"></d-cite> and <a href="https://arxiv.org/abs/2506.10918">prefix scannable models (PSM)</a> <d-cite key="yau2025sequential"></d-cite> introduce new hierarchical layers that generalize modern recurrent models using a binary tree of hierarchies, improving their expressivity by increasing the constant size state to logarithmic (in sequence length).</li> </ul> <p>While these models are elegant exercises in algorithm design and engineering, and definitely valuable contributions to the community, I personally think there might be problems long-term with building hierarchy directly into the layer. The root cause is the difficulty of having a dynamic or flexible hierarchy, which also ties to hardware considerations.</p> <p>In NSA, for example, there is a <em>block size</em> hyperparameter (set to $64$ by default, I think) that governs the lower level of the hierarchy, motivated by hardware alignment. This doesn‚Äôt feel ‚Äúright‚Äù to me for some reason.<d-footnote>Another appeal to aesthetics for the future rather than the current state of the world; I've been told NSA works pretty well in practice right now!</d-footnote> I guess it‚Äôs because I think that while hardware considerations are important, they should be connected to the <em>model algorithm</em> rather than the <em>model definition</em>. For example, while <a href="https://arxiv.org/abs/2405.21060">Mamba-2</a> <d-cite key="dao2024transformers"></d-cite> also has a block size hyperparameter (also set to $64$ by default) related to the size of matrix multiplication tiles, this only affects its implementation/efficiency and not the <em>definition</em> of the model. In contrast, the block size hyperparameter of NSA fundamentally changes what functions (sequence transformations) it can represent.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/psm-480.webp 480w,/assets/img/2025-07-11-hnet/psm-800.webp 800w,/assets/img/2025-07-11-hnet/psm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/psm.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Concurrent works propose other types of hierarchical sequence models with logarithmically-scaling state sizes.</figcaption> </figure> <p>As another example, the log-linear models are tied to a static binary-tree hierarchy. But a major theme of the H-Net paper is that static hierarchies are not the right structure!</p> <blockquote class="block-tip"> <h4 id="hypothesis-6">Hypothesis</h4> <p>The best way of building hierarchical models will be in the holistic architecture‚Äôs <strong>global network structure</strong> like H-Net, not in individual layers.</p> </blockquote> <p>A future direction of H-Nets is to see how far the hierarchy can extend. For example, one can build a binary tree-like architecture with repeated downsampling by a factor of roughly 2 (a controllable parameter in the H-Net), which leads to the same holistic properties as log-linear layers ‚Äì linear-scaling computation in sequence length with logarithmic-scaling state size ‚Äì but with a dynamic hierarchy. By carefully balancing the depth/widths of sub-networks as well, one can actually get very fine control over the scaling of both compute and state size, potentially targeting <strong>polynomial-scaling state sizes</strong> which is <a href="http://arxiv.org/abs/2503.04725v1">hypothesized to be optimal for language modeling</a> <d-cite key="chen2025l"></d-cite>. I‚Äôm quite interested in exploring this, let me know if you want to collaborate!</p> <h3 id="long-context">Long context</h3> <p>One question I have is whether deep hierarchies can actually allow one to completely get rid of global attention. What happens if one uses a deep recursive H-Net with pure constant-state-size layers like SSMs or sliding window attention (SWA)? In a normal isotropic model, these suffer on long sequences because they simply can‚Äôt encode enough information. But in a deep hierarchical model, information is constantly being compressed and abstracted, shrinking the sequence length and perhaps substantially improving the effectiveness of these layers.</p> <p>Maybe some lightweight global attention will still be needed for certain retrieval abilities? But I think hierarchical structure can only help long context significantly (which is indeed an explicit motivation for many prior works on hierarchical sequence models!).</p> <blockquote class="block-tip"> <h4 id="hypothesis-7">Hypothesis</h4> <p>H-Nets will substantially <strong>improve long context abilities</strong>.</p> </blockquote> <p>We originally wanted to explore some long context benchmarks in the H-Net paper, but there were too many facets to show already so we didn‚Äôt get around to it. Hopefully someone will investigate this in the future!</p> <h3 id="hybrid-models">Hybrid models</h3> <p>While hybrid models combining linear layers with quadratic attention have become much more popular, I always wondered if the simple interleaving strategies were the most natural way.</p> <p>One nice thing about H-Nets is that they can hybridize linear and quadratic layers in a more elegant way, in my opinion. (In my head, another potential meaning of H-Net stands for <strong>hybrid network</strong>!) Linear layers go on the outside, both for efficiency <em>and</em> modeling reasons (as covered in <a href="/blog/2025/tradeoffs/#so-what-happens-without-tokenization">my Tradeoffs post</a>), and powerful quadratic attention layers can go on the inside, operating over higher levels of abstraction where they are most suited.</p> <p>However, figuring out the exact right combination of layers is pretty non-trivial. We did endless ablations over the course of this project (and included many of them in the paper, but that was only a small subset), and it was pretty hard to come to conclusive answers.</p> <p>For example, these were the conclusions found for a 2-stage H-Net (three sequence lengths):</p> <ul> <li><strong>Outer</strong>: Pure Mamba layers perform best, and seem indispensable.</li> <li><strong>Middle</strong>: After the outer layers have shrunk the sequences by a reasonable length (almost $3\times$), this is much closer to tokenized language, and I wouldn‚Äôt have been surprised if pure Transformer layers were fine here. But we found that Mamba was still crucial, which validates that its effect is not <em>just</em> because it‚Äôs good at high resolution, but because it‚Äôs doing a form of <a href="/blog/2025/hnet-past/#ssms-as-compressive-models"><strong>active compression that benefits dynamic chunking</strong></a>.</li> <li><strong>Inner</strong>: The innermost model has the most parameters and is essentially a standard isotropic language model operating on coarsely tokenized data (but with better ‚Äútokens‚Äù that are dynamic and learned from data!). In the paper, we stuck to pure Transformers because that was our main baseline. However, this is completely orthogonal to the rest of the H-Net design; we did experiment a bit and did an ablation showing that general findings for LM architectures still transfer, such as that <strong>hybrid main networks</strong> (we tried 3-to-1 Mamba-to-Transformer) <strong>still have noticeably better perplexity</strong> <d-cite key="waleffe2024empirical"></d-cite>!</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/hybrid-480.webp 480w,/assets/img/2025-07-11-hnet/hybrid-800.webp 800w,/assets/img/2025-07-11-hnet/hybrid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/hybrid.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Ablating the main network architecture of a 2-stage H-Net.</figcaption> </figure> <p>More explicitly, I think the following is true (we didn‚Äôt show ablations but ran some early tests).</p> <blockquote> <p>H-Nets would <strong>work reasonably well without attention</strong> (only SSM layers), but <strong>not at all without SSMs</strong> (only Transformer layers).</p> </blockquote> <p>At the very least, moving toward such hierarchical models will necessitate expanding the space of primitives used; I‚Äôm pretty sure <a href="/blog/2025/tradeoffs/">standard attention is not sufficient</a>.</p> <blockquote class="block-tip"> <h4 id="hypothesis-8">Hypothesis</h4> <p>Linear sequence models such as <strong>state space models will become core primitives</strong> of language models, if only for acting as the byte-level interface.</p> </blockquote> <p>In turn, research papers on such models should start <strong>incorporating byte-level language modeling</strong> as a standard evaluation.</p> <h3 id="its-the-wild-west">It‚Äôs the wild west</h3> <p>I have to emphasize again that creating the H-Net was a <a href="/blog/2025/hnet-past/#a-world-of-improvements">fiendishly difficult design problem</a>, and we still don‚Äôt completely understand how a lot of things work. I wouldn‚Äôt be too surprised if someone came out next week with a simplification of our routing mechanism that was better (well, I‚Äôd be pretty surprised actually ‚Äì but I do expect it to happen at some point). At any rate, there are so many new axes of variation, knobs to turn, and completely new directions to explore. Things are just getting started!</p> <h2 id="closing">Closing</h2> <p>Let me return once again to the higher-level question: is all of this actually useful? Are hierarchical models the future?</p> <p>In this post, I haven‚Äôt said anything that was actually technical or rigorous, only a loose set of connections and intuitions. But somehow to me these point to some type of deeper truth. Something about certain models just ‚Äúfeels right‚Äù to me, and H-Nets <em>feel right</em>.</p> <p>Perhaps the most concrete answer I can give, though, can be summarized by just two points:</p> <ol> <li>Hierarchical pipelines are <em>already</em> used everywhere, often implicitly.</li> <li>Consolidating them into general, trainable methods is at the heart of AI (<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>).</li> </ol> <blockquote> <h4 id="observation">Observation</h4> <p>Existing LLM <strong>pipelines</strong> are already <strong>implicitly hierarchical</strong>, such as<br/> (1) the core language model pipeline (tokenizer ‚Äì language model)<br/> (2) the speculative decoding pipeline (draft model ‚Äì verification model)</p> </blockquote> <blockquote class="block-tip"> <h4 id="hypothesis-9">Hypothesis</h4> <p>As we get closer to finding ‚Äúthe right architecture‚Äù, these explicitly engineered pipelines will be subsumed by an end-to-end model. <strong><em>Maybe the H-Net?</em></strong></p> </blockquote>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[This post is part of a two-part series. H-Nets: the Past H-Nets: the Future]]></summary></entry><entry><title type="html">H-Nets - the Past</title><link href="https://goombalab.github.io/blog/2025/hnet-past/" rel="alternate" type="text/html" title="H-Nets - the Past"/><published>2025-07-10T00:00:00+00:00</published><updated>2025-07-10T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2025/hnet-past</id><content type="html" xml:base="https://goombalab.github.io/blog/2025/hnet-past/"><![CDATA[<p>This post is part of a two-part series.</p> <ol> <li>H-Nets: the Past</li> <li><a href="/blog/2025/hnet-future/">H-Nets: the Future</a></li> </ol> <p>The <a href="https://arxiv.org/abs/2507.07955">H-Net</a> model has been a dream of mine for years. I‚Äôve been fortunate enough to be able to work with my student <a href="https://sukjunhwang.github.io/">Sukjun Hwang</a> who made the technical breakthroughs behind this model, solving (or at least making serious progress towards) what I consider to be a very difficult but foundational problem for deep learning.</p> <p>In this post, I provide an informal personal recounting of the motivation and history of this project, providing various context and discussion that wouldn‚Äôt make sense in a paper. This post is really just for fun ‚Äì I‚Äôve always found it interesting to read about the windy history of research instead of just the streamlined final result, and I felt like telling this story.</p> <p>In the next post <a href="/blog/2025/hnet-future/">H-Nets (the Future)</a>, I‚Äôll explain why I think this model is so important and a variety of potential downstream implications.</p> <h2 id="hierarchy-is-everywhere">Hierarchy is Everywhere</h2> <p>The idea of hierarchy has always resonated with me. Perhaps its comes from my penchant for discrete math and roots in competitive programming, where I spent all day working with algorithms on trees, the canonical hierarchical structure. Perhaps it reflects the way I tend to think; I structure almost everything hierarchically, for example using bulleted outlines for everything to a perhaps excessive degree (I‚Äôd write papers in outline format if I could!).</p> <p>At some point early in my PhD, when I wasn‚Äôt doing much productive work, I took an online class for fun on <a href="https://www.coursera.org/learn/learning-how-to-learn">Learning How to Learn</a>. I think this is where I came across the concept of <a href="https://en.wikipedia.org/wiki/Chunking_(psychology)">chunking</a> in human cognition, which really resonated with me and felt fundamental to intelligence. For example, our processing is strongly hierarchical: language is parsed into words, phrases, clauses, sentences, and beyond; perception is organized into objects and events; tasks are decomposed into subtasks and goals. Raw streams of thought are chunked into <em>abstractions</em> and <em>ideas</em>. The <em>chunking</em> concept seems really important to me, and later became adopted for this project‚Äôs terminology.</p> <h2 id="hierarchical-rnns">Hierarchical RNNs</h2> <p>From the very beginning of my work on sequence models, I was drawn to hierarchical models.</p> <p>My first exposure to sequence models came during an internship in 2019 at DeepMind under the amazing [Caglar Gulcehre] and [Nando de Freitas]. I started thinking about RNNs with help from Caglar, who was an encyclopedia of classical sequence models (e.g. he was on the original <a href="https://arxiv.org/abs/1412.3555">GRU</a> paper <d-cite key="chung2014empirical"></d-cite>) and pointed me to a lot of interesting ideas.</p> <p>I was particularly interested in the notion of improving long-range dependencies through hierarchies. This had been attempted in many flavors of RNNs, such as the Clockwork RNN <d-cite key="koutnik2014clockwork"></d-cite>, Hierarchical Multiscale RNN <d-cite key="chung2017hierarchical"></d-cite>, and Dilated RNN <d-cite key="chang2017dilated"></d-cite>. I played around with tons of ideas on incorporating hierarchies into RNNs; I particularly liked the idea of the <a href="https://arxiv.org/abs/1810.09536">Ordered Neuron LSTM</a> (ICLR 2019 best paper!) <d-cite key="shen2019ordered"></d-cite>, which introduced a clever mechanism to incorporate an implicit hierarchy in the forget/input gates of an LSTM. My <a href="https://arxiv.org/abs/1910.09890">first ever paper on sequence models</a> <d-cite key="gu2020improving"></d-cite> was heavily inspired by this paper (as well as by <a href="https://arxiv.org/abs/1804.11188">Chrono LSTM</a> <d-cite key="tallec2018can"></d-cite>, which inspired me to think of RNNs as continuous systems and led to a lot of my work on state space models ‚Äì but that‚Äôs another story).</p> <h2 id="s4-and-sashimi">S4 and SaShiMi</h2> <p>After DeepMind, my research switched completely to sequence models<d-footnote>By the way, this happened in the 5th year of my PhD. I always tell junior PhD students that it's never too late and to focus on your fundamentals first!</d-footnote>, and I started developing state space models from <a href="https://arxiv.org/abs/2008.07669">HIPPO</a> <d-cite key="gu2020hippo"></d-cite> to <a href="https://arxiv.org/abs/2110.13985">LSSL</a> <d-cite key="gu2021combining"></d-cite> to <a href="https://arxiv.org/abs/2111.00396">S4</a> <d-cite key="gu2022efficiently"></d-cite>, which was the culmination of my PhD. A little known fact (I‚Äôm guessing) about the S4 paper is that I actually introduced a hierarchical architecture in one of the experiments!</p> <h3 id="autoregressive-u-nets-for-image-modeling">Autoregressive U-Nets for image modeling</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/s4_cifar-480.webp 480w,/assets/img/2025-07-11-hnet/s4_cifar-800.webp 800w,/assets/img/2025-07-11-hnet/s4_cifar-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/s4_cifar.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In this paper, we tested S4‚Äôs autoregressive modeling ability on a variety of distributions: on the left is the CIFAR-10 density estimation problem, and on the right is language modeling on WikiText-103 (at the time a popular dataset, but far too small scale now).</p> <p>To improve the pixel modeling problem performance, I actually introduced an <em>autoregressive U-Net</em> structure that had two stages of downsampling. The overall structure of this model, such as the fixed-width pooling and how to preserve causality for the autoregressive sampling, is very similar to a bunch of models that have since been used for language (such as Hourglass Transformer, <a href="https://arxiv.org/abs/2305.07185">MegaByte</a> <d-cite key="yu2023megabyte"></d-cite>, <a href="https://arxiv.org/abs/2406.02657">Block Transformer</a> <d-cite key="ho2024block"></d-cite>, and a few others).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/s4_cifar_appendix-480.webp 480w,/assets/img/2025-07-11-hnet/s4_cifar_appendix-800.webp 800w,/assets/img/2025-07-11-hnet/s4_cifar_appendix-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/s4_cifar_appendix.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">I just realized that I never mentioned this detail anywhere in the main body, which seems like a big oversight. Oops.</figcaption> </figure> <p>As far as I‚Äôm aware, this paper (concurrently with <a href="https://arxiv.org/abs/2110.13711">Hourglass Transformer</a> <d-cite key="nawrot2022hierarchical"></d-cite>, submitted to arXiv just 5 days apart and with nearly identical architectures!) might have been the first to actually use an autoregressive U-Net?<d-footnote>On 1D sequences specifically; <a href="https://arxiv.org/abs/1701.05517">PixelCNN++</a> <d-cite key="salimans2017pixelcnn++"></d-cite> did something similar on 2D structure, which was the motivation for my architecture.</d-footnote> Although it‚Äôs not particularly complicated, so I wouldn‚Äôt be surprised if there were some earlier ones I missed.</p> <h3 id="u-nets-dont-work-for-language">U-Nets don't work for language</h3> <p>Something not reported in the paper, though, was that I tried the same backbone on the WikiText-103 language modeling experiments as well. I really wanted it to work, since I felt like hierarchy should be important! But no matter what I did, the hierarchical version was always <em>noticeably worse</em> than the isotropic backbone for language modeling.</p> <p>I eventually gave up on this, and I think it formed the basis for some of my intuition around language modeling. In particular, I realized that fixed-width pooling just didn‚Äôt make sense for language. (And moreover, I realized that this is essentially the same as convolutions, or at least has the same inductive bias, and this helped form the basis of my hypothesis about how convolutions don‚Äôt work for language modeling and more general ‚Äúdiscrete data‚Äù that has variable spacing. See this part of my <a href="/blog/2025/tradeoffs/#a-hypothetical-litmus-test">Tradeoffs post</a> for a bit more discussion.)</p> <p>To be honest, I don‚Äôt really understand how so many papers on this U-Net-like strategy have been published for language modeling. I‚Äôm pretty skeptical that they work at all, as has also been shown in ablations by follow-up works on byte-level language modeling like <a href="https://arxiv.org/abs/2404.14408">SpaceByte</a> <d-cite key="slagle2024spacebyte"></d-cite> and <a href="https://arxiv.org/abs/2412.09871">BLT</a> <d-cite key="pagnoni2024byte"></d-cite>. I guess one major difference is that all of these papers were using Transformers, which is much better for language than S4, but I still don‚Äôt think it really makes sense ü§∑.</p> <h3 id="but-they-work-great-for-audio">But they work great for audio!</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/sashimi-480.webp 480w,/assets/img/2025-07-11-hnet/sashimi-800.webp 800w,/assets/img/2025-07-11-hnet/sashimi-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/sashimi.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Instead, I realized that S4 and U-Nets (and convolutions in general) have a great inductive bias for data that has an underlying notion of uniform sampling rate, namely <strong>perceptual modalities</strong> like audio, images, and video. So <a href="https://www.linkedin.com/in/krandiash/">Karan</a> took this autoregressive U-Net backbone with S4 layers and applied it to modeling/generating raw audio waveforms, where <a href="https://arxiv.org/abs/2202.09729">it worked really well</a> <d-cite key="goel2022raw"></d-cite>!<d-footnote> I'm still proud of the method name and title, my most important contribution to this paper üòÅ</d-footnote></p> <h2 id="a-differentiable-chunking-mechanism">A Differentiable Chunking Mechanism?</h2> <p>Around 2022-2023, I was idly thinking about hierarchies in the back of my mind, and the way I conceptualized the goal was something like this.</p> <h3 id="attention-as-a-primitive">Attention as a primitive</h3> <p>To me the self-attention mechanism could be phrased as: A simple module that captures the ‚Äúprimitive‚Äù of forming <em>dense connections</em> and building <em>functions on sets</em>. What makes it powerful is that it feels simple and foundational; capturing some fundamental transformation in a simple differentiable way that allows it to be scaled.</p> <p>My research is always after these <strong>primitives</strong>: lower-level building blocks that capture fundamental properties and transformations, and can be flexibly applied and scaled. My work on state space models, for example, is really about fleshing out the primitive of <strong>recurrence</strong>, which I also do view as being a fundamental property of the world. (It‚Äôs probably not a very well-known fact that my academic job talk was not about SSMs per se; it was called <em>New Structured Primitives for Machine Learning</em>!)</p> <h3 id="chunking-as-a-primitive">Chunking as a primitive</h3> <p>I thought that <em>chunking</em> was another such fundamental primitive. I thought: would it be possible to invent a simple differentiable building block that encapsulated the idea of chunking? Based on my experience with language modeling, and my overall priors about the world, I felt like this would be an incredibly powerful module that would allow models to organize raw data into more meaningful units. Surely this was a more appropriate way of modeling language and much more!</p> <h3 id="tokenization-as-a-case-study">Tokenization as a case study</h3> <p>Back then, I had already formulated tokenization as a major problem that I wanted to solve. The first reason is, as I explained in my [<a href="/blog/2025/tradeoffs/#should-we-get-rid-of-tokenization">previous blog</a>], an appeal to aesthetics and generality, which to me felt important enough. But the more specific reason was that I felt that the proper way to solve tokenization would be through creating a <em>differentiable chunking mechanism</em>, which would be so much more powerful. This is still the way that I like to pitch the problem now, as some researchers that have talked to me directly might know from the way I hinted at the H-Net project over the last few months.</p> <blockquote> <p>Overcoming tokenization is <strong>not about tokenizers</strong>, but about <strong>learning abstractions</strong>. Discovering a tool that can do this will unlock new capabilities.</p> </blockquote> <h3 id="stuck">Stuck.</h3> <p>At this point, tokenization and chunking became one of my personal ‚Äúholy grail‚Äù problems. But how can one create such a differentiable chunking primitive? Grouping discrete units of data together is, of course, a discrete selection problem, which is generally very hard for differentiable optimization. Unfortunately, <strong>I had absolutely no idea how to do this</strong>. So this was always just an idle thought in the back of my head, and I never actively worked on it.</p> <h2 id="information-based-chunking">Information-based Chunking</h2> <p>In 2024, I started idly thinking about this again; maybe partly motivated by <a href="http://cartesia.ai/">Cartesia</a>‚Äôs focus on audio, where it seemed like differentiable chunking could be very useful.</p> <h3 id="total-information-content-as-a-chunking-strategy">Total information content as a chunking strategy</h3> <p>I finally had an idea for how to do this: I still didn‚Äôt know how to make this process differentiable, but at the very least, perhaps we could come up with some smart general principles for what chunks should look like. It seemed like the key for a chunk to be ‚Äúmeaningful‚Äù is that it should have a certain amount of ‚Äúinformation‚Äù in it. And there‚Äôs an easy proxy to create such constant-information chunks: Simply train a neural network to model the information content of the data (which can be read off of the conditional distributions of a standard autoregressive model, of course) and use that to segment the data, by grouping tokens together until the total negative log-likelihood of that chunk overflowed some threshold. This intuitively had a number of desirable properties, such as</p> <ul> <li>if there were a lot of low-information tokens that are very easy to predict, then there probably isn‚Äôt valuable meaning to them, so they should all be grouped together;</li> <li>if the model is surprised by the next token (such as the beginning of a word, which generally has higher entropy and intrinsic information), then the model would be forced to draw a boundary and start the next chunk, which ideally represented a new unit of meaning.</li> </ul> <p>(Fast forwarding a bit: in H-Net, we don‚Äôt <em>completely</em> understand where and why it decides to draw chunk boundaries, but we did notice a connection to this uncertainty principle; it tends to draw boundaries when surprised. But that‚Äôs the subject for a future post.)</p> <h3 id="the-project-begins">The project begins</h3> <p>About a year ago, in summer 2024, I gave a research talk (an early version of my [<a href="/blog/2025/tradeoffs/">Tradeoffs blog post</a>]) to my academic lab at CMU and highlighted some key problems and approaches that I thought could be important. Solving tokenization and finding a way to create hierarchical models that fit the natural information of the data was one such problem, and I pitched the above approach as a concrete direction to get started.</p> <p>Some students got interested and started thinking about this. I was still very, very interested in the above approach about information-based chunking; <a href="https://www.linkedin.com/in/ricardobuitrago/">Ricardo</a> explored it a little bit but didn‚Äôt get too far as he started working on another project on <a href="https://goombalab.github.io/blog/2025/improving-length-generalization/">length generalization in recurrent models</a> <d-cite key="buitrago2025understanding"></d-cite> (I‚Äôm also very fond of this result!). I kept trying to get Sukjun to try this approach as well, which I was convinced was a good idea‚Ä¶</p> <h2 id="differentiable-selection-with-modern-techniques">Differentiable Selection with Modern Techniques</h2> <p>Fortunately, Sukjun refused to listen to me because he had a much better idea! (A recurring theme in this project‚Ä¶)</p> <p>He wanted to try something loosely inspired by mixture-of-experts (MoE), the idea being that MoE (where each token gets ‚Äúrouted‚Äù to a fixed set of $k$ ‚Äúexperts‚Äù) is essentially a discrete selection problem like the one we want to solve.</p> <p>The main advantage of this approach is that it provides some hope for building a <em>differentiable</em> chunking strategy in an end-to-end model, unlike my information-based heuristic which would require training a separate proxy model first. At the time, I actually basically didn‚Äôt know anything at all how MoE worked, so I left Sukjun to his own devices for a long time, and he got the main ideas working relatively quickly. But there were a lot of really confusing training behaviors and instabilities, so he spent a long time building intuition and slowly improving the model.</p> <p>Incidentally, it‚Äôs kind of interesting to me how obvious in hindsight, but not in foresight, the main idea is. As this project developed, and I told a bunch of people about it, their reaction usually went from ‚Äúthis differentiable selection problem seems impossible‚Äù to ‚Äúoh yeah that might work‚Äù as soon as I mentioned the connection to MoE being another variant of a discrete-selection problem.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/whiteboard_1-480.webp 480w,/assets/img/2025-07-11-hnet/whiteboard_1-800.webp 800w,/assets/img/2025-07-11-hnet/whiteboard_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/whiteboard_1.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/whiteboard_2-480.webp 480w,/assets/img/2025-07-11-hnet/whiteboard_2-800.webp 800w,/assets/img/2025-07-11-hnet/whiteboard_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/whiteboard_2.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Found some random whiteboard pictures from late 2024. Looks like on the left we were trying to figure out how to improve signal propagation into the router weights, including considering tricks like norm layers, straight-through estimator, and Gumbel noise. On the right we had just added the <i>per-network norms</i> idea from the paper and also had to properly rescale the initialization of each layer accordingly to balance (I just realized we might have forgotten to report this in the paper; there were <i>so many details</i> to get right, although all motivated by first principles). </div> <h2 id="discovering-prior-and-concurrent-works">Discovering Prior and Concurrent Works</h2> <p>When we started this project, we hadn‚Äôt done a very serious literature search attempt because there are incredibly few papers on this topic ‚Äì I guess it‚Äôs a pretty hard problem after all. Over the course of the project, we discovered two related works that were most directly related, and I expect we‚Äôll get plenty of questions about how they compare.</p> <h3 id="hourglass-and-dynamic-pooling-transformer">Hourglass and Dynamic Pooling Transformer</h3> <p>Around late October 2024, we discovered the <a href="https://arxiv.org/abs/2211.09761">Dynamic Pooling Transformer (DPT)</a> <d-cite key="nawrot2023efficient"></d-cite> (and its prior work, the Hourglass Transformer, which introduced an autoregressive U-Net structure concurrently with S4/SaShiMi). To the best of our knowledge, the DPT is the first, and I think essentially only, prior attempt at building an <strong>end-to-end hierarchical autoregressive model with dynamic chunking</strong>. The overall network structure was a similar autoregressive U-Net as Hourglass, the main difference being the incorporation of a binary-valued <em>boundary predictor</em> that tries to learn where to chunk the data.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/hourglass-480.webp 480w,/assets/img/2025-07-11-hnet/hourglass-800.webp 800w,/assets/img/2025-07-11-hnet/hourglass-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/hourglass.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The Hourglass Transformer</figcaption> </figure> <p>Their motivation for how to overcome the discrete-choice problem was delegating to stochastic reparametrization techniques, namely the Gumbel-softmax trick <d-cite key="maddison2017concrete"></d-cite> <d-cite key="jang2017categorical"></d-cite>. In their paper, this fully end-to-end version didn‚Äôt seem to work too well; it had stability issues and underperformed their alternative variants that relied on more heuristics and supervision. Ultimately, I guess that DPT attempted to tackle this problem somewhat prematurely and important ideas weren‚Äôt available yet (for example, the most popular <a href="https://arxiv.org/abs/2101.03961">Switch Transformer</a> MoE method <d-cite key="fedus2022switch"></d-cite> was pretty new and perhaps not as established yet). But I think this paper deserves more credit for at least making a real attempt at this problem!</p> <p>Actually, before we found this paper we had tried many variations of Gumbel noise already; I‚Äôm rather fond of the Gumbel-softmax trick myself and kept trying to get Sukjun to incorporate it. But it never seemed to help empirically ü•≤.</p> <details><summary>A missed naming opportunity</summary> <p>Just for fun, I‚Äôll mention an earlier working name for the model I considered.</p> <p>I think Hourglass is an aesthetically pleasing name, and also elegantly describes the characteristic shape of the architecture; it‚Äôs essentially equivalent to a U-Net, but refers to the actual tensor shapes (gradually shrinking and then expanding the sequence, like an hourglass figure) instead of the abstract downsampling/upsampling flowchart.</p> <p>I thought about calling our model the <strong>Hourglass Network</strong> (shortened to H-Net), and the inner dynamic chunking module the <strong>TickTok</strong> layer. I really liked this name because there are several layers of meaning to it:</p> <ol> <li>Hourglass still refers to the shape of the hierarchical network. (Calling it ‚ÄúNetwork‚Äù also differentiated from the Hourglass Transformer, I suppose, since one of our important details was using SSMs in the outer layers.)</li> <li>The network structure could be viewed as <em>multi-rate sequence modeling</em> involving modalities or streams of data that <strong>tick</strong> at different rates, such as <em>characters vs. words</em>, or <em>audio vs. phonemes</em>. The <strong>Tick</strong>Tok layer would be the interface between these modalities.</li> <li>The network could also be viewed as performing a form of dynamic <strong>tok</strong>enization that compresses contiguous inputs just as tokenizers do. The Tick<strong>Tok</strong> layer would again be the mechanism that performs this compression. (<em>Dynamic tokenization</em>, instead of dynamic chunking, was also the working name for this project for a long time.)</li> <li>Finally, there‚Äôs a <strong>time</strong>-related theme to all of it: <em>Hourglass</em> Networks would be defined as models that have <em>temporally-dynamic</em> downsampling rates, where the core mechanism is the <em>‚Äútick tock‚Äù</em> layer.</li> </ol> <p>Unfortunately, I was a bit concerned about the name ‚ÄúHourglass‚Äù being confused for parallel lineages of work; it‚Äôs also been used by plenty of other <a href="https://arxiv.org/abs/1603.06937">unrelated</a> <a href="https://arxiv.org/abs/2401.11605">models</a>. ‚ÄúTickTok‚Äù might also have been a little cheesy; and the TikTok name is also of course a well-recognized brand that might be confusing, and has also already been used by a <a href="https://github.com/openai/tiktoken">very popular tokenizer</a>. So we didn‚Äôt go with it. Alas ü•≤</p> <p>(I still liked the H-Net name, so we kept it and later ‚Äúbackronymed‚Äù it to Hierarchical Network. In my head I still like the name H-Net for this Hourglass connection though.)</p> </details> <h3 id="blt">BLT</h3> <p>By December, we had made a lot of progress and pretty much had the main architecture done (in particular, Sukjun had come up with the <em>smoothing module</em> which at the time we were calling the ‚ÄúEMA detokenizer‚Äù; as we show in the paper‚Äôs ablations, this was one of the most important new techniques introduced).</p> <p>At this point, the <a href="https://arxiv.org/abs/2412.09871">Byte Latent Transformer (BLT)</a> <d-cite key="pagnoni2024byte"></d-cite> paper came out, which made a big splash for claiming to solve tokenization end-to-end, through a similar hierarchical architecture using an entropy-based segmentation rule. (Actually, DPT above already proposed similar entropy heuristics, but didn‚Äôt execute it as well.) I think the terminology is a little confusing because they indeed built a model operating ‚Äúend-to-end‚Äù on bytes, but to me, the crux of the problem is <em>learning the dynamic boundaries</em> jointly with the model itself. This is not possible (at least not obviously) with the entropy heuristic and is the main reason why we didn‚Äôt explore this approach. In the H-Net paper, we specifically use ‚Äúend-to-end‚Äù to refer to this stronger definition. Still, it was gratifying to see that my idea to use information-based chunking heuristics does (somewhat) work!</p> <details><summary>A deeper dive into this approach</summary> <p>There is one major difference between the <a href="#total-information-content-as-a-chunking-strategy">original idea I had</a> and what BLT did. Since I‚Äôm not going to work on this approach, I might as well put it here.</p> <p>In my idea, although I also think of it as an ‚Äúentropy-based approach‚Äù, the actual quantity used to determine chunk boundaries is the <em>information content</em> or negative log-likelihood of the next token. In BLT, the quantity used is the <em>conditional entropy</em> $H(x_t | x_0, \ldots, x_{t-1})$.</p> <p>The intuition in both is that segmentations should happen if the model is ‚Äúsurprised‚Äù by the next token. But surprise is better measured by the former quantity, not the latter. A concrete example is any conditional distribution that puts most of its mass on one vocab word $w$; this conditional distribution would have very low entropy and so the conditional entropy heuristic would never create a new chunk. But in the event that the actual token that appears is a different word $v \neq w$, the model should be <em>very</em> surprised and draw a boundary in the rare case this happens. The negative log-likelihood captures this actual information content.</p> <p>I‚Äôm a little confused by why both DPT and BLT used the conditional entropy instead. Maybe I‚Äôm missing something, or for some reason empirically it just works better?</p> </details> <h2 id="a-world-of-improvements">A World of Improvements</h2> <p>I think the architecture has been relatively stable since early 2025, but we spent a long time trying to understand and simplify every part of it. For example (this is just a small subset),</p> <ul> <li><strong>Architecture components</strong>: our final architecture included post-norm layers at the end of every sub-network, as well as linear projections on the residual connection. We spent a long time trying to remove these since I felt like they were non-standard techniques (most U-Nets don‚Äôt have these) that complicated the model. But in the end, they seem very useful‚Äîmaybe essential‚Äîand we kept them.</li> <li><strong>Sparsity auxiliary loss</strong>: Our auxiliary loss targets a specific down-sampling ratio (Section 2.3 of the paper). This seems a little artificial to me and introduces an annoying new hyperparameter (thankfully, the only one!). I felt like it would be cleaner to simply impose a ‚Äúsparsity loss‚Äù that encouraged higher compression rates (it would be counter-balanced by the main loss function which encourages more compute, hence lower compression rates). We found some things that sort of worked but it didn‚Äôt seem as consistent, so for this version of the H-Net we kept the targeted loss.</li> <li><strong>Chunking mechanisms</strong>: We tried so many different variations of the routing module, upsampling step, downsampling step, smoothing module, and every other component. Sukjun‚Äôs PowerPoint deck of ideas and variations has hundreds of slides, and we were only able to report a very small subset of ablations in the paper of things we did.</li> <li><strong>Layer allocation</strong>: Normal LLM scaling laws might be concerned with how to scale the width and depth of the model as the parameter count grows. But here, there are so many more choices for how many layers to put in each sub-network and how wide to make them and so on. Beyond that, the question of which layers to use (e.g. Transformer vs. Mamba) in each network is also not obvious, and required tons of experiments to understand (and I‚Äôm still not sure we have a completely clear grasp of how they predictably affect performance).</li> </ul> <p>Although our final (current) design has a lot of moving parts, they all had concrete motivations, and are as simple as we could make it without sacrificing stability or performance.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/tuning-480.webp 480w,/assets/img/2025-07-11-hnet/tuning-800.webp 800w,/assets/img/2025-07-11-hnet/tuning-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/tuning.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">A random slide from the deck of ideas involving multiple residual connections and extra stages/sub-networks, which remained as flags in the internal code for a long time.</figcaption> </figure> <p>I haven‚Äôt been able to run experiments personally in a while, and it was like watching an artist at work to see Sukjun improve the architecture steadily over time with multiple creative and essential ideas ‚Äì if it isn‚Äôt clear, he has by far the best intuition for everything related to H-Nets.</p> <p>I have to say that this was one of the most difficult empirical projects I‚Äôve witnessed (and I think I‚Äôm somewhat decent at creating architectures üëÄ). Normally when tuning models, one hopes that different axes of variation act somewhat independently so they can be ablated in isolation and we can perform a ‚Äúcoordinate descent in design space‚Äù. But somehow for this architecture, it felt like there were 12 different axes that were all critical and that <em>all had to interact together in exactly the right way</em>. There were so many times when we thought that option ‚ÄúA‚Äù was better than ‚ÄúB‚Äù, before changing some other design decisions and flip-flopping to B, before changing even more things and going back to A. And this happened with every axis of variation. Even after we had all the main ingredients, it was really difficult to get them to interact together seamlessly. The main driving force here was Sukjun‚Äôs intuition and persistence.</p> <h2 id="tradeoffs-of-ssms-and-transformers">Tradeoffs of SSMs and Transformers</h2> <p>In my previous blog post, I argued that we as a community should care about getting rid of tokenization. I hope the H-Net has done that, or at least shown the first step that it‚Äôs possible. Aside from that, there are a couple of points in the previous post that I‚Äôll specifically call back to.</p> <h3 id="noisy-data">Noisy data</h3> <p>I proposed [<a href="/blog/2025/tradeoffs/#a-hypothetical-litmus-test">a hypothetical litmus test</a>] involving sequences padded with information-less noise tokens. This was a thought experiment that partially motivated why I was so interested in hierarchical models. The idea is that one should be able to train some lightweight encoder and decoder networks that perform the compression of each chunk, folding each contiguous run of noise tokens into the previous meaningful token. This should be able to compress and strip away the noise tokens before feeding it to a main network that operates just over the meaningful tokens.</p> <p>Can the H-Net actually solve this in practice? I don‚Äôt know, maybe some more work would need to be done. I was more interested in the litmus test as a thought experiment to guide architecture design, not necessarily as an actual synthetic task. I did hypothesize though that something that could theoretically solve the litmus test might be better at working with noisier data, but I‚Äôd say I don‚Äôt have particularly high confidence in this hypothesis (I really don‚Äôt know).</p> <h3 id="ssms-as-compressive-models">SSMs as compressive models</h3> <p>Finally, let me show perhaps my favorite experiment from the paper, even though this is buried somewhere late in the ablations because it‚Äôs subtle.</p> <p>In the previous post, I asked the question: [<a href="/blog/2025/tradeoffs/#compression-bug-feature">Is compression a bug, or a feature?</a>] And I floated the idea that perhaps SSMs had hidden strengths due to their compressive abilities.</p> <p>The more obvious piece of evidence I gave for this was comparing the inductive bias of SSMs and Transformers, for example, contrasting these two facts:</p> <ol> <li>Transformers and SSMs have similar performance on tokenized language</li> <li>Transformers seriously underperform SSMs on untokenized language</li> </ol> <p>My intuitive explanation was that on high-resolution data without meaning (such as characters), attention is a poor inductive bias, and understanding the data requires compressing it. Ablations in the H-Net paper corroborate this: <strong>any parts of the model interacting directly with byte-level input resolution strongly benefit from SSM layers</strong>.</p> <p>This raises a natural question though: is the importance of Mamba layers because</p> <ol> <li>they are better at <strong>processing fine-grained byte inputs</strong>, as we already knew?</li> <li>or because they are better for <strong>compressing information into the next stage</strong>, even if given coarser inputs?</li> </ol> <p>We can disentangle these two hypotheses by simply applying an H-Net on data that‚Äôs not byte-level. After all, it‚Äôs just a generic sequence model architecture that can be applied on any data! In particular, let‚Äôs apply it to data that‚Äôs already BPE-tokenized.</p> <ol> <li>If the first hypothesis holds, then we would expect Mamba to not help in the encoder/decoder, since (as mentioned above) they have similar performance to Transformers on standard tokenized language modeling.</li> <li>If the second hypothesis holds, then we would expect that encoders/decoders using some Mamba layers to be better than pure Transformer layers.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/abl_enc_dec_combination_BPE-480.webp 480w,/assets/img/2025-07-11-hnet/abl_enc_dec_combination_BPE-800.webp 800w,/assets/img/2025-07-11-hnet/abl_enc_dec_combination_BPE-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/abl_enc_dec_combination_BPE.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">A 1-stage H-Net trained on top of standard BPE-tokenized inputs, with T2M2-M2T2 for example denoting 2 Transformer layers and 2 Mamba layers in the encoder (and the reverse in the decoder). Note that Transformer layers are 2x the size of Mamba layers here, so everything is parameter/compute matched.</figcaption> </figure> <p>This figure shows that it‚Äôs indeed the second hypothesis that holds. This is really interesting and to me provides serious evidence that SSMs really are doing something interesting that other models can‚Äôt do, and that <strong>perhaps compression <em>is</em> fundamental to intelligence</strong>.</p> <h2 id="the-future">The Future</h2> <p>There‚Äôs a ton of stuff we could do and ideas that we have that we left on the table. A bunch of questions are pointed out in the Discussions section of the paper, and in my next post, although this is only a subset of the wide range of follow-up directions that I think will be interesting to work on.</p> <p>One question that I expect we‚Äôll get is: do H-Nets <strong>actually scale better</strong>? In the paper, we showed training loss curves, where they display a striking trend of seeming to scale better with more data. We use these as proxies for a qualitative assessment of their scaling behavior, which I do believe is valuable (a justification is in the Discussion section of the paper). I also actually do believe that H-Nets will shift the scaling law curve in a non-trivial way. However, what we did aren‚Äôt formal scaling laws.</p> <p>Why didn‚Äôt we do those? Well, I guess the main reason is limited compute and more so the <a href="#a-world-of-improvements">sheer difficulty</a> of developing this model, which is where all our resources were invested. At no point did it seem worth completely freezing the architecture to run out expensive formal scaling laws, when I think we were able to get an intuitive understanding of their behavior from our proxy protocol; and moreover, when it always seems like there are core questions we don‚Äôt understand and low-hanging improvements to make to the model.</p> <div style="max-width: 600px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/bpb_curve_xl_hnet_white-480.webp 480w,/assets/img/2025-07-11-hnet/bpb_curve_xl_hnet_white-800.webp 800w,/assets/img/2025-07-11-hnet/bpb_curve_xl_hnet_white-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-11-hnet/bpb_curve_xl_hnet_white.png" width="50%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> On a similar note, I realized we should have just ran this out for much more data to see if the gap widens ü§¶üèª <br/> We didn't because we only ran things out enough to follow our protocols and provide signal to improving the model. </div> <p>We have a lot of ideas for where to go from here; I‚Äôll touch on some of these in the <a href="/blog/2025/hnet-future/">next post</a>.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[This post is part of a two-part series. H-Nets: the Past H-Nets: the Future]]></summary></entry><entry><title type="html">On the Tradeoffs of SSMs and Transformers</title><link href="https://goombalab.github.io/blog/2025/tradeoffs/" rel="alternate" type="text/html" title="On the Tradeoffs of SSMs and Transformers"/><published>2025-07-08T00:00:00+00:00</published><updated>2025-07-08T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2025/tradeoffs</id><content type="html" xml:base="https://goombalab.github.io/blog/2025/tradeoffs/"><![CDATA[<p>This blog post was adapted from a talk I‚Äôve given a handful of times over the last year. It was meant to be a high-level talk accessible to a fairly broad audience, but hopefully has some interesting insights, opinions, and intuitions around sequence models for the dedicated researchers too.</p> <h2 id="state-space-models">State Space Models</h2> <p>Just so we‚Äôre on the same page, I‚Äôll start by defining what I mean by a state space model. (This section isn‚Äôt strictly necessary to get to the main part of this post though; feel free to skip directly to <a href="#states-brains-and-databases">the next section</a>.)</p> \[\begin{equation} \label{eq:ssm} \begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} h_t \end{aligned} \end{equation}\] <p>These equations define the (structured) state space model (SSM) as developed in a line of work <d-cite key="gu2023thesis"></d-cite> culminating in Mamba <d-cite key="gu2023mamba"></d-cite>. They can be viewed as a modern version of a recurrent neural network (RNN) with a few key characteristics. While a lot of technical work was involved in getting this family of models to work, I‚Äôll start by trying to abstract away what I view as the main high-level ingredients that made these models successful, e.g. match the performance of Transformers on language modeling.</p> <h3 id="the-three-ingredients">The three ingredients</h3> <h4 id="1-state-size">1. State size</h4> <p>A characteristic of the SSM is that its hidden state $h_t$ has a larger size than the the inputs and outputs $x_t, y_t$. The key idea is that the hidden state of any recurrent model is its only access to the model‚Äôs context (in an autoregressive setting). So for modeling information-dense modalities such as language, the model needs a large enough state to store the relevant information that it wants to access later.</p> <p>In SSMs, if each input $x_t$ is a 1-dimensional scalar, then the hidden state $h_t$ is an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the <em>state size, state dimension, or state expansion factor</em>. This is also known as a SISO (single-input single-output) SSM and allows the models to store $\mathtt{N}$ times as much information as older RNNs such as LSTMs and GRUs <d-cite key="lstm"></d-cite><d-cite key="chung2014empirical"></d-cite>.</p> <h4 id="2-state-expressivity">2. State expressivity</h4> <p>Not only does the model need to have a large enough state to <em>theoretically</em> store relevant context, it needs to have an expressive enough state update function to encode and access exactly the information it needs.</p> <p>Earlier versions of ‚Äúlinear time-invariant‚Äù SSMs used simple recurrences $h_{t} = A h_{t-1} + B x_t$ whose updates are constant at every time step <d-cite key="gu2023thesis"></d-cite>. While this works great for compressible data like audio, it doesn‚Äôt provide enough flexibility for sequences with variable information rates like language, where the model may have to selectively choose what information to remember. <strong>Selective SSMs</strong> like Mamba fix this by making the recurrence more expressive by letting the transition matrices vary through time and depend on the data itself. These mechanisms are closely related to the gating mechanisms of classical RNNs!</p> <p>This is the area with the most active research on modern recurrent models, which are focused on understanding the theoretical expressivity of different parameterizations of the transition matrix $A_t$ and what they allow the model to remember in its state.</p> <h4 id="3-training-efficiency">3. Training efficiency</h4> <p>Having a larger and more expressive recurrent state is important, but comes with a critical trade-off ‚Äì the model becomes much harder to compute. Mamba addressed this with careful parameterization of the recurrence and utilizing the classic parallel scan algorithm<d-cite key="blelloch1990prefix"></d-cite><d-cite key="martin2018parallelizing"></d-cite>.</p> <p>Many other algorithmic innovations have appeared, all with a few shared characteristics:</p> <ul> <li><strong>Parallelization</strong>: They aim to be parallelizable and practically efficient on accelerators like GPUs and TPUs ‚Äì preferably leveraging matrix multiplications (matmuls) as the workhorse.</li> <li><strong>Memory management</strong>: They have to control memory usage carefully. In particular, any model that uses state expansion can‚Äôt actually materialize the state in main memory! While Mamba brute-forced the problem using clever awareness of the GPU memory hierarchy <d-cite key="gu2023mamba"></d-cite>, most alternatives find ways of rewriting the equations entirely to use different computation paths that don‚Äôt need to compute the state explicitly during a parallel training pass.</li> <li><strong>Linearity</strong>: The model generally has to be linear in $x_t$, leading some to call this whole family of models <em>linear recurrent models</em>. Linearity plays a role in both computational efficiency as well as modeling/optimization ability, which I won‚Äôt get into here.</li> </ul> <h3 id="mamba---putting-it-all-together">Mamba - putting it all together</h3> <p>None of these three ingredients is new:</p> <ol> <li>Linear attention <d-cite key="katharopoulos2020transformers"></d-cite><d-cite key="sun2023retentive"></d-cite> and earlier SSMs <d-cite key="gu2021combining"></d-cite><d-cite key="gu2022efficiently"></d-cite> had similar equations utilizing state expansion.</li> <li>Selectivity was inspired by, and closely related to, gating mechanisms in classical RNNs like the LSTM and GRU <d-cite key="lstm"></d-cite><d-cite key="chung2014empirical"></d-cite>.</li> <li>Parallel scans were utilized in earlier SSMs/linear RNNs like S5 <d-cite key="smith2023s5"></d-cite> and LRU <d-cite key="orvieto2023resurrecting"></d-cite>. Linear attention variants also used parallelizable training algorithms leveraging matmuls.</li> </ol> <p>What Mamba did was show that <strong>combining all of these together</strong> was the key to a step change in empirical performance and approaching Transformers on language modeling.</p> <h3 id="modern-recurrent-models">Modern recurrent models</h3> <p>Since then, there‚Äôs been a flurry of activity on continuing to understand and improve recurrent models. Many of them come from different motivations with different nomenclatures and terminologies.</p> <ul> <li>Some models such as RWKV <d-cite key="peng2023rwkv"></d-cite><d-cite key="peng2024eagle"></d-cite><d-cite key="peng2025rwkv"></d-cite>, xLSTM <d-cite key="katharopoulos2020transformers"></d-cite>, and Griffin <d-cite key="de2024griffin"></d-cite> come from an <strong>RNN-centric</strong> point of view and call Ingredient 1 <em>matrix-valued states</em> and Ingredient 2 <em>gating</em>.</li> <li><strong>Linear attention</strong> <d-cite key="katharopoulos2020transformers"></d-cite> first combined Ingredients 1 and 3; later variants such as GLA<d-cite key="yang2024gated"></d-cite> and Gated DeltaNet<d-cite key="yang2025gated"></d-cite> incorporate various forms of selectivity (data-dependent recurrence) and use attention-based terminology such as using $(K, Q, V)$ instead of $(B, C, X)$. Mamba-2 can also be simultaneously seen as either an SSM or a linear attention <d-cite key="dao2024transformers"></d-cite>.</li> <li>Recently, many of these models have been cast into a framework of <strong>test-time training/regression</strong><d-cite key="liu2024longhorn"></d-cite><d-cite key="sun2024learning"></d-cite><d-cite key="wang2025test"></d-cite><d-cite key="von2025mesanet"></d-cite>, which views the recurrent update as online optimization on some objective for remembering the context. The state is viewed as an <em>associative memory</em> and parallelization happens through a notion of <em>minibatch gradient descent</em>.</li> </ul> <p>A core commonality is that almost all of these models can be cast into the same SSM equation \eqref{eq:ssm}, with the main axes of variations being in the structure of $A_t$ (Ingredient 2) and corresponding efficient training algorithms (Ingredient 3). So I‚Äôll use the term <strong>state space model</strong> (or just ‚Äúmodern recurrent model‚Äù) to refer broadly to this large class of new models, as it captures their main shared characteristics (e.g. SISO linear recurrence with state expansion). But of course, there are many other reasonable names given the closely related ideas!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/recurrent_models-480.webp 480w,/assets/img/2025-07-08-tradeoffs/recurrent_models-800.webp 800w,/assets/img/2025-07-08-tradeoffs/recurrent_models-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/recurrent_models.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">This figure is from Songlin Yang's excellent <a href="https://arxiv.org/abs/2406.06484">DeltaNet</a> paper, which shows how the huge proliferation of modern recurrent models all fits into this framework (using linear attention notation).</figcaption> </figure> <p>Despite the accelerating amount of research into this direction and steady stream of new models, however, I think that all of them are still quite similar to each other and have roughly similar empirical performance, for the most part. In particular, <strong>all of these models are much more similar to each other than they are to quadratic attention</strong>. So in the rest of this post, we‚Äôre going to try to get a grasp on the higher-level tradeoffs between SSMs and Transformers.</p> <h2 id="states-databases-and-brains">States, Databases, and Brains</h2> <p>I claim that we can understand the trade-offs of different models better by looking at what they store in (and how they manipulate) their <strong>autoregressive state</strong>.</p> <p>What does that mean? In some sense, every <em>autoregressive model</em> ‚Äì one that generates data sequentially left-to-right like modern LLMs ‚Äì is a ‚Äústate space model‚Äù that holds some state in memory and evolves it on every time step (e.g. in between every generated word for an LLM).</p> <h3 id="autoregressive-states-of-sequence-models">Autoregressive states of sequence models</h3> <p>(Causal) self-attention, the core component of autoregressive Transformers, is often defined through a specific operation involving computing the pairwise interactions between every element of the sequence <d-cite key="vaswani2017attention"></d-cite>. Consequently, its computation cost scales <em>quadratically</em> in the sequence length, which is often viewed as the main drawback of attention.</p> <p>On the other hand, because computing one step of the recurrence \eqref{eq:ssm} takes constant time, processing an entire sequence scales <em>linearly</em> with the length of the sequence, which is often viewed as the main advantage of state space models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/state-480.webp 480w,/assets/img/2025-07-08-tradeoffs/state-800.webp 800w,/assets/img/2025-07-08-tradeoffs/state-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/state.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>But instead of thinking of the training cost of these models, I find it more illuminating to think about what happens at inference time when they process a new input.</p> <ul> <li>When a self-attention layer receives a new token, it needs to compare it to all the previously seen elements of the sequence, which means that <em>it must have cached a representation for every single prior token in the context</em>. Every new input it sees must get added to the cache, which therefore grows linearly in the context size.</li> <li>On the other hand, a state space model has always summarized its context $x_1, \cdots, x_t$ into the hidden state $h_t$ (equation \eqref{eq:ssm}), which always has a constant size. This fixed-size state is the only means by which the model can interact with data: it streams data in, compresses it into its state, and uses that to make decisions or produce new outputs.</li> </ul> <p>Without even getting into the details of the definitions of these various models, I think it‚Äôs roughly accurate to say that we could have defined them from first principles through their autoregressive states:</p> <ul> <li><strong>Transformers (self-attention) are characterized by a state that caches every element of its history</strong>, and interacts with new data by doing a pass over every element of the cache.</li> <li><strong>SSMs are characterized by a state that compresses all its history</strong>, and interacts with new data in an online streaming fashion.</li> </ul> <details><summary>Aside: The ‚ÄúKV‚Äù cache</summary> <p>The Transformer cache is, of course, more formally known as the <strong>KV cache</strong>, where ‚ÄúKV‚Äù refers to specific parts of how attention was first defined and named (key and value).</p> <p>But the point of this description is that I think that rather than defining the KV cache as a consequence of attention; perhaps in an alternative universe, (causal) self-attention could have been derived from first principles as the canonical model that stores a cache (‚ÄúKV‚Äù or not) of its context. So in this post, I mainly call it a ‚Äúcontext cache‚Äù or ‚Äútoken cache‚Äù instead to abstract out the main principle instead of implementation detail.</p> <p>As an aside, it‚Äôs rather interesting/amusing to me that often when I talk to LLM researchers, they call the recurrent state of SSMs a ‚Äútype of KV cache‚Äù rather than calling the KV cache a type of state, which IMO is much more accurate and descriptive.</p> </details> <h3 id="a-coarse-analogy">A coarse analogy</h3> <p>Although SSMs are often viewed as more efficient but somewhat weaker versions of Transformers, it‚Äôs not as simple as that. Even ignoring computational efficiency, these models do have different tradeoffs in their inductive biases (or modeling power). Given the nature of the way they process data, here‚Äôs a rough analogy that I like.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/analogy-480.webp 480w,/assets/img/2025-07-08-tradeoffs/analogy-800.webp 800w,/assets/img/2025-07-08-tradeoffs/analogy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/analogy.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Transformers are like databases</strong>: they treat every new observation as an important item that is filed away for future reference.</p> <p>On the other hand, <strong>SSMs are like brains</strong>: finite-sized memories that are always on, processing new inputs and producing new outputs in real-time.</p> <p>This analogy is a bit superficial, but does help intuitively explain some of the empirical behaviors that are observed. For example, SSMs can‚Äôt memorize a phonebook in one pass and then recite it back, or recall an arbitary person‚Äôs phone number from memory <d-cite key="jelassi2024repeat"></d-cite><d-cite key="waleffe2024empirical"></d-cite>. But then of course, neither can humans ‚Äì we‚Äôre hopelessly bad at exact memorization and retrieval ‚Äì but that doesn‚Äôt seem to hinder intelligence from arising! On the other hand, Transformers have a fundamental hard limit on context length (once the cache size is exceeded), while recurrent models like SSMs can hypothetically maintain an infinitely long (but fuzzy) memory of the past like humans have.</p> <details><summary>Aside: Context compression</summary> <p>The aforementioned limitation on context length might be circumvented by newer context compression techniques, which involve a more clever iterative process of throwing out the entire cache and trying to compress it into a shorter summary, so that new information can be processed that otherwise would overflow the cache. This of course must be lossy, and makes the whole system start resembling an SSM more.</p> <p>Similarly, the limitations of SSM may be alleviated by more clever iterative techniques of interacting with the data. For example, issues with recall might be remedied by giving them another pass over the data ‚Äì just as how humans will look things up in external references.</p> <p>The theme here is that sometimes limitations of methods are not so black-and-white. They can depend on the way in which models are used and more generally on higher system-level changes. But we‚Äôre not going to get into these nuances for the purposes of this post.</p> </details> <details><summary>Aside: Long context</summary> <p>Something worth pointing out is that ‚Äúlong context‚Äù is a very popular, but horribly overloaded and ill-defined term. Both Transformers and SSMs have been touted as having better ‚Äúlong-context abilities‚Äù as a blanket statement, which can‚Äôt both be accurate.</p> <p>The reason is because they have very different <em>types</em> of memory. Going back to the analogy, I wouldn‚Äôt say that there is a clear winner comparing, say, my own memory vs. my research notes. They‚Äôre both just different: my notes lets me refer back to specific details I may have forgotten, but my brain remembers a much longer history of fuzzy context. Transformers and SSMs probably have similar qualitative differences that are difficult to measure.</p> <p>I‚Äôm very curious, for example, if large-scale SSMs (if trained properly with modern <a href="https://goombalab.github.io/blog/2025/improving-length-generalization/">length extrapolation techniques</a> <d-cite key="buitrago2025understanding"></d-cite>) would overcome the finite context problem that some chatbot users have complained about. Maintaining a continual conversation with an assistant is much more like human conversations and relationships: what matters is a long, persistent <em>summary</em> of the context, remembering the <em>shape and flow</em> of the interactions without needing to recall every specific detail. No one needs a scratchpad to have a continual relationship with their friend. This is exactly where the more brain-like nature of SSMs is more suitable than the database-like nature of Transformers, which instead may be better suited for AI tasks requiring precision and retrieval.</p> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-480.webp 480w,/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-800.webp 800w,/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/intelligence_hybrid.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A more intriguing empirical finding that might be predicted from the analogy is that combining both types of information processing may be even more capable! Just as human intelligence is augmented by having explicit scratch pads and external references, language models get better when combining SSMs with attention layers by a simple interleaving strategy.</p> <p>And what‚Äôs even more intriguing is that the optimal ratio of these layers, as independently verified by dozens of research groups by now (<a href="https://arxiv.org/abs/2212.14052">H3</a>, <a href="https://arxiv.org/abs/2403.19887">Jamba</a>, <a href="https://arxiv.org/abs/2405.16712">Zamba</a>, <a href="https://arxiv.org/abs/2406.07522">Samba</a>, and many more that followed after)<d-cite key="dao2023hungry"></d-cite><d-cite key="lieber2024jamba"></d-cite><d-cite key="glorioso2024zamba"></d-cite><d-cite key="ren2025samba"></d-cite>, is somewhere between a roughly 3:1 to 10:1 ratio of SSM:attention layers.<d-footnote>Note that this isn't factoring in computation cost (which is usually what's highlighted when comparing Transformers vs SSMs) - we're just talking about raw modeling ability. Put another way, taking a pure Transformer model and replacing some (or most) of the layers with SSM layers would both improve efficiency *and* performance.</d-footnote> This might track the coarse analogy if one believed that human intelligence is mostly in the brain and augmented by lightweight access to external databases! These hybrid models have now been scaled up to very serious sizes (e.g. MoE with 560B total parameters) by major labs, like NVIDIA‚Äôs <a href="https://research.nvidia.com/labs/adlr/nemotronh/">Nemotron-H</a> <d-cite key="blakeman2025nemotron"></d-cite> and Tencent‚Äôs <a href="https://tencent.github.io/llm.hunyuan.T1/README_EN.html">T1/TurboS</a> <d-cite key="liu2025hunyuan"></d-cite> with state-of-the-art performance.<d-footnote>Fun fact: both of these models were announced on the same day, my birthday üòÇ (completely by coincidence)</d-footnote></p> <details><summary>Aside: Perplexity</summary> <p>When I talk about performance here, I‚Äôm specifically referring to perplexity. As a community, we now know that there are more nuances to the downstream performance, in particular <em>algorithmic capabilities</em> of different types of models<d-cite key="bick2025understanding"></d-cite>. But perplexity is still perhaps the most pure metric of the <em>statistical ability to model language as a distribution of sequences</em>, the original definition of language modeling.</p> <p>I actually believe that pound-for-pound (or FLOP-for-FLOP), SSMs are better than Transformers at modeling language, in this sense. But of course, there are many other downstream capabilities that have other differences and are important to understand.</p> </details> <h2 id="is-attention-all-you-need">Is Attention All You Need?</h2> <p>So <a href="https://arxiv.org/abs/1706.03762">attention is all you need</a>, right? There‚Äôs a perception of Transformers being the ultimate architecture that can learn anything from raw data, the more the better, with having enough compute being the only bottleneck.</p> <blockquote class="block-danger"> <h4 id="myth">Myth</h4> <p>Just throw your data at a Transformer <em>üôÇ</em></p> </blockquote> <p>Well, not quite. Attention is indeed amazing and has become an effective backbone for pretty much all modalities, from its original applications in language to <a href="https://arxiv.org/abs/2010.11929">vision</a> and <a href="https://arxiv.org/abs/2005.08100">audio</a> and beyond<d-cite key="dosovitskiy2021image"></d-cite><d-cite key="gulati2020conformer"></d-cite>. But there is some more nuance to it.</p> <blockquote class="block-tip"> <h4 id="reality">Reality</h4> <p>Attention is most effective on<br/> <strong>pre-compressed data</strong> at the <strong><em>‚Äúright level of abstraction‚Äù</em></strong></p> </blockquote> <p>I claim instead that in order to use a Transformer effectively, the <em>data has to be substantially processed</em>. To support this claim, let‚Äôs look at how they‚Äôre actually used in practice.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/patches-480.webp 480w,/assets/img/2025-07-08-tradeoffs/patches-800.webp 800w,/assets/img/2025-07-08-tradeoffs/patches-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/patches.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/tokenizers-480.webp 480w,/assets/img/2025-07-08-tradeoffs/tokenizers-800.webp 800w,/assets/img/2025-07-08-tradeoffs/tokenizers-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/tokenizers.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In pretty much all real pipelines, raw data is processed by an encoder before being fed to a Transformer, for example:</p> <ul> <li>The <strong>patchification</strong> step in vision pipelines (whether <a href="https://arxiv.org/abs/2010.11929">classification</a> or <a href="https://arxiv.org/abs/2212.09748">generation</a>)<d-cite key="dosovitskiy2021image"></d-cite><d-cite key="peebles2023scalable"></d-cite>.</li> <li>The <strong>tokenization</strong> step of language modeling.</li> </ul> <p>This may seem intuitive: after all, because of the quadratic complexity of attention, of course it makes sense to try to simplify the data (such as shortening input sequences).</p> <p>But my claim is <em>not just about computational efficiency</em>; I‚Äôm making a stronger statement about limitations in <em>modeling power</em>.</p> <p>Let‚Äôs dig in more here.</p> <h3 id="should-we-get-rid-of-tokenization">Should we get rid of tokenization?</h3> <p>Tokenization is a notorious step of all language modeling pipelines (most commonly the ‚ÄúBPE‚Äù algorithm <d-cite key="sennrich2016neural"></d-cite>, which I‚Äôll use interchangeably with ‚Äútokenization‚Äù), where textual data is processed into contiguous chunks, essentially encoding them into coarser features than the raw character-level data. It has a number of failure modes such as the <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">SolidGoldMagikarp</a> edge case and the infamous ‚ÄúHow many R‚Äòs are there in the word ‚Äòstrawberry‚Äô?‚Äù test.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/karpathy-480.webp 480w,/assets/img/2025-07-08-tradeoffs/karpathy-800.webp 800w,/assets/img/2025-07-08-tradeoffs/karpathy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/karpathy.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Taken with permission from the most prominent <a href="https://x.com/karpathy/status/1657949234535211009">hater</a> <a href="https://x.com/karpathy/status/1759996551378940395">of</a> <a href="https://x.com/karpathy/status/1816637781659254908">tokenizers</a>. The enemy of my enemy is a friend of mine!</figcaption> </figure> <p>So why do we use it?</p> <p>From polling a lot of opinions, almost everyone agrees that tokenizers are clunky and ugly, but a ‚Äúnecessary evil‚Äù.<d-footnote>It's kind of uncanny how many people use this exact phrasing!</d-footnote> Practically speaking, they sub-sample the sequence by a factor of around $5\times$ which dramatically improves the efficiency of the core language model. Despite the edge cases ‚Äì which are gradually being understood and patched out ‚Äì they <em>just work</em>, for the most part. It would be <em>nice</em> to get rid of them, but it‚Äôs not worth a dedicated effort.</p> <p>I, on the other hand, <strong>deeply believe that we should get rid of tokenization</strong>. I‚Äôm driven by aesthetics much more than the average person, I‚Äôd guess, and it‚Äôs because I think that they are rooted in intuition and intangible reasons that usually lead to deeper consequences down the line, even if we can‚Äôt predict them. Indeed, an example is all the work I‚Äôm now known for on RNNs/SSMs, which only became useful after years of work, which stemmed from an aesthetic conviction that recurrence is elegant and fundamental. In this case, I think that the consequences of overcoming tokenization <em>will extend far beyond the surface-level implications</em>.</p> <blockquote> <p>We should care about removing tokenization, not (just) for the practical reasons, but for the aesthetic and intangible reasons.</p> </blockquote> <p>Besides fixing the edge cases, removing tokenization simply <strong>adheres closer to the spirit of deep learning</strong>. Deep learning has always been about replacing handcrafted feature engineering with powerful end-to-end neural networks that can learn patterns automatically from data. From CNNs replacing manually engineered edge detectors in computer vision, to Transformers replacing linguistic features in NLP, major advances in AI have always happened with <strong>less data processing and more automatic learning</strong> (as popularly espoused by <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>)<d-cite key="sutton2019bitter"></d-cite>.</p> <p>I believe that replacing tokenization with end-to-end models will have huge consequences for</p> <ul> <li><strong>scaling laws</strong>: learning better patterns from raw data always leads to more powerful models;</li> <li><strong>multilingual and multimodal</strong>: tokenization is notoriously hard or impossible for certain languages and other types of sequential data;</li> <li><strong>reasoning</strong>: because models can learn more semantically meaningful patterns from the data, and reason over higher levels of abstraction;</li> </ul> <p>and much more, some of which I see and some of which I haven‚Äôt thought of yet.</p> <p>(As I was writing this post up, Luca Periƒá released a parallel blog post focused specifically on tokenization and tokenizer-free architectures. <a href="https://lucalp.dev/bitter-lesson-tokenization-and-blt/">Check it out</a>!)</p> <h3 id="so-what-happens-without-tokenization">So what happens without tokenization?</h3> <p>In the modern era of LLMs, there‚Äôve been astonishingly few papers that have thought about or tried to address this problem. It‚Äôs hard to even find trustworthy benchmarks about the performance of tokenizer-free models.</p> <p>So here‚Äôs a plot from our upcoming paper where we carefully ran standard architectures on byte-level language modeling (essentially, treating each English character as a separate token). (Note: Byte-level modeling with Mamba was first attempted by <a href="https://arxiv.org/abs/2401.13660">MambaByte</a> <d-cite key="wang2024mambabyte"></d-cite> from Sasha Rush‚Äôs group. This is a reproduction.)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/bpb_curve-480.webp 480w,/assets/img/2025-07-08-tradeoffs/bpb_curve-800.webp 800w,/assets/img/2025-07-08-tradeoffs/bpb_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/bpb_curve.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Byte-level models trained on FineWeb-Edu (context length 8192). Sliding window attention (width=1024) is FLOP matched to Mamba, while global attention uses $2\times$ the FLOPs.</figcaption> </figure> <p>There are a number of implications here that most LLM researchers seem to find surprising.</p> <p>The first thing to note is that the SSM performs <em>much</em> better than the FLOP-matched Transformer. This might not seem that surprising because byte sequences are much longer than BPE-token sequences, and the quadratic complexity of Transformers kicks in.</p> <p>But as I said earlier, the weakness of Transformers is not just about efficiency, but about modeling power. And what‚Äôs notable about this plot (in particular, focusing on global attention) is that <strong>when matching for <em>data</em> instead of compute, allowing the Transformer to use many more FLOPs, the SSM still outperforms it consistently</strong>!<d-footnote>This plot is the result after we specifically tuned for the global Transformer baseline; in other settings (e.g. different combinations of network width/depth/optimizer hyperparameters), there was a much larger gap between Mamba and global attention.</d-footnote></p> <p>For contrast: if we compared these models on the <em>exact same data, but tokenized</em><d-footnote>This experiment used sequences of 8k characters, which would be roughly 2k tokens long, a standard length for LLMs where we understand the empirical performance of different backbones well.</d-footnote>, their perplexity curves would look approximately the same (or the Transformer would be slightly better), and their FLOPs would also be similar. So keeping the <em>same models</em> and the <em>same data</em>, but simply untokenizing the inputs, simultaneously <strong>lets the Transformer use much more compute</strong> but also <strong>decreases its performance relative to the SSM</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/dna_scaling-480.webp 480w,/assets/img/2025-07-08-tradeoffs/dna_scaling-800.webp 800w,/assets/img/2025-07-08-tradeoffs/dna_scaling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/dna_scaling.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Here‚Äôs another example. This plot is from the original Mamba paper, where we showed that Mamba scaled substantially better than Transformer out-of-the-box on DNA language modeling. Once again, this is a ‚Äútokenization-free‚Äù language with high-resolution input and small vocabulary size (just 4!), and the SSM strongly outperforms the Transformer when <em>data-matched</em> (while using less compute).</p> <p>(By the way, I hypothesize that these results for tokenizer-free models would hold for any reasonable variant of SSMs, such as probably most of the [<a href="#modern-recurrent-models">modern recurrent models</a>].)</p> <h3 id="a-heuristic-explanation">A heuristic explanation</h3> <p>A useful model of what‚Äôs happening is to turn back to the autoregressive state. In a nutshell, because Transformers have an explicit cache of all prior tokens, they have an <strong>inductive bias to pay attention to individual tokens</strong>.<d-footnote>To be precise, though, while the first layer sees individual tokens, token boundaries then get blurred in later layers as each sequence position can represent some combination of all tokens before it. This is meant to be an intuitive heuristic, not a mechanistic explanation.</d-footnote> Or, perhaps more succinctly:</p> <blockquote> <p>The <strong>inductive bias</strong> of soft attention is <strong>hard attention</strong>.</p> </blockquote> <p>Here are some useful heuristics for when attention is naturally suited to the task:</p> <ul> <li>Does caching a representation for every ‚Äútoken‚Äù of data make sense?</li> <li>Does hard attention (focusing on or recalling an individual token) make sense?</li> </ul> <p>These questions point at the following idea: <strong>is each individual token semantically meaningful?</strong> For example, when reading language, we pay attention to units at the level of words (or subwords like prefixes/suffixes), which have <em>meaning</em>. But on the other hand, when this doesn‚Äôt hold ‚Äì for example, it‚Äôs rare that we‚Äôd ever want to pay attention to an individual <em>character</em> when reading ‚Äì the performance of attention suffers.<d-footnote>A slightly different explanation that some would propose is that attention simply gets confused by distractors in general, which is exacerbated when the data is too high-resolution, like at the character level. This explanation is also useful and I think actually points to the same underlying principle as mine.</d-footnote><d-footnote>On a related note, another researcher hypothesized (and has preliminary evidence) that SSMs may be less prone to hallucination than Transformers. It hasn't been fully fleshed out, but if true would make sense from this intuition.</d-footnote></p> <p>What‚Äôs interesting is thinking about many other types of data which lie somewhere in between. For example, image patches can be quite meaningful when they capture some feature, but often can be useless or only partially meaningful.</p> <table> <thead> <tr> <th>Data</th> <th>Is a token ‚Äúmeaningful‚Äù?</th> </tr> </thead> <tbody> <tr> <td>Words / subword tokens</td> <td>:heavy_check_mark:</td> </tr> <tr> <td>Characters</td> <td>:x:</td> </tr> <tr> <td>DNA base pairs</td> <td>:x:</td> </tr> <tr> <td>Image, video, audio patches</td> <td>:question:</td> </tr> <tr> <td>Time series datapoints</td> <td>:question:</td> </tr> </tbody> </table> <p>This is why I do think that attention is indispensable for data like tokenized language, which has largely been processed to a degree of meaning.<d-footnote>Many people will nitpick about whether BPE tokens represent any meaning. For sure they don't -- which is again a major reason I think tokenization needs to go. But to some approximation they do tend to find important repeated subwords like prefixes; and moreover there are a lot of hacks built-in, such as first segmenting on whitespace so that tokens can't cross word boundaries (which is very important to its performance; another indicator of just how broken tokenization is). So in practice, LLM vocabularies tend to contain lots of actual words, which could be considered "meaningful".</d-footnote></p> <p>On the other hand, when the data is generally not meaningful (in the sense of requiring a model to pay attention to individual units), such as character-level language or DNA<d-footnote>I'm aware that sometimes you do need to pay attention to individual characters or base pairs, and that understanding the interactions of single base pairs is actually a big problem for machine learning on DNA. This heuristic is a deliberate oversimplification that I still think is generally useful.</d-footnote>, Transformers don‚Äôt work well, and other models like SSMs hold a clear edge. SSMs in particular, with their compressed states, may be particularly suited for these because when data appears at resolutions that are too high to be useful, what the model needs to do is <strong>compress the data into more meaningful abstractions</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/applications-480.webp 480w,/assets/img/2025-07-08-tradeoffs/applications-800.webp 800w,/assets/img/2025-07-08-tradeoffs/applications-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/applications.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Mamba applications in the first 6 months after its release.</figcaption> </figure> <p>The above figure, which was helpfully sent to me by the hosts of <a href="https://www.cognitiverevolution.ai/">The Cognitive Revolution</a> podcast, shows the breakdown of where Mamba was actually used after being published. Despite being motivated by and focusing on language modeling in the paper, the majority of its applications were actually in other modalities!<d-footnote>I don't work in computer vision, and part of me is unsure how much of Mamba's popularity there is just trend following üòú but I've been told, at least, that SSMs work pretty well!</d-footnote><d-footnote>Without giving away too much, I can also say that they are certainly <a href="https://cartesia.ai">very powerful for audio</a> if understood well and used correctly.</d-footnote> I think this is probably related to the above explanation: it‚Äôs very hard to find good ‚Äútokenizers‚Äù that provide meaning in data like time series, audio, and vision. And models that naturally compress, like SSMs, may have an advantage in inductive bias over Transformers.</p> <p>These heuristics are, of course, very unrefined, and I‚Äôm sure many researchers would take issue with this depiction. But I‚Äôve found it helpful for intuition and has been pretty good at predicting when various models are effective in practice.</p> <details><summary>Aside: Theories of tokenization</summary> <p>As people start thinking about tokenization more, there are some interesting theoretical results that have emerged which support this central thesis (that Transformers require meaningful tokens).</p> <ol> <li> <p><a href="https://arxiv.org/abs/2402.18376">Tokenization Is More Than Compression</a> examined the hypothesis that <em>the primary role of tokenization is to shrink the input sequence length</em>. They invented a new tokenizer that has even higher compression rates than BPE (actually, they even keep the same vocabulary, but simply find different segmentations that are more compressed) yet leads to worse language models, providing evidence against the hypothesis<d-cite key="schmidt2024tokenization"></d-cite>.</p> </li> <li> <p><a href="https://openreview.net/forum?id=wm9JZq7RCe">An Analysis of Tokenization: Transformers under Markov Data</a> showed that for certain data distributions, applying tokenization qualitatively changes what Transformers can learn. Intuitively, commonly used tokenizers like BPE and Unigram are somewhat based in information-theoretic heuristics, and play a particular role in smoothing out the non-uniform information rate of raw data into a form that‚Äôs more easily processed by a Transformer<d-cite key="rajaraman2024analysis"></d-cite>.</p> </li> </ol> </details> <details><summary>Aside: Do SSMs not need meaningful input?</summary> <p>Of course, working on more meaningful inputs would benefit all models, not just Transformers. But I hypothesize that Transformers particularly rely on it.</p> <p>In one of the iterations that I gave this talk, an audience member asked me the question of what I thought would happen if Transformers or SSMs were run on ‚Äú$n$-gram tokenized‚Äù language (instead of using BPE tokens, divide up the text into fixed windows of $n$ characters) or some other suboptimal tokenization.</p> <p>I predicted that both models would get worse on poorly segmented data, but it would affect SSMs less: in order of performance,</p> <p><code class="language-plaintext highlighter-rouge">Transformer (bad tokens) &lt; SSM (bad tokens) &lt; SSM (good tokens) &lt;= Transformer (good tokens)</code></p> <p>Byte/character-level modeling (equivalent to $n$=1) certainly provides some evidence for this.</p> </details> <h3 id="a-hypothetical-litmus-test">A hypothetical litmus test</h3> <p>Another thought experiment that‚Äôs intrigued me is what happens in the presence of noise. LLM data notoriously requires immense amounts of processing, filtering, and cleaning, but real-world data (and other modalities) aren‚Äôt like that. Humans also learn just fine from noisy data!</p> <p>So, what happens in a very simple scenario where information-less filler tokens are inserted into the sequence?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/thoughtexperiment-480.webp 480w,/assets/img/2025-07-08-tradeoffs/thoughtexperiment-800.webp 800w,/assets/img/2025-07-08-tradeoffs/thoughtexperiment-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/thoughtexperiment.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This figure illustrates a redundancy factor of $2\times$, but of course this can be arbitrarily increased to $k\times$ in the thought experiment. I think this shows another clear failure mode of standard attention: the compute shouldn‚Äôt be scaling as $k^2$, and the (inference) memory certainly shouldn‚Äôt scale in $k$ either ‚Äì caching the noise tokens is pointless.</p> <p>SSMs are much better: as $k$ increases, the memory doesn‚Äôt grow. But it actually doesn‚Äôt fully fix the problem either, as <em>any</em> standard architecture would have compute scaling with $k$ (since every token is processed by the entire model). And so all LLMs suffer from this sort of noise and redundancy.<d-footnote>More recent ideas like mixture-of-depths and other conditional compute approaches may make some progress here, but I think don't sufficiently address it yet and I'm guessing would be brittle.</d-footnote></p> <p>In fact, I think thought experiments like this provide useful litmus tests for what <strong>‚Äúthe right architecture‚Äù</strong> should look like. And I‚Äôll informally propose this one as a goal for the future of architecture design (maybe someone will help me formalize it in a paper someday?).</p> <blockquote> <h4 id="a-litmus-test">A Litmus Test</h4> <p>An ideal architecture should be able to process this sequence-with-fillers task <strong>without (substantially) increased compute or memory usage</strong>.</p> </blockquote> <p>More generally, suppose we have two copies of a dataset, one of which contains a lot of extra noise, but overall they have essentially the same ‚Äúinformation content‚Äù. We should expect ‚Äúthe right architecture‚Äù to behave essentially identically on both of these data sets.</p> <details><summary>Aside: Convolutions for language modeling</summary> <p>On a somewhat tangential note, I originally came up with the thought experiment in the figure above years ago, as a means to convince myself that convolutions don‚Äôt work for language modeling. When <a href="https://arxiv.org/abs/2111.00396">S4</a> was published, the community was excited about its potential on various modalities, and it spawned a wave of follow-up work on <a href="https://arxiv.org/abs/2302.10866">pure convolutional language models</a> <d-cite key="gu2022efficiently"></d-cite><d-cite key="poli2023hyena"></d-cite>.</p> <p>But over the course of working on linear time-invariant SSMs, I quickly realized they were hopeless for language modeling. This example shows why: because language doesn‚Äôt have an intrinsic ‚Äúsampling rate‚Äù, tokens can be spaced somewhat arbitrarily. Clearly, even simple mis-spacings would drastically change what features a convolution could pick up on ‚Äì in the above example, the convolution could not possibly output the same feature on both of those input sequences, in contrast to input-dependent sequence mixing layers like attention or selective SSMs.</p> <p>On the other hand, convolutions exhibit strong inductive bias exactly when there‚Äôs a notion of sampling rate that spaces inputs out at a consistent rate. This is another way of phrasing the ‚Äúshift equivariant‚Äù inductive bias that makes them so great for (raw) perceptual modalities like vision and audio.</p> </details> <h3 id="is-attention-all-you-need-redux">Is attention all you need? (redux)</h3> <p>So through these discussions and examples, hopefully I‚Äôve made a case for my original claim, which I‚Äôll repeat here:</p> <blockquote> <p>Attention is most effective on<br/> <strong>pre-compressed data</strong> at the <strong><em><span style="color:red">‚Äúright level of abstraction‚Äù</span></em></strong></p> </blockquote> <p>This is, of course, an oversimplification of the picture ‚Äì and I wouldn‚Äôt even know how to try to formally define a ‚Äúlevel of abstraction‚Äù ‚Äì but I do believe this is true in some fuzzy sense.</p> <h2 id="the-tradeoffs-of-state-space-models-and-transformers">The Tradeoffs of State Space Models and Transformers</h2> <p>Let‚Äôs finally return to the main topic for this blog post.</p> <h3 id="state-space-models-1">State space models</h3> <p id="compression-bug-feature">The trade-offs of SSMs are pretty clear from thinking intuitively about its autoregressive state.</p> <blockquote class="block-tip"> <h4 id="the-strength">The Strength</h4> <p>SSMs are the natural <strong>stateful model</strong> with efficient, interactive, online processing.</p> </blockquote> <blockquote class="block-danger"> <h4 id="the-weakness">The Weakness</h4> <p>SSMs lack fine-grained <strong>recall and retrieval</strong> abilities.</p> </blockquote> <p>Both of these are two sides of the same coin ‚Äì consequences of its compressed state.</p> <p>I want to note, however, that I think there are strengths that are more subtle, and difficult to measure or even articulate.</p> <p>Going back to the [<a href="#a-coarse-analogy">brain analogy</a>], one question that intrigues me is whether <strong>compression is actually fundamental to intelligence</strong>.<d-footnote>My student Isaac has explored this hypothesis from a different angle: <a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html">[link]</a></d-footnote> Is it possible that forcing information into a smaller state forces a model to learn more useful patterns and abstractions? While compressed states are often viewed as a <a href="https://arxiv.org/abs/2402.01032">drawback</a> in the literature<d-cite key="jelassi2024repeat"></d-cite>, I think it might be because it‚Äôs very easy to measure these particular weaknesses but very hard to measure more subtle qualitative effects.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/bugfeature-480.webp 480w,/assets/img/2025-07-08-tradeoffs/bugfeature-800.webp 800w,/assets/img/2025-07-08-tradeoffs/bugfeature-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/bugfeature.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>At any rate, there are certainly many interesting applications where SSMs are the right tool for the job right now. And in my lab‚Äôs next release, we‚Äôll show another interesting and important use case (for language!) where the <em>compressive inductive bias of SSMs turns out to be essential</em>. Stay tuned!</p> <h3 id="transformers">Transformers</h3> <p>Transformers perform exceptionally well, and in fact are pretty much the only tool for the job, on tasks that require paying attention to individual tokens in the context.</p> <blockquote class="block-tip"> <h4 id="the-strength-1">The Strength</h4> <p>Transformers have <strong>perfect recall</strong> and <strong>fine-grained manipulation</strong> of individual tokens in their context.</p> </blockquote> <p>And what about the downsides? Everyone knows that the main weakness of Transformers is their quadratic complexity, right?</p> <p>Not exactly. The main theme of this post is to convey that Transformers <em>do have inductive biases</em> that gives them weaknesses in terms of modeling power, not just efficiency. And just as with SSMs, both the high-level strengths and weaknesses of Transformers are two sides of the same coin, consequences of the structure of their autoregressive state: the token cache <strong>maintains the granularity of the input resolution</strong> it‚Äôs given.</p> <blockquote class="block-danger"> <h4 id="the-weakness-1">The Weakness</h4> <p>Transformers are <strong><em>beholden</em></strong> to the <strong>tokens</strong> they are given.</p> </blockquote> <p>In other words, they are more sensitive to the <em>resolution</em> and <em>semantic content</em> of the data. Transformers are characterized by their context cache, which stores a separate representation for every element of the sequence, which means that every element better be useful.</p> <details><summary>Aside: What about efficient attention?</summary> <p>Many variants of attention exist, which have been primarily motivated by the efficiency angle. I think my framing gives us better intuition of how these variants might behave. For example, I hypothesize that the same weakness is present for any variant of attention that maintains an explicit token cache; in particular, for example, any type of sparse attention. The core weakness is still there (and perhaps even exacerbated in the case of sparse attention): the model is biased toward <em>attending</em> to individual tokens.</p> <p>On the other hand, some variants of efficient attention ‚Äúblur‚Äù the boundaries of tokens, including <a href="https://arxiv.org/abs/2006.04768">low-rank approximations</a><d-cite key="wang2020linformer"></d-cite> and any variant of linear attention. (More abstractly, these belong to a larger family of attention variants that make <em><a href="https://arxiv.org/abs/2405.21060">structured approximations</a></em> to the quadratic attention matrix<d-cite key="dao2024transformers"></d-cite>, any of which would have similar properties, I think.) Because of lacking a token-level cache, these models would not have the same weakness and would instead inherit properties much closer to SSMs.</p> <p>Incidentally, this is another more subtle reason why I somewhat prefer using ‚Äústate space model‚Äù or ‚Äúrecurrent model‚Äù as a descriptive term over ‚Äúlinear attention‚Äù.<d-footnote>I still like the term linear attention though for interesting theoretical connections, as well as the historical significance.</d-footnote> To me, the term ‚Äúattention‚Äù is <em>characterized</em> by maintaining a token-resolution state and having access to individual elements ‚Äì in other words, being able to <strong>pay attention</strong> to a single token.</p> </details> <h2 id="scaling-laws">Scaling Laws</h2> <p>To end, let‚Äôs talk about one of the major drivers of the current wave of progress in AI:<br/> <strong>scaling laws</strong>, or the phenomenon that spending more compute on models consistently leads to more capabilities.</p> <p>These laws are always plotted with FLOPs on the x-axis and some measure of performance on the y-axis, with the idea being that the slope of this line measures ‚Äúthe rate at which <strong>compute</strong> is converted into <strong>capabilities</strong>‚Äù. Indeed, I think there‚Äôs a popular viewpoint that Transformers are simply a vehicle that optimally performs this conversion.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/scaling-480.webp 480w,/assets/img/2025-07-08-tradeoffs/scaling-800.webp 800w,/assets/img/2025-07-08-tradeoffs/scaling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/scaling.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And I think this is a great depiction of the goal of architecture research. We‚Äôre simply looking for <strong>the black box that performs this conversion in the best way possible</strong>. From this perspective, there is only one central question:</p> <blockquote> <p>Is my model using its compute wisely?</p> </blockquote> <p>In other words, we want every FLOP to count. And as is hopefully clear after this post (at least, I‚Äôve convinced myself!), Transformers are far from optimal.</p> <details><summary>Aside: Does it actually matter?</summary> <p>There‚Äôs another layer to the picture that I haven‚Äôt touched on, which is the practical efficiency of models. As <a href="https://tridao.me/">Tri</a> likes to frame it, what we actually care about is ‚Äúdollars-to-capabilities‚Äù, which can be factored into (1) ‚Äúdollars-to-FLOPs‚Äù and (2) ‚ÄúFLOPs-to-capabilities‚Äù. One might need to balance these two, for example, by accepting a suboptimal architecture for (2) in return for much more efficient (1). And some might say that Transformers have optimized the combination of these two.</p> <p>I still care primarily about question (2), partly because I personally find it more intellectually interesting, but also because I truly believe there are substantial advances to be made that change the balance even factoring in (1).</p> <p>A second higher-level question touching on whether it actually matters is: do we need to improve on anything to get to AGI/ASI? The answer here might be no ‚Äì tokenized Transformers may very well represent a viable path ‚Äì but I think that finding improvements may either get us there faster, or ultimately lead to more intelligent models.</p> </details> <p>Don‚Äôt get me wrong: despite being known as a leader of the Transformer-alternatives direction, I think Transformers are amazing and <em>attention is truly a fundamental modeling primitive</em>. But I also think it‚Äôs clear that they, by themselves, are not the final solution. We still have work to do.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/meme-480.webp 480w,/assets/img/2025-07-08-tradeoffs/meme-800.webp 800w,/assets/img/2025-07-08-tradeoffs/meme-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/meme.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="whats-next">What‚Äôs next</h3> <p>Part of my reason for writing this post was to broadcast this content to a wider audience, and so this talk can be sunsetted :)</p> <p>But it‚Äôs also setting up for the next major architecture advancement‚Ä¶</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>Thanks to Tri Dao and Luca Periƒá for feedback on this post.</p> <p>The MambaByte vs. LlamaByte plot is from Sukjun Hwang from his upcoming paper.</p> <h3 id="cite-this-post">Cite this post</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@online{gu2025tradeoffs,
  author  = {Albert Gu},
  title   = {On the Tradeoffs of State Space Models and Transformers},
  year    = {2025},
  url     = {https://goombalab.github.io/blog/2025/tradeoffs/},
}
</code></pre></div></div>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[(or - tokens are bs)]]></summary></entry><entry><title type="html">Understanding and Improving Length Generalization in Recurrent Models</title><link href="https://goombalab.github.io/blog/2025/improving-length-generalization/" rel="alternate" type="text/html" title="Understanding and Improving Length Generalization in Recurrent Models"/><published>2025-07-06T00:00:00+00:00</published><updated>2025-07-06T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2025/improving-length-generalization</id><content type="html" xml:base="https://goombalab.github.io/blog/2025/improving-length-generalization/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2507.02782">Paper</a>]</p> <h2 id="existing-recurrent-models-still-fall-short">Existing Recurrent Models Still Fall Short</h2> <div style="text-align: justify; margin-bottom: 1em;"> Linear recurrent models such as Mamba <d-cite key="mamba"></d-cite><d-cite key="mamba2"></d-cite> and linear attention <d-cite key="LA_katharopoulos2020transformers"></d-cite><d-cite key="RQKV"></d-cite><d-cite key="gated_linear_attention_yang2024gla"></d-cite><d-cite key="delta_net"></d-cite> possess <strong>a remarkable feature: they can process extremely long sequences</strong>, which is key for applications that require long context reasoning (like summarizing long texts or agents with long term memory). Indeed, this is their key advantage over their main competitor, the Transformers <d-cite key="attention_is_all_you_need"></d-cite>, which are bottlenecked by their finite context window and quadratic complexity over the sequence length. </div> <div style="text-align: justify; margin-bottom: 1em;"> Previously, the issue with recurrent models was their performance: on short sequences they were less capable than Transformers. But recent architecture breakthroughs have improved the performance of recurrent models and brought them on par with Transformers, to the point that they are currently used in several industry applications like audio modeling <d-cite key="goel2024sonic"></d-cite> or code completion <d-cite key="mistral2024codestral"></d-cite>. However, several recent works have found out that <em>recurrent models still fall short</em>: they might have comparable performance to Transformers, <strong> but in many cases they struggle to generalize past the training length.</strong> </div> <div style="text-align: justify; margin-bottom: 1em;"> Indeed, we show the performance of the official Mamba-2 checkpoints <d-cite key="mamba2"></d-cite> as a function of the sequence position $t$ (using perplexity, the lower the better). It can be seen that for positions $t$ beyond the training context $T=2048$, these models become virtually useless: they fail to <em>length generalize</em>. </div> <div style="max-width: 500px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2-480.webp 480w,/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2-800.webp 800w,/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: justify; margin-bottom: 1em;"> This is an issue: existing recurrent models have low performance on long sequences, and are not much more efficient than Transformers in shorter sequences; so they seem to be falling short on both sides. </div> <div style="text-align: justify; margin-bottom: 1em;"> Does this mean that recurrent models are useless? Not at all! In our work, we show that <strong>length generalization is easily achievable in recurrent models through simple training interventions: post-training for 500 steps (~0.1% of the pre-training budget) enables length generalization in up to 256k sequences!</strong> Therefore, recurrent models possess an <em>unrealised potential</em> rather than a <em>fundamental limitation</em>. </div> <h2 id="why-do-recurrent-models-fail-to-length-generalize-the-unexplored-states-hypothesis">Why Do Recurrent Models Fail to Length Generalize? The <em>Unexplored States Hypothesis</em></h2> <div style="text-align: justify; margin-bottom: 1em;"> For an input sequence with $t$ elements $(x_1, x_2, ..., x_{t-1}, x_t)$, recurrent models compress the input context $(x_1, x_2, ..., x_{t-1})$ into a fixed-size <em>recurrent state</em> $h_{t-1}$. At time $t=0$, the state is initialized with some value $h_{-1}$, and then it is updated at each $t$ with an update function $f$: </div> <div style="text-align: center;"> $ h_t = f(h_{t-1}, x_t) $ </div> <div style="text-align: justify; margin-bottom: 1em;"> Similarly, the output at time $t$ only depends on the state $h_t$ and the current input $x_t$, i.e. for some other function $g$ the output $y_t$ can be written as </div> <div style="text-align: center;"> $y_t = g(h_t, x_t)$ </div> <div style="text-align: justify; margin-bottom: 1em;"> The functions $f$ and $g$ do not depend on the position $t$, so in theory recurrent models can naturally process any sequence length. But then, how can it be that they fail when $t$ is large? </div> <div style="text-align: justify; margin-bottom: 1em;"> In our work we show that <strong>the distribution of the state $h_t$ changes over time</strong>. Therefore, even if $g$ and $f$ work correctly up to some $T$, other $h_t$ with $t&gt;T$ might be significantly different, and thus the model fails to produce the correct output. Indeed, in the following figure we show how the norm of the state of Mamba-2 <d-cite key="mamba2"></d-cite> increases significantly over time: </div> <div style="max-width: 400px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/statemetrics_base-480.webp 480w,/assets/img/2025-07-06-length-generalization/statemetrics_base-800.webp 800w,/assets/img/2025-07-06-length-generalization/statemetrics_base-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/statemetrics_base.png" width="0.1" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: justify; margin-bottom: 1em;"> This explains why recurrent models fail to length generalize: when processing sequences longer than those seen during training, they encounter states $h_t$ that have not been explored during training, and thus they have not learnt to process them. Based on this insight, we propose the <strong>unexplored states hypothesis</strong> to explain the failure to length generalize: </div> <blockquote> <h4 id="unexplored-states-hypothesis">Unexplored States Hypothesis</h4> <div style="text-align: justify; margin-bottom: 1em;">Recurrent models fail to length generalize when they are trained only on a <strong>subset of all attainable state distributions</strong>&mdash;i.e. on a subset of the states that would be attained if the state recurrence was rolled out indefinitely. </div> <div style="text-align: justify; margin-bottom: 1em;"> When trained for long enough, the models <strong>overfit to this subset</strong> and perform poorly on long sequences because they <strong>encounter unexplored state distributions</strong>. </div> </blockquote> <h2 id="interventions-to-enable-length-generalization">Interventions to Enable Length Generalization</h2> <div style="text-align: justify; margin-bottom: 1em;"> The unexplored states hypothesis indicates that length generalization can be achieved not by changing the architecture or its mechanisms, but by training the model on a more diverse set of state distributions&mdash;in particular, on the distributions that arise when rolling out the state recurrence on long sequences. To do so, we could directly train the model on longer sequences, but this might not always be possible due to GPU memory constraints or due to lack of sufficiently long training sequences. </div> <blockquote class="block-tip"> <h4 id="the-recipe-to-achieve-length-generalization-interventions-on-the-initial-state">The recipe to achieve length generalization: interventions on the initial state</h4> <div style="text-align: justify; margin-bottom: 1em;"> Most modern architectures assume a zero initial state ($h_{-1}=0$). In our work, we consider four simple interventions on the <strong>initial state</strong> $h_{-1}$, which increase the diversity of states that the model explores during training without the need of training on longer sequences.</div> </blockquote> <div style="text-align: justify; margin-bottom: 0.5em;"> The four training interventions can be seen as sampling the initial state $h_{-1}$ from four different distributions that progressively get closer to the distribution of attainable states: </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 1. <strong>Random Noise</strong>: The state is initialized with an IID Gaussian with zero mean and a constant standard deviation (using the same mean / standard deviation for all layers and heads). </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 2. <strong>Fitted Noise</strong>: During training, we record the mean and standard deviation of the final states of the sequences across all layers and heads. Then, we initialize the state with an IID Gaussian distribution with mean and standard deviation fitted to the ones seen during training (using a different mean / standard deviation for each layer and head). </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 3. <strong>State Passing (SP)</strong><sup id="fnref1"><a href="#fn1">1</a></sup>: We use the final state of a previous (unrelated) sequence as the initial state. These final states are obtained by applying the state recurrence on a given sequence, <em>attaining</em> $h_T$ and using it as $h_{-1}$ for another sequence. This is similar to what happens at validation: the model doesn't stop at $T$, but rather keeps rolling the state and producing outputs from $h_T$. </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 4. <strong>Truncated Backpropagation Through Time (TBTT)</strong> <d-cite key="TBTT_1990"></d-cite> <d-cite key="TBTT_sutskever"></d-cite>: In this case, we split a long sequence into smaller chunks, and use the final state of each chunk as the initial state of the next one. This is equivalent to processing the whole sequence, yet stopping the gradient propagation between chunks. </div> <details><summary>Difference between SP and TBTT</summary> <p>For simplicity, we implement SP by using the final state of the previous batch of sequences as the initial state of the new one. Thus, in practice the only difference between SP and TBTT is that TBTT requires carefully setting up the dataloader so that the sequences of the previous batch correspond to the prior parts of the sequences in the new batch.</p> </details> <div style="text-align: justify; margin-bottom: 1em;"> The following figures show the results of post-training the official Mamba-2 models for 500 steps (~0.1% of pre-training budget) with each intervention: </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/interventions_2-480.webp 480w,/assets/img/2025-07-06-length-generalization/interventions_2-800.webp 800w,/assets/img/2025-07-06-length-generalization/interventions_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/interventions_2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/interventions_1-480.webp 480w,/assets/img/2025-07-06-length-generalization/interventions_1-800.webp 800w,/assets/img/2025-07-06-length-generalization/interventions_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/interventions_1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="takeaway-1-sp-and-tbtt-enable-length-generalization">Takeaway #1: SP and TBTT enable length generalization</h3> <p>State Passing and TBTT ‚Äì which are the interventions that are closer to realistic states ‚Äì allow length generalization in sequences much longer than those seen during training. Thus:</p> <blockquote class="block-tip"> <h4 id="takeaway">Takeaway</h4> <div style="text-align: justify; margin-bottom: 0.5em;"> Length generalization is expected to be <strong>readily achievable in recurrent models</strong> through <strong>simple training interventions</strong>.</div> </blockquote> <p>Note that our results were achieved <em>with only ~0.02% of the original pre-training budget</em>!</p> <h3 id="takeaway-2-properties-of-the-state-of-recurrent-models">Takeaway #2: Properties of the state of recurrent models</h3> <blockquote class="block-tip"> <h4 id="takeaway-1">Takeaway</h4> <div style="text-align: justify; margin-bottom: 0.5em;"> We can infer properties of the <strong>distribution of the state</strong> of recurrent models by looking at the <strong>performance of the interventions</strong></div> <p>.</p> </blockquote> <div style="text-align: justify; margin-bottom: 1em;"> The Random Noise intervention fails to length generalize in the 370m, whereas Fitted Noise works. This suggests that for the 370m model the distribution of attainable states cannot be approximated with a Gaussian with fixed variance, but it can be approximated with an IID Gaussian with fitted variance in each layer and head of the state. However, the Fitted Noise intervention fails to achieve length generalization in the 1.3b model, indicating that the state of large models probably has complex dependency relationships among its elements and thus cannot be approximated with IID values. </div> <div style="text-align: justify; margin-bottom: 1em;"> Additionally, the interventions also fix the increasing state norm behavior we showed before, by making the model output states with similar norm at all timesteps: </div> <div style="max-width: 400px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/statemetrics_full-480.webp 480w,/assets/img/2025-07-06-length-generalization/statemetrics_full-800.webp 800w,/assets/img/2025-07-06-length-generalization/statemetrics_full-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/statemetrics_full.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <details><summary>SP in prior works</summary> <p id="fn1"> <sup>1</sup> Prior works have used the State Passing technique <d-cite key="longssm-wang2024longssmlengthextensionstatespace"></d-cite><d-cite key="end_to_end_bansal2022end"></d-cite>, yet it was applied to different recurrent architectures (e.g. time-invariant ones) or to tasks different to text modeling. To the best of our knowledge, we are the first to show that this technique used as a training intervention can greatly improve the length generalization of several recurrent models, and that it is as effective as TBTT in text modeling. <a href="#fnref1">‚Ü©</a> </p> </details> <h2 id="performance-on-long-context-tasks">Performance on Long Context Tasks</h2> <div style="text-align: justify; margin-bottom: 1em;"> We have seen that the interventions enable length <em>robustness</em> (i.e. not having decreased peformance after the training context $T$), but it is not clear whether they enable length <em>generalization</em> (i.e. solving tasks that require exploiting relationships between tokens that are separated by more than $T$ positions). One may wonder whether the interventions enable length robustness by simply preventing the model from reasoning beyond the training context length&mdash;similar to sliding window attention, which can't reason over tokens separated by more than the sliding window&mdash;in which case the models would have constant performance for all evaluation contexts $t &gt; T$, but could not solve tasks that require long context reasoning. In our work we show that <strong>the interventions do enable length generalization</strong> by showing results on three long context tasks. </div> <div style="text-align: justify; margin-bottom: 1em;"> <strong>BABILong</strong><d-cite key="babilong"></d-cite>. BABILong is a challenging benchmark which tests both the common sense understanding of a model as well as its ability to capture long range dependencies in text. In the figure below it can be observed that <strong>State Passing enhances the length generalization of the model in both the few-shot and finetuned settings</strong> (we recall that the model is trained and finetuned on sequences of length 2048). Therefore, State Passing is not only useful in fixing the diverging perplexity of established language models, but also in enhancing their ability to solve long context reasoning tasks. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/babilong-480.webp 480w,/assets/img/2025-07-06-length-generalization/babilong-800.webp 800w,/assets/img/2025-07-06-length-generalization/babilong-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/babilong.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="text-align: justify; margin-bottom: 1em;"> <strong>Passkey retrieval</strong><d-cite key="landmark_attention_mohtashami2023randomaccess"></d-cite>. The passkey retrieval task requires the model to retrieve a 5-digit passkey inserted at a given depth of a long context. In the figure below we show the performance of the Mamba-2 370m and 780m official checkpoints in three settings: zero shot, regular finetuning, and finetuning with fitted noise<sup id="fnref2"><a href="#fn2">2</a></sup>. The models finetuned with fitted noise are capable of exploiting relationships between tokens that are much more than 2048 positions apart (the training context length). In particular, <strong>the 780m model can solve the passkey perfectly for sequences of length 256k</strong>. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/passkey_2-480.webp 480w,/assets/img/2025-07-06-length-generalization/passkey_2-800.webp 800w,/assets/img/2025-07-06-length-generalization/passkey_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/passkey_2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <details><summary>Choice of intervention for passkey retrieval</summary> <p id="fn2"> <sup>2</sup> Contrary to typical language modeling datasets, the distribution of tokens in the passkey task is not stationary (in other words, there is not a well defined behavior for what the model should do after revealing the passkey). This is why we show results for the fitted noise intervention, as it does not require using the final state of a sequence (i.e., right after revealing the passkey), which might not be appropriate as the initial state.. <a href="#fnref2">‚Ü©</a> </p> </details> <div style="text-align: justify; margin-bottom: 1em;"> <strong>Synthetic Copying</strong><d-cite key="transformers-better-copying-pmlr-v235-jelassi24a"></d-cite>. The synthetic copying task consists in copying an arbitrary sequence of tokens. In the table below we show that using State Passing during training greatly improves the validation performance in sequences more than three times longer. Thus, <strong>state passing helps the model length generalize, solving long context tasks that are harder than those seen during training</strong>. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/synthetic_copying-480.webp 480w,/assets/img/2025-07-06-length-generalization/synthetic_copying-800.webp 800w,/assets/img/2025-07-06-length-generalization/synthetic_copying-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/synthetic_copying.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="a-deeper-look-into-how-recurrent-models-process-context">A Deeper Look into How Recurrent Models Process Context</h2> <div style="text-align: justify; margin-bottom: 1em;"> We have shown that the interventions on the initial state enable length robustness and allow solving long context tasks. On top of these findings, we now present a metric that sheds light on how sequence models process their context. </div> <div style="text-align: justify; margin-bottom: 1em;"> Ideally, in the case of text modeling we would like the model to pay attention to the recent context, and not focus too much on tokens that are too far away. But how can we quantify this behavior? We introduce <strong>Effective Remembrance</strong> to measure <strong>how much an autoregressive model is "effectively" remembering previous tokens</strong>. Denote by $q(\cdot \| \text{context})$ the probabilities that an autoregressive sequential model outputs for the next token given a context. Then, we define: </div> <div style="text-align: center;"> $ \text{EffRem}_T(t) = d(q(\cdot | x[0:T],q(\cdot | x[t:T])) $ </div> <div style="text-align: justify; margin-bottom: 1em;"> Where \( d(p,\bar{p}) \) is a distance between probability distributions (e.g., Total Variation). \(\text{EffRem}_T(t)\) roughly measures how much the model "effectively remembers" the tokens \( x[0:t-1] \) at time \( T \). If \( \text{EffRem}_T(t) = 0 \), this means that the predictions using \( x[t:T] \) and using \( x[0:T] \) are the same, meaning that <strong>the model does not "effectively remember" any of the past tokens \( x[0:t-1] \)</strong>. Conversely, if \( \text{EffRem}_T(t) \) is high, <strong>the model is substantially influenced by the tokens \( x[0:t-1] \)</strong>, since removing them from the context changes the prediction significantly. </div> <div style="text-align: justify; margin-bottom: 1em;"> The following figure shows $\text{EffRem}_T(t)$ for two official Mamba-2 checkpoints (<strong>which fail to length generalize</strong>) for varying $t$ and $T=8192$ (four times the training context): </div> <div style="max-width: 500px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/effrem_undesirable-480.webp 480w,/assets/img/2025-07-06-length-generalization/effrem_undesirable-800.webp 800w,/assets/img/2025-07-06-length-generalization/effrem_undesirable-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/effrem_undesirable.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: justify; margin-bottom: 1em;"> Intuitively we would expect that while every token contributes to the model‚Äôs output, the most recent tokens should have a significantly stronger influence. However, notice how the $\text{EffRem}$ curves immediately jump up and then gradually taper off. This behavior is clearly problematic: the next-token prediction at time $T=8192$ shouldn't change drastically depending on whether the model sees only the recent tokens \( x[4096:8192] \) or the full sequence \( x[0:8192] \). In natural language, the model should primarily rely on recent context, and earlier tokens \( x[0:4096] \) shouldn't completely alter the prediction&mdash;especially not to the extent that the total variation between the two output probability distributions approaches 1. This means that the model is disproportionately influenced by tokens at the beginning of the sequence. </div> <blockquote class="block-tip"> <h4 id="intuition">Intuition</h4> <div style="text-align: justify; margin-bottom: 1em;">We hypothesize that when a model is always trained with a zero initial state, it uses the <strong>first few tokens it sees</strong> to rapidly differentiate the state, which in turn causes <strong>overfitting to these tokens</strong>.</div> </blockquote> <h3 id="state-passing-fixes-effective-remembrance">State Passing fixes Effective Remembrance</h3> <p>After post-training with State Passing, the $\text{EffRem}$ curves show a gradual increase, indicating that the model places minimal weight on distant tokens and places progressively more weight on recent ones. In particular, tokens in the immediate context (e.g. the previous words in a sentence) have a critical impact on the next token predictions, which is the desired behavior in text modeling.</p> <div style="max-width: 500px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced-480.webp 480w,/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced-800.webp 800w,/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <blockquote class="block-tip"> <h4 id="takeaway-2">Takeaway</h4> <div style="text-align: justify; margin-bottom: 1em;"> Through Effective Remembrance, we can check that <strong>State Passing helps the models prioritize recent context</strong> and not be needlessly disrupted by tokens that are far away in the past. </div> </blockquote> <h2 id="conclusion">Conclusion</h2> <div style="text-align: justify; margin-bottom: 1em;"> We have shown that <strong>length generalization is expected to be achievable in recurrent models</strong> through simple training interventions, without the need of changing the architecture nor the internal mechanisms of the model. Moreover, these interventions <strong>improve their performance on long context reasoning tasks</strong>, suggesting that existing recurrent models are not realising their full potential and can be easily improved. </div> <div style="text-align: justify; margin-bottom: 1em;"> Secondly, we believe that this work has significant implications for architecture research. For example, it has become very popular for modern recurrent architecture works to compare out-of-length extrapolation abilities <d-cite key="rwkv-v6-peng2024eaglefinchrwkvmatrixvalued"></d-cite><d-cite key="gated_delta_net_yang2024gateddeltanetworksimproving"></d-cite><d-cite key="beck2024xlstm"></d-cite>. In our work we show that <strong>simple training interventions substantially improve length generalization across several recurrent architectures</strong>, and thus research can focus mostly on the in-length performance (or if directly studying length generalization, it would be important to account for these interventions). </div> <div style="text-align: justify; margin-bottom: 1em;"> Lastly, <strong>we have proposed Effective Remembrance as a tool to understand how any autoregressive sequence model processes its context</strong>, thus making it easy to quantify how much models are "effectively remembering" parts of the context. </div>]]></content><author><name>Ricardo Buitrago Ruiz</name></author><summary type="html"><![CDATA[[Paper]]]></summary></entry><entry><title type="html">Cross-Architecture Distillation Part I - The MOHAWK Framework</title><link href="https://goombalab.github.io/blog/2024/distillation-part1-mohawk/" rel="alternate" type="text/html" title="Cross-Architecture Distillation Part I - The MOHAWK Framework"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/distillation-part1-mohawk</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/distillation-part1-mohawk/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2408.10189">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba">Code</a>]</p> <ol> <li>Part I - MOHAWK</li> <li><a href="/blog/2024/distillation-part2-phi-mamba/">Part II - Phi-Mamba</a></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>We start off by summarizing some important aspects from <d-cite key="ssd"></d-cite>, specifically the sequence transformation/mixer viewpoint and the Mamba-2 SSM variant.</p> <p><strong>Definition:</strong> A <em>sequence transformation/mixer</em> refers to a parameterized map on sequences $Y = f_{\theta}(X)$ where $X, Y \in \mathbb{R}^{(T, P)}$ and $\theta$ is an arbitrary collection of parameters. $T$ represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_t, Y_t \in \mathbb{R}^P$.</p> <p>In layman‚Äôs terms, <em>sequence mixers</em> aggregate tokens across various time steps. This ability to learn temporal interactions and information forms the foundation of modern deep sequence models, like Transformers.</p> <p><strong>Definition:</strong> <em>Matrix mixers</em> are a specific type of sequence mixers that can be represented as $Y = MX$ for matrix $M \in \mathbb{R}^{(T,T)}$.</p> <p>Examples of <em>matrix mixers</em> which fall under this definition include vanilla self-attention, where $M = \text{Softmax}(\mathbf{Q}\mathbf{K}^\top)$ <d-cite key="vaswani2023attention"></d-cite>, linear attention <d-cite key="katharopoulos2020transformers"></d-cite>, and Toeplitz matrices <d-cite key="qin2023toeplitz"></d-cite>.</p> <h3 id="mamba-2">Mamba-2</h3> <p>Mamba-2 <d-cite key="ssd"></d-cite> is a recent variant of Structured State Space Models (SSMs) <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> which can be viewed as a matrix mixer that can be applied onto an input sequence in subquadratic time due to structured matrix multiplication. Mamba-2 is a time-varying SSM, defined as</p> \[\begin{aligned} h_{t+1} &amp;= A_t h_t + B_t x_t \\ y_t &amp;= C_t h_t \end{aligned}\] <p>where $B_t$ and $C_t$, like in Mamba-1 <d-cite key="gu2023mamba"></d-cite>, are input-dependent projections, but $A_t$ is the identity matrix $I$ multiplied by a scalar $\alpha_t$. Importantly, Mamba-2 identified the <em>Structured State Space Duality (SSD)</em> connection which found that specific variants of SSMs can be viewed as a form of causal linear attention <d-cite key="katharopoulos2020transformers"></d-cite>.</p> <p>Formally, the Mamba-2 SSD matrix mixer can be represented as</p> \[\begin{equation} \label{eq:ssd-matrix-mixer} \begin{aligned} \begin{bmatrix} \alpha_{1} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{2:1} &amp; \alpha_{2} &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{3:1} &amp; \alpha_{3:2} &amp; \alpha_{3} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \alpha_{n:1} &amp; \alpha_{n:2} &amp; \alpha_{n:3} &amp; \cdots &amp; \alpha_{n} \end{bmatrix} \circ (C \cdot B^\top) \cdot X \end{aligned} \end{equation}\] <p>where $\alpha_{t:i} = \alpha_{t-1} \cdot \alpha_{t-2} \cdots \alpha_{i}$.</p> <p>From this representation, one can see that Mamba-2 can be viewed as causal linear attention with a learnable causal mask!</p> <h2 id="mohawk-method">MOHAWK Method</h2> <p>Inspired by the <em>matrix mixer</em> viewpoint which provides a common lense for viewing the key components of various architectures, we introduce the <strong>MOHAWK</strong> framework for cross-architectural distillation, which is composed of three stages:</p> <ol> <li><strong>M</strong>atrix <strong>O</strong>rientation</li> <li><strong>H</strong>idden-State <strong>A</strong>lignment</li> <li><strong>W</strong>eight-Transfer and <strong>K</strong>nowledge Distillation</li> </ol> <p>These three sequential stages distill the student model from the bottom up, steadily increasing the number of components distilled into at each stage until the end student model has been distilled end-to-end. We find that this multi-stage process is much more effective than traditional knowledge distillation.</p> <p>Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba ‚Äìbased on the Phi-1.5 model <d-cite key="gunasekar2023textbooks"></d-cite>‚Äì along with the specifics of its distillation process.</p> <p>For clarity, we refer to the term <em>block</em> as a repeating component that forms the backbone of the end-to-end model. <em>Blocks</em> are composed of layers, for instance the Llama block is composed of a self-attention layer followed by a MLP layer. <em>Layers</em> can be composed of numerous subcomponents, like the self-attention layer, which encompasses the projections and the self-attention mechanism, and the Mamba layer, which includes the projections, convolution, and SSM mixer, etc.</p> <h3 id="stage-1-matrix-orientation">Stage 1: Matrix Orientation</h3> <p>We begin the first stage of MOHAWK by matching the matrix mixer of both the student and teacher. Prior to directly aligning the matrix mixers themselves, we first adjust the <em>matrix mixer layer</em> to be analogous to that of the teacher‚Äôs, i.e., structurally both layers are the same except the matrix mixer component. We then minimize the distance between the matrix mixer of the teacher and student layers, which can be expressed as the following equation:</p> <p>\(\begin{equation} \label{eq:matrix-orientation-minimization} \min_{\mathbf{\phi}} \|\mathrm{TeacherMixer}(\mathbf{u}) - \mathrm{StudentMixer}_{\boldsymbol{\phi}}(\mathbf{u})\|_F \end{equation}\) where $\phi$ represents the parameters in the layer and $\mathbf{u}$ is the shared input derived from the teacher model. The stage ensures that the student can closely approximate the teacher‚Äôs matrix mixer layer which sets a strong foundation for teacher matching in subsequent stages of the MOHAWK process.</p> <p>For Phi-Mamba: Because the student model uses the Mamba-2 mixer, we initialize the convolution to identity and discarded the nonlinear activation after the convolution to ensure the components upstream of the matrix mixers roughly equivalent to the self-attention layer. The loss calculate was between the self-attention matrix of the teacher and the ‚Äúunraveled‚Äù SSM matrix as shown in Equation \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="stage-2-hidden-state-alignment">Stage 2: Hidden-State Alignment</h3> <p>After optimizing Equation \eqref{eq:matrix-orientation-minimization} in Stage 1, Stage 2 proceeds to match the outputs of the student and teacher blocks.</p> <p>\(\begin{equation} \label{eq:hidden-state-minimization} \min_{\mathbf{\phi}} \|\mathrm{AttnBlock}(\mathbf{u}) - \mathrm{StudentMixerBlock}_{\boldsymbol{\phi}}(\mathbf{u})\|_2 \end{equation}\) where once again the inputs are the same. Like Stage 1, Stage 2 can be run in parallel. We find that the distance between the layer outputs is strongly correlated with the student model‚Äôs ability to recover the teacher model‚Äôs knowledge.</p> <p>For Phi-Mamba: To keep the block architectures as similar as possible, we initialized the Mamba-2 gate to be a value of 1 to simulate Phi‚Äôs lack of gating and removed the norm prior to the output projection.</p> <h3 id="stage-3-weight-transfer-and-knowledge-distillation">Stage 3: Weight-Transfer and Knowledge Distillation</h3> <p>The final stage aims to fine-tune the entire student model to match the performance of the teacher. This stage is critical for mending the potential discrepancies between post-Stage 2 blocks. We also initialize information dense components of the student model, in particular the MLPs, embedding, and LM head, before fine-tuning the student end-to-end. Given the weight transfer of critical architectural components, the overall block structure of the student mirror that of the teacher model, e.g., our student model has the MLPs and matrix mixer layers in parallel. Finally, we use knowledge distillation loss <d-cite key="hinton2015distilling"></d-cite> to encourage the student to imitate the teacher‚Äôs distribution:</p> \[\begin{equation} \min_{\mathbf{\phi}} \mathbf{\mathcal{L}}_{\mathrm{CE}}\big(\mathrm{Teacher}(\mathbf{x}), \mathrm{Student}_{\boldsymbol{\phi}} (\mathbf{x})\big) \end{equation}\] <p>For Phi-Mamba: We create a new Phi-Mamba block that has the same parallel MLP-matrix mixer layer structure as the original Phi-1.5 block. We copy over the MLP and norm weights, token embeddings, and language model head and pre-head norm as it has been hypothesized that much of a model‚Äôs information is stored in these components. We also find that the MLPs can be frozen after the transfer with only a slight decrease in performance but reduce the number of trainable parameters by more than half!</p> <h2 id="approximating-self-attention">Approximating Self-Attention</h2> <p>With the MOHAWK method we can now distill from any quadratic self-attention model to any model that utilizes a <em>matrix mixer</em> for sequential modeling. But, a caveat is that the performance of the student model is inherently constrained by the expressivity of its matrix mixer. So why did we decide to use the Mamba-2 mixer instead of an alternative like linear attention or gated convolution? In this next section, we will empirically explore Mamba-2‚Äôs ability to approximate the self-attention matrix $\text{Softmax}(QK^\top)$ and compare it to some other popular sub-quadratic matrix mixer families. We describe a couple of them below.</p> <h3 id="linear-attention-and-ssd">Linear Attention and SSD</h3> <p>When describing linear attention matrices, we can utilize the fact that both $Q$ and $K$ are token-dependent projections of some input $x \in \mathbb{R}^{d_{in}}$ onto $\mathbb{R}^{d_{out}}$, and therefore the rank of $Q$ and $K$ are bounded by $\min{ { d_{in}, d_{out} } }$ For multi-head linear attention, $d_{out}$, which corresponds to the head dimension, is typically a small value (e.g., $64$ and $128$ for Phi-1.5 and Llama2-7b-Chat respectively). Thus, we approximate linear attention matrix mixers using causal low-rank matrices $\mathbf{L \circ Q K}^\top$, where $\mathbf{L}$ is a lower-triangular causal mask of 1s, and $\mathbf{Q}$, $\mathbf{K}$ are in $\mathbb{R}^{n \times d}$ with $d \ll n$.</p> <p>For the multi-head Mamba-2 matrix family, we utilize the state space dual (SSD) layer in a manner similar to the previous linear attention class, but imbuing the causal matrix $\mathbf{L}$ with an $n$-degree rolling multiplicative structure for $\mathrm{SSD}$. This can be seen as a more expressive mask that generalizes the lower-triangular, ones-only causal mask \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="general-semi-separable-and-toeplitz">General Semi-separable and Toeplitz</h3> <p>To approximate the general class of semi-separable matrices (abbreviated as ‚ÄúSSM‚Äù in the following table), we utilize <em>balanced truncation</em>. This method is used in the field of time-invariant Dynamical System model reduction <d-cite key="BTSurvery"></d-cite> and has been modified for use in time-varying systems <d-cite key="TVBTSurvery"></d-cite>. Similarly, for the family of Toeplitz matrices, which represent a convolution operation, we apply a causal mask, the same one used for causal low-rank matrices, on top a Toeplitz matrix.</p> <h3 id="empirical-approximation">Empirical Approximation</h3> <p>To empirically validate the expressiveness of the four aforementioned families, we sample 1,000 attention matrices, each consisting of 512 tokens, from the Llama2-7B-Chat <d-cite key="touvron2023llama"></d-cite> model on four different datasets. One attention head, and its respective attention matrix, from each layer was chosen at random. Both (causal) low-rank (LR) and SSD matrix families were approximated with 10,000 steps of gradient descent per sample. SSM and Toeplitz were both calculated without using gradient descent using balanced truncation and a simple heuristic respectively. We calculate the Frobenius distance between each ‚Äúground truth‚Äù self-attention matrix and the approximated matrix of each family.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-480.webp 480w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-800.webp 800w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx.png" width="100%" height="auto" title="Matrix Approximation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Self Attention matrix approximation by structured matrix mixers</figcaption> </figure> <p>Given the previous table‚Äôs experiment was conducted in a very controlled setting, we further explore the ability of the various families‚Äô abilities to approximate the self-attention matrix within a language model. We replace the self-attention matrix mixers of a Phi-1.5 model with either input-dependent Toeplitz, causal low-rank, or SSD (our Mamba-2 variant) matrix mixers, and ran the second and third stages of our MOHAWK procedure for 1B tokens each.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl.png" width="100%" height="auto" title="Matrix Structure Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Evaluations of various structured matrices on downstream tasks</figcaption> </figure> <p>We find that there is a constant correlation between the self attention approximation abilities (measured via projection distances) of a matrix family and the downstream performance metrics (accuracy) of the matrix mixer integrated into an end-to-end language model. This finding that more expressive matrix mixers lead to more effective models is echoed in <d-cite key="hwang2024hydrabidirectionalstatespace"></d-cite>.</p> <h2 id="next-up">Next Up</h2> <p>The <a href="/blog/2024/distillation-part2-phi-mamba/">following section</a> will cover MOHAWK in action, distilling our final Phi-Mamba and Hybrid-Phi-Mamba models, and explore the training laws regarding each stage of MOHAWK.</p>]]></content><author><name>Aviv Bick*</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws</title><link href="https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba/" rel="alternate" type="text/html" title="Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2408.10189">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba">Code</a>]</p> <ol> <li><a href="/blog/2024/distillation-part1-mohawk/">Part I - MOHAWK</a></li> <li>Part II - Phi-Mamba</li> </ol> <p>In <a href="/blog/2024/distillation-part1-mohawk/">Part I</a> of this series, we covered important terminology, the Mamba-2 architecture, and the MOHAWK architecture. We also demonstrated Mamba-2‚Äôs ability to match the self-attention matrix of Transformers, which influenced our choice to use it as the student model for validating our MOHAWK method.</p> <p>In this section, we will explore the training laws regarding each of the three stages of MOHAWK and empirically validate the importance of all stages. We use the cumulative insights gained to then distill a <strong>fully subquadratic Mamba model using only 3B tokens</strong> - less than 1% of many of the other models‚Äô token budget - while being <strong>competitive with many of the current state-of-the-art open-source subquadratic models</strong>! We also distill a strong Mamba-Attention hybrid.</p> <h2 id="final-results">Final Results</h2> <p>We empirically validate the MOHAWK framework by distilling the pretrained Phi-1.5 model into a 1.5B Mamba variant, dubbed Phi-Mamba. Our final model was distilled with <strong>only 3B tokens</strong>, with a 80M/160M/2.76B token split among Stage 1/2/3, from the C4 dataset with a context length of 2048. The choices for these token splits were influenced by our verification of the importance of all three stages and training laws that determined, given a fixed token budget, how to allocate resources, which we detail in the following sections.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance.png" width="100%" height="auto" title="Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance of Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <h2 id="importance-of-each-mohawk-stage">Importance of Each MOHAWK Stage</h2> <p>A brief recap of the three stages of MOHAWK are</p> <p>1) <strong>Matrix Orientation</strong>: matches the matrix mixer of each respective block.</p> <p>2) <strong>Hidden-State Alignment</strong>: independently compares the block output given the same input across all layers of the student model.</p> <p>3) <strong>Weight-Transfer and Knowledge Distillation</strong>: performs knowledge distillation of logits from teacher to student and copies over crucial weights from the teacher model.</p> <p><strong>Each stage plays a crucial role</strong> as shown in our ablations below. All the runs were performed with a fixed total token count.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl.png" width="100%" height="auto" title="MOHAWK Ablations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Effects of various MOHAWK stage ablations on downstream performance</figcaption> </figure> <p>As expected, Stage 3‚Äôs end-to-end alignment is important as the <strong>previous stages only match the block outputs</strong>, leaving the blocks disjoint if the hidden state cannot be completely matched, as shown with both the Phi-Mamba and Hybrid-Phi-Mamba trained on Stage 3 outperform their counterparts trained with Stage 2. Of course, student models that have more mixing layers similar to the teacher may see a diminished impact of Stage 3 as the layers may be aligned more with only Stage 2.</p> <p>The addition of a Stage 2 initialization provides additional synergy, <strong>boosting performance significantly compared to Stage 3 only</strong>. We also note that the effects of adding Stage 2 is more pronounced in cases where the student architecture is less similar to the teacher architecture, e.g., the improvement for Phi-Mamba which has zero attention layers is larger than Hybrid-Phi-Mamba which has four.</p> <p>Stage 1 also provides a good in downstream performance. For example, only with the addition of Stage 1 on top of Stage 2 and 3 can a Phi-to-Phi distillation <strong>recover the original teacher Phi‚Äôs overall performance</strong>. And, we see in the two other architectures that performance gains can also be observed.</p> <h2 id="training-laws-for-mohawk">Training Laws for MOHAWK</h2> <p>We aimed to evaluate the impact the preceding stage had on the current stage‚Äôs performance.</p> <p>For the Stage 2 + 3 pair, we trained Phi-Mamba instances from scratch using Stage 2 to various checkpoints. These checkpoints were then used to initialize Phi-Mamba instances that were trained using Stage 3 to different total budgets. The figure below shows that given an adequate training budget, <strong>student models initialized from Stage 2 outperform students trained only with Stage 3</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage23-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage23.png" width="100%" height="auto" title="Stage 2 + 3 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Training Laws for Stage 2 and 3 of MOHAWK</figcaption> </figure> <p>Given the previous finding, we then analyze how matrix mixer matching (Stage 1) can impact the student‚Äôs ability to match the overall mixer block with the teacher (Stage 2). Similar to before, we train numerous Phi-Mamba models using Stage 1 and use them as initializations for Stage 2 and compare them against each other and also a Stage 2 only model. Here, we find that <strong>even a small budget allocated to Stage 1 can help the subsequent stage</strong> perform better than random initialization.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage12-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage12.png" width="100%" height="auto" title="Stage 1 + 2 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Training Laws for Stage 1 and 2 of MOHAWK</figcaption> </figure> <h3 id="training-the-final-phi-mamba-model">Training the Final Phi-Mamba Model</h3> <p>Using the insights gained in the training laws above, we finalized our training regime given a fixed budget of 3B tokens. Stage 1 was allocated 80M due to the strong performance on matrix distance and hidden state distance. Stage 2 was trained for 160M tokens given the seeming saturation of both hidden state distance and perplexity when compared to the other initialization states, e.g., 10M, 20M, 40M, etc. We train Stage 3 to reach 3B tokens in total, but reduced the learning rate of the last stage to alleviate training instabilities. We hypothesize that this is due to the Stage 1 + 2 initialization‚Äôs Mamba component being quite similar to that of the teacher model, so a large learning rate coupled with disconnect between blocks, which are addressed in Stage 3, can cause training instabilities.</p> <h2 id="hybrid-phi-mamba-model">Hybrid Phi-Mamba Model</h2> <p>There has been a growing body of work that combines both Attention and SSM mechanisms, leading to improved performance over either one used by itself <d-cite key="Samba"></d-cite><d-cite key="jamba2024"></d-cite><d-cite key="MambaVision"></d-cite>. Although incorporating Attention layers does make the model quadratic, limiting their number allows us to mitigate the efficiency drawbacks while increasing expressivity and performance!</p> <p>Thus, we distill the Phi-1.5 model into a Mamba-Attention hybrid model that maintains only four quadratic Attention layers. The remaining layers use the Mamba-2 layer variant also used in our Phi-Mamba model. Trained with 5B tokens using the MOHAWK method, our model achieves an average score of $66.0$ on downstream metrics, <strong>outperforming Phi-Mamba</strong>‚Äôs $65.1$ and <strong>approaching Phi-1.5</strong>‚Äôs $67.2$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance.png" width="100%" height="auto" title="Hybrid-Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance of Hybrid-Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <p>Our Hybrid-Phi-Mamba model is <strong>performs comparably</strong> to strong Attention-Mamba hybrids at the 1.5B range <strong>while using less Attention layers</strong> than Samba (12) and Mamba-SWA-MLP (18). We find that interleaving the Attention layers with the Mamba layers resulted in the strongest model, an observation that was also seen in <d-cite key="Samba"></d-cite>. We also find that increasing the number of Attention layers improved performance.</p>]]></content><author><name>Aviv Bick*</name></author><summary type="html"><![CDATA[[Paper] [Code]]]></summary></entry><entry><title type="html">Hydra Part I - Matrix Mixer Framework</title><link href="https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer/" rel="alternate" type="text/html" title="Hydra Part I - Matrix Mixer Framework"/><published>2024-07-16T00:00:00+00:00</published><updated>2024-07-16T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2407.09941">Paper</a>] [<a href="https://github.com/goombalab/hydra">Code</a>]</p> <ol> <li>Part I - Matrix Mixer Framework</li> <li><a href="/blog/2024/hydra-part2-model/">Part II - Hydra: The Model</a></li> </ol> <p>Attention mechanisms<d-footnote>In this work, Attention<d-cite key="attention"></d-cite> exclusively refers to Self-Attention<d-cite key="transformer"></d-cite></d-footnote> have taken center stage in the world of sequence mixing, celebrated for their significant flexibility and performance. However, this power comes with a price: high computational and memory demands. Despite these challenges, attention has become the go-to solution for many applications.</p> <p>In modern state-of-the-art models, architectural designs typically split into two main components: the sequence mixer and the channel mixer. To illustrate, let‚Äôs look at the Transformer encoder architecture. It consists of two key elements: Multi-Head Attention and a Feed-Forward Network (FFN). The Multi-Head Attention serves as the sequence mixer, efficiently managing interactions across the input sequence. Meanwhile, the FFN acts as the channel mixer, processing information within each sequence element.</p> <p>Take a glance at the figure below to see this architecture in action. You‚Äôll notice how these components work together to create the robust models we rely on today.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_mixer_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/matrix_mixer_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In our work, we study the large and important class of sequence mixers that can be represented as basic matrix multiplications: $\textbf{Y} = \textbf{M}\textbf{X}$. We call this approach <strong><em>the matrix mixer framework</em></strong>. This framework includes diverse and important classes of sequence models such as Attention, convolutions<d-cite key="ckconv"></d-cite><d-cite key="tnn"></d-cite>, and state-space models<d-cite key="s4"></d-cite><d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite>. For example, the typical self-attention mechanism, $\textbf{Y} = \text{softmax}(\textbf{Q}\textbf{K}^T)\textbf{V}$, can be seen as a special case where the matrix $\textbf{M}$ is defined as $\text{softmax}(\textbf{Q}\textbf{K}^T)$.</p> <p>Viewing sequence mixers through this lens has a significant advantage: designing new sequence mixers becomes a matter of finding the optimal matrix $\textbf{M}$. This perspective opens up a systematic way to explore and innovate in the field of sequence modeling.</p> <p>So, now the question is, what is a good $\textbf{M}$? Key desiderata for such a matrix would include:</p> <ul> <li>Efficiency: We want sub-quadratic matrix multiplication and parameterization to ensure our models run swiftly and handle long sequences with ease.</li> <li>Performance: The matrix mixer should match the high standards of Attention mechanisms in modeling diverse sequence data across various modalities.</li> <li>Flexibility: The solution should work well with sequences of different lengths (+ capable of both causal and bidirectional sequence modeling, which we will tackle in <a href="/blog/2024/hydra-part2-model/">Part II</a>)</li> </ul> <p>Check out the table below to see how various sequence mixers measure up. While several models like MLP-Mixer<d-cite key="mlpmixer"></d-cite>, FNet<d-cite key="fnet"></d-cite>, TNN<d-cite key="tnn"></d-cite>, LA<d-cite key="la"></d-cite>, and M2<d-cite key="m2"></d-cite> have been introduced, none of them fully meet all our criteria.</p> <table> <thead> <tr> <th>¬†</th> <th>Sub-quadratic</th> <th>Performance</th> <th>Flexibility</th> </tr> </thead> <tbody> <tr> <td>MLP-Mixer</td> <td>üò≠</td> <td>üò≠</td> <td>üò≠</td> </tr> <tr> <td>FNet</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>TNN</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>LA</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>M2</td> <td>ü§ó</td> <td>üò≠</td> <td>üò≠</td> </tr> <tr> <td>Transformer</td> <td>üò≠</td> <td>ü§ó</td> <td>ü§ó</td> </tr> </tbody> </table> <p>As you can see, each of these models has its strengths and weaknesses, but none perfectly hit all the marks. This gap highlights the need for another approach in developing sequence mixers.</p> <blockquote> <p><strong>So, is it even possible to meet all three key criteria?</strong></p> </blockquote> <p>We believe the answer lies in examining <strong><em>the structures</em></strong> of the mixer matrix $\textbf{M}$. Our work begins with an in-depth theoretical and empirical analysis of various sequence mixers using the matrix mixer framework. We then extend this idea, offering a systematic approach to designing new sequence mixers. By fully leveraging this framework, we have developed <strong>multiple</strong> novel architectures, including a new bidirectional mixer named <strong><em>Hydra</em></strong>.</p> <p>Let‚Äôs dive into more details, which is outlined as follows:</p> <ul> <li>We study and formalize the matrix mixer framework, introducing new theoretical concepts about structures of $\textbf{M}$ that can capture such desiderata.</li> <li>Guided by the properties of different matrix classes, we introduce a series of sequence models with strong and predictable performance.</li> <li>We provide careful systematic studies on these matrix classes, comparing empirical performances by varying only the matrix mixer</li> </ul> <h2 id="formalization-of-the-matrix-mixer-framework">Formalization of the Matrix Mixer Framework</h2> <p>We begin by further formalizing our matrix mixer framework. While this framework can be applied to multi-head architectures, we will focus on the single-headed scenario here for simplicity.</p> <p>In essence, a sequence mixer transforms an input $\textbf{X} \in \mathbb{R}^{L \times C}$ into an output $\textbf{Y} \in \mathbb{R}^{L \times C}$, where $L$ is the sequence length and $C$ is the number of channels.</p> <ol> <li>Input preprocessing function: Denoted as $f_X \colon \mathbb{R}^{L \times C} \rightarrow \mathbb{R}^{L \times D}$, this function handles common data transformations before the mixing process.</li> <li>Matrix construction function: Denoted as $f_{\mathcal{M}} \colon \mathbb{R}^{L \times C} \times \Theta \rightarrow \mathcal{M} \subseteq \mathbb{R}^{L \times L}$, this function maps input data to mixer matrices. Here, $\Theta$ represents the space of learnable parameters, and $\mathcal{M}$ represents the class of mixer matrices.</li> </ol> <p>Given these functions, we denote the mixer matrix as $\textbf{M} = f_{\mathcal{M}}(\textbf{X}, \theta)$. The matrix mixer framework is then defined by the equation: \(\textbf{Y} = \textbf{M} (f_X(\textbf{X})).\)</p> <p>Using this framework, we are now playing a game of finding the optimal $\textbf{M}$ that satisfies all three requirements: efficiency, performance, and flexibility! This systematic approach allows us to analyze the characteristics of different sequence mixers and formalize the properties needed to meet our criteria.</p> <p>Let‚Äôs break down these objectives step-by-step and explore which matrices work best in achieving them.</p> <h2 id="solution-for-sub-quadratic-complexity-structured-matrices">Solution for Sub-quadratic Complexity: Structured Matrices</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_classes_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_classes_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_classes_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/matrix_classes_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To meet our first key requirement ‚Äì sub-quadratic matrix multiplication ‚Äì we can focus on a special type of matrices known as <strong>structured matrices</strong>. For a general matrix $\textbf{M}$, matrix multiplication typically incurs a computational cost of $O(L^2)$. However, structured matrices, with their compressed representation, allow us to perform these operations much more efficiently, achieving sub-quadratic complexity. We refer to sequence mixers using these matrices as <strong><em>structured matrix mixers</em></strong>.</p> <p>Structured matrices provide a broad array of options for our matrix mixer $\mathcal{M}$, as illustrated in the figure above. By leveraging these matrices, we can significantly reduce computational overhead while maintaining an efficient parameter count.</p> <p>All previous versions of sub-quadratic sequence mixers fit within the matrix mixer framework. This categorization by the class of mixer matrices helps us systematically analyze and understand the strengths and weaknesses of different approaches.</p> <details><summary>Notations</summary> <p>Think of bold capital letters like $\textbf{X}$ as matrices, bold small letters like $\textbf{x}$ as vectors, and regular small letters like $x$ as scalars. When we talk about elements in a matrix, we‚Äôll use subscripts. So, if we have a matrix $\textbf{X} \in \mathbb{R}^{M \times N}$, the element in the $i$-th row and $j$-th column is $x_{ij}$. If we‚Äôre looking at the whole $i$-th row, it‚Äôs $\textbf{x}_i$.</p> </details> <table> <thead> <tr> <th>Matrix Structure $\mathcal{M}$</th> <th>Formulation (\(ùëö_{ij}\))</th> <th>Complexity</th> <th>Method Instantiations</th> </tr> </thead> <tbody> <tr> <td>Dense</td> <td>$m_{ij}$</td> <td>$O(L^2)$</td> <td>MLP-Mixer<d-cite key="mlpmixer"></d-cite></td> </tr> <tr> <td>Dense (Softmax Attention)</td> <td>$\text{softmax}_j(q^T_i k_j)$</td> <td>$O(L^2)$</td> <td>Transformer<d-cite key="transformer"></d-cite></td> </tr> <tr> <td>Low-rank (Linear Attention)</td> <td>$q^T_i k_j$</td> <td>$O(L)$</td> <td>Linear Attention<d-cite key="la"></d-cite>, Linformer<d-cite key="linformer"></d-cite></td> </tr> <tr> <td>Butterfly</td> <td>Refer to <d-cite key="kaleidoscope"></d-cite><d-cite key="monarch"></d-cite></td> <td>$O(L \log L)$</td> <td>Kaleidoscope<d-cite key="kaleidoscope"></d-cite>, Monarch<d-cite key="monarch"></d-cite></td> </tr> <tr> <td>Toeplitz (Convolution)</td> <td>$m_{j-i}$</td> <td>$O(L \log L)$</td> <td>S4<d-cite key="s4"></d-cite>, H3<d-cite key="h3"></d-cite>, TNN<d-cite key="tnn"></d-cite>, CKConv<d-cite key="ckconv"></d-cite></td> </tr> <tr> <td>Discrete Fourier Transform</td> <td>$w^{ij}$</td> <td>$O(L \log^2 L)$</td> <td>FNet<d-cite key="fnet"></d-cite></td> </tr> <tr> <td>Semiseparable</td> <td>\(\textbf{c}^T_i \textbf{A}^{\times}_{i:j} \textbf{b}_j \mathbb{1}_{\{i \geq j\}}\)</td> <td>$O(L)$</td> <td>Mamba (S6, SSD) <d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite></td> </tr> </tbody> </table> <p>As shown in the table above, using structured matrices (all but the dense variants) as the mixer matrix directly leads to sub-quadratic computational complexity.</p> <h2 id="solution-for-all-desiderata-sequence-aligned-matrices">Solution for All Desiderata: Sequence Aligned Matrices</h2> <p>So, can we simply choose any structured matrix as our sequence mixer matrix and expect it to meet all our requirements for efficiency, performance, and flexibility? Unfortunately, not all structured matrix mixers are up to the task. This begs the question: Is there a class of mixer matrices that can satisfy all three requirements? Fortunately, the answer is yes!</p> <p>We introduce a special subset of structured matrices called <strong><em>Sequence Aligned Matrices (SAM)</em></strong>. SAMs are designed to achieve efficiency, high performance, and flexibility all at once.</p> <h4 id="what-are-sequence-aligned-matrices-sam">What are Sequence Aligned Matrices (SAM)?</h4> <p>In simple terms, SAMs ensure that the parameters for every submatrix $\textbf{M}[: i+1, : i+1]$ are only functions of the tokens up to index $i$. Here is a formal definition of SAM.</p> <details><summary>Formal definition of Sequence Alignment</summary> <p><strong>Definition</strong> <em>(Sequence Aligned Matrices)</em> Let $L$ be the sequence length and let $\textbf{M} \in \mathbb{R}^{L \times L}$ denote a matrix with a parameter set $\mathcal{P}$. Then, we say that $\textbf{M}$ is a Sequence Aligned Matrix if there exists a partition $\Pi$ of $\hat{\mathcal{P}} \subseteq \mathcal{P}$, and $\hat{\mathcal{P}} \neq \phi$, such that for all sets $\mathcal{E} \in \Pi$, there exists a bijective map $f_{\mathcal{E}} : [L] \rightarrow \mathcal{E}$, and, for each $i \in [L]$, the sub-matrix $\textbf{M}[:i+1,:i+1]$ is composed solely from the parameters in the subset $\cup_{\mathcal{E}, k \le i} f_{\mathcal{E}}(k) \subseteq \mathcal{P}$.</p> </details> <h4 id="properties-of-sam">Properties of SAM</h4> <p>SAM matrices come with two crucial properties that make them stand out:</p> <ul> <li><strong>Data Dependency</strong>: SAM matrices are dynamically generated from the input data. This means they adapt in real-time based on the information they process.</li> <li><strong>Extendability</strong>: SAM matrices can handle inputs of arbitrary lengths, making them versatile for various applications.</li> </ul> <p>Take, for instance, the Attention mechanism in Transformers. It‚Äôs a perfect example of a SAM matrix: the Query-Key-Value components are all dynamically projected from the input data, and the mechanism itself adapts seamlessly to different sequence lengths.</p> <p>These two properties are not just nice-to-haves; they are essential for the flexibility and performance of modern models. Our experimental results strongly highlight the necessity of SAM, showing that SAM-based mixer matrices significantly enhance the performance of models.</p> <h3 id="sam-variations">SAM Variations</h3> <p>Let‚Äôs dive into a series of new SAM-based models we developed: <em>Toeplitz, Cauchy, Vandermonde, and quasiseparable</em> sequence mixers. By making these mixer matrices SAM, we achieved significant improvements. To make this explanation easier, we‚Äôll assume that Query-Key-Value are projected from an input sequence.</p> <h4 id="cauchy-code">Cauchy <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/cauchy.py">(Code)</a></h4> <p>We begin with our Cauchy variant, as it shares a significant similarity with the Attention mechanism: the norm of $m_{ij}$ represents the magnitude of correlations between the $i$-th and $j$-th tokens. Following the definition of Cauchy matrices, our SAM Cauchy mixer works as follows:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d} \frac{1}{(q_{id} - k_{jd} + c)} \space, \end{equation}\] <p>where $\textbf{Q}, \textbf{K} \in \mathbb{R}^{L \times D}$, and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected matrices from $\textbf{X}$, and $c$ is a trainable constant that stabilizes training by preventing divide-by-zero errors.</p> <h4 id="vandermonde-code">Vandermonde <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/vandermonde.py">(Code)</a></h4> <p>Recall the definition of Vandermonde matrices: $m_{rs} = (m_r)^s$. Due to the exponential values, this can lead to instability during training. Therefore, we use the formulation $q_{rs} = \mathfrak{R}(e^{i \cdot r \cdot q_s})$ and $k_{rs} = \mathfrak{R}(e^{i \cdot s \cdot k_r})$ for $\textbf{Q}$ and $\textbf{K}$. This technique, taking the real part of complex numbers, is commonly used in SSMs. Under the same setting as our SAM Cauchy mixer, our SAM Vandermonde mixer $\textbf{M}$ is parameterized as:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d}(\cos(2 \pi q_{id}^j) - \cos(2 \pi k_{jd}^i)) \space, \end{equation}\] <p>where the cosine function comes from <a href="https://en.wikipedia.org/wiki/Euler's_formula">Euler‚Äôs formula</a>.</p> <h4 id="toeplitz-code">Toeplitz <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/toeplitz.py">(Code)</a></h4> <p>A Toeplitz matrix mixer is inherently a convolution between weights $\textbf{w} \in \mathbb{R}^{2L-1}$ and an input sequence $\textbf{V} \in \mathbb{R}^{L \times C}$. Usually, a general convolution adopts input-independent $\textbf{w}$, which does not satisfy the definition of SAM. Therefore, we extend our Toeplitz matrix mixer to be SAM as follows:</p> \[\begin{equation} \textbf{Y} = \mathcal{F}^{-1}(\mathcal{F}_\textbf{w} \odot \mathcal{F}_\textbf{V}), \qquad \qquad \textbf{w}_{i} = \begin{cases} q_{i-L+1} &amp; \text{if } i \geq L \\ k_{L-i+1} &amp; \text{if } i \lt L \\ \end{cases} \space , \end{equation}\] <p>where the convolution is implemented using FFT $\mathcal{F}$, and $\textbf{q}, \textbf{k} \in \mathbb{R}^{L}$ and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected from $\textbf{X}$.</p> <h4 id="quasiseparable-code">Quasiseparable <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/quasiseparable.py">(Code)</a></h4> <blockquote class="block-tip"> <p><strong>This variant has a separate name, Hydra. Stay tuned for <a href="/blog/2024/hydra-part2-model/">Part II</a> ü§≠</strong></p> </blockquote> <h2 id="impact-of-sam-parameterization">Impact of SAM Parameterization</h2> <p>Now, we validate that the SAM matrix mixers are better than non-SAM mixers. To prove this claim, we conducted strictly controlled systematic albations where the only variable was the mixer matrix. Check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixer.py">our efforts</a> for a comprehensive and fair comparison!</p> <table> <tr> <td style="font-weight:bold;">Structure</td> <td style="text-align:center;">Data Dependent</td> <td style="text-align:center;"># Params</td> <td style="text-align:center;">GLUE Avg</td> <td style="text-align:center;">Œî</td> </tr> <tr> <td style="font-weight:bold;">Dense</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.7</td> <td style="text-align:center;"></td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Toeplitz</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">75.8</td> <td rowspan="2" style="text-align:center;">+1.9</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">77.7</td> </tr> <tr> <td style="font-weight:bold;">DFT</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.7</td> <td rowspan="3" style="text-align:center;">+5.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Vandermonde</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">70.8</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">76.0</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Cauchy</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.2</td> <td rowspan="2" style="text-align:center;">+4.0</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Low-rank</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.9</td> <td rowspan="2" style="text-align:center;">+3.5</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.4</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Attention</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.9</td> <td rowspan="2" style="text-align:center;">+6.9</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.8</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Quasiseparable</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">75.1</td> <td rowspan="2" style="text-align:center;">+4.6</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">79.7</td> </tr> </table> <p>The results in the table above clearly demonstrate the importance of SAM. Regardless of the matrix class, incorporating the SAM property always leads to a significant performance boost. Additionally, our SAM-based Toeplitz, Cauchy, and low-rank mixers perform remarkably well, with quasiseparable mixers even surpassing Attention. These findings underscore the immense potential of structured matrix mixers as efficient yet powerful sequence mixers.</p> <h2 id="next-up">Next Up</h2> <p>Curious about the quasiseparable matrix mixer? In <a href="">the next part</a>, we‚Äôll introduce Hydra, our bidirectional extension of SSMs that not only surpasses Attention but also achieves sub-quadratic complexity. Stay tuned!</p>]]></content><author><name>Sukjun Hwang*</name></author><summary type="html"><![CDATA[[Paper] [Code]]]></summary></entry><entry><title type="html">Hydra Part II - The Model</title><link href="https://goombalab.github.io/blog/2024/hydra-part2-model/" rel="alternate" type="text/html" title="Hydra Part II - The Model"/><published>2024-07-16T00:00:00+00:00</published><updated>2024-07-16T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/hydra-part2-model</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/hydra-part2-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/logo_trans-480.webp 480w,/assets/img/2024-07-16-hydra/logo_trans-800.webp 800w,/assets/img/2024-07-16-hydra/logo_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/logo_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2407.09941">Paper</a>] [<a href="https://github.com/goombalab/hydra">Code</a>]</p> <ol> <li><a href="/blog/2024/hydra-part1-matrix-mixer/">Part I - Matrix Mixer Framework</a></li> <li>Part II - Hydra: The Model</li> </ol> <p>In our previous post, we systematically compared various sequence models with different mixer matrices, and the quasiseparable SAM mixer emerged as the top performer. So, what exactly is it?</p> <h2 id="recap-ssms-are-semiseparable-matrix-mixers">Recap: SSMs Are Semiseparable Matrix Mixers</h2> <p>Before diving into the details of quasiseparable SAM mixers, let‚Äôs briefly revisit some key findings from <a href="https://arxiv.org/abs/2405.21060">Mamba-2</a><d-cite key="ssd"></d-cite>. Recently, Mamba-2 has shown that the mixer matrices of SSMs are inherently parametrized to one of the fundamental structured matrix classes ‚Äì semiseparable matrices.</p> <blockquote> <p><strong>Defintion</strong> of Semiseparable Matrices <br/> A lower triangular matrix $\textbf{M}$ is $N$-semiseparable iff any submatrix from the lower triangle (on or below the diagonal) has a rank of at most $N$. See (a) in the figure below.</p> </blockquote> <p>So why are SSMs semiseparable matrix mixers? Using our previously defined matrix mixer framework, we can represent SSMs as follows:</p> \[\begin{align} \textbf{y}_t &amp;= \sum^{t}_{s=0} \textbf{C}^T_t \left(\prod_{k=s+1}^{i} \textbf{A}_{k}\right) \textbf{B}_s \textbf{x}_s \\ \\ \textbf{Y} &amp;= \text{SSM}(\textbf{A}, \textbf{B}, \textbf{C})(\textbf{X}) = \textbf{M} \textbf{X} \space ,\\ \\ m_{ij} &amp; = \textbf{c}^T_i \textbf{A}_i \cdots \textbf{A}_{j+1} \textbf{b}_j \end{align}\] <p>where each matrix $\textbf{A}_i \in \mathbb{R}^{N \times N}$ and vector $\textbf{c}_i, \textbf{b}_i \in \mathbb{R}^{N \times 1}$. This decomposition shows that SSMs are indeed semiseparable mixers. [If you are not familiar with this concept, we recommend checking out this <a href="/blog/2024/mamba2-part2-theory/">blog post</a> for a great explanation.]</p> <p>Semiseparable matrices are an excellent choice for mixer matrices ‚Äì they are sub-quadratic, performant, and can be extended to handle sequences of various lengths. However, there‚Äôs one significant limitation: due to their definition, the upper right triangle of semiseparable matrices is filled with zeros, making them inevitably causal. This limitation makes SSMs incapable of <strong>bidirectional sequence processing</strong>.</p> <p>Why is bidirectionality important? Bidirectional processing is crucial for several reasons. One major reason is its importance in handling multiple modalities, such as processing 2D images. Without bidirectionality, models can‚Äôt fully leverage information from both past and future contexts within a sequence, which is essential for comprehensive data analysis across various applications.</p> <p>A straightforward way to make SSMs bidirectional is to use two separate SSMs: one for forward sequence processing and one for reverse sequence processing. There are several approaches to combine their outputs, such as adding, multiplying, or concatenating them <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite><d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite>. While these heuristics can work, they lack a principled design philosophy, leading to different heuristics being used for different tasks without a systematic approach.</p> <p>But what if we could use the matrix mixer framework to systematically derive the optimal $\textbf{M}$? Absolutely, we can! In addition to the three desiderata we discussed previously ‚Äì sub-quadratic complexity, extendability, and high-performance ‚Äì let‚Äôs add one more requirement: <strong>bidirectionality</strong>. For the mixer matrix to achieve bidirectionality, it must feature upper triangular components. So, how should we fill them?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/semiquasi_trans-480.webp 480w,/assets/img/2024-07-16-hydra/semiquasi_trans-800.webp 800w,/assets/img/2024-07-16-hydra/semiquasi_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/semiquasi_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="structured-matrix-of-our-choice-quasiseparable-matrices">Structured Matrix of Our Choice: Quasiseparable Matrices</h2> <p>For our bidirectional sequence mixer, we choose quasiseparable matrices. So, what makes quasiseparable matrices stand out? Let‚Äôs start by looking at their definition.</p> <blockquote> <p><strong>Defintion</strong> of Quasiseparable Matrices by the Rank Characterization. <br/> A matrix $\textbf{M}$ is $N$-quasiseparable iff any submatrix from either the strictly upper or lower triangle (off from the diagonal) has a rank of at most $N$. See (b) in the figure above.</p> </blockquote> <p>At first glance, this definition might seem similar to that of semiseparable matrices. To clarify, let‚Äôs highlight the key differences between quasiseparable and semiseparable matrices:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Semiseparable</strong></th> <th><strong>Quasiseparable</strong></th> </tr> </thead> <tbody> <tr> <td>(I)</td> <td>any submatrix from <em>the lower triangle</em></td> <td>any submatrix from either the strictly <em>upper or lower triangle</em></td> </tr> <tr> <td>(II)</td> <td><em>on or below</em> the diagonal</td> <td><em>off</em> from the diagonal</td> </tr> </tbody> </table> <h3 id="quasiseparable-matrices-supset-semiseparable-and-low-rank-matrices">Quasiseparable Matrices $\supset$ Semiseparable and Low-Rank Matrices</h3> <p>Although the differences between quasiseparable and semiseparable matrices might seem subtle, they lead to significant improvements in expressivity. According to difference <strong>(I)</strong>, semiseparable matrices zero out the upper triangular elements, while quasiseparable matrices extend to include these elements, enabling bidirectionality. Consequently, semiseparable matrices can only generalize mixers that use causal low-rank matrices, such as Linear Attention, whereas quasiseparable matrices generalize typical low-rank matrices. Moreover, both differences <strong>(I)</strong> and <strong>(II)</strong> mean that quasiseparable matrices not only generalize but also extend semiseparable matrices.</p> <ul> <li><strong><em>Quasiseparable matrices generalize low-rank matrices.</em></strong></li> <li><strong><em>Quasiseparable matrices generalize and extend semiseparable matrices.</em></strong></li> </ul> <h3 id="quasiseparable-matrices-supset-two-separate-ssms">Quasiseparable Matrices $\supset$ Two Separate SSMs</h3> <p>We now understand that for bidirectional processing scenarios, quasiseparable mixers are indeed better than semiseparable matrices. But what makes quasiseparable mixers superior to the bidirectional extensions using two separate SSMs?</p> <p>Heuristic variants that use the Hadamard product and concatenation <d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite> are difficult to analyze systematically within the matrix mixer framework. Moreover, concatenation variants double the number of output channels, necessitating additional parameters for reducing the number of channels.</p> <p>In contrast, addition-based variants <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite> can be formulated using the matrix mixer framework, as shown in (c) of the figure above, which resembles quasiseparable matrices in (d). However, difference <strong>(II)</strong> highlights that the diagonals of semiseparable matrices are also constrained by the rank characterization, and consequently, so are the diagonals of addition-based extensions. Quasiseparable matrices, on the other hand, do not have this constraint on the diagonals, allowing them to be complete free parameters. This flexibility makes quasiseparable matrices more mathematically expressive than addition-based bidirectional extensions.</p> <ul> <li><strong><em>Quasiseparable matrices are strictly more expressive than mixer matrices of addition-based bidirectional SSMs.</em></strong></li> </ul> <p>This property of complete freedom in the diagonals of quasiseparable matrices is more evident in another definition of quasiseparable matrices:</p> <blockquote> <p>A matrix $\textbf{M}$ is $N$-quasiseparable if each element $m_{ij}$ satisfies:</p> \[\begin{equation} m_{ij} = \begin{cases} \overrightarrow{\textbf{c}^{T}_{i}} \overrightarrow{\textbf{A}_i} \cdots \overrightarrow{\textbf{A}_{j+1}} \overrightarrow{\textbf{b}_{j}}, &amp; \text{if } i &gt; j \\ \delta_{i}, &amp; \text{if } i = j \\ \overleftarrow{\textbf{c}^{T}_{i}} \overleftarrow{\textbf{A}_{i}} \cdots \overleftarrow{\textbf{A}_{j-1}} \overleftarrow{\textbf{b}_{j}}, &amp; \text{if } i &lt; j\\ \end{cases},\\ \end{equation}\] <p>where each $\delta_i$ is a scalar, $\textbf{b}_i, \textbf{c}_i \in \mathbb{R}^{N \times 1}$, and $\textbf{A}_i \in \mathbb{R}^{N \times N}$.</p> </blockquote> <p>These are the actual results we obtained for the C4 and GLUE benchmark, along with the validation loss curve. Supported by these theoretical claims, our Hydra model, which uses a quasiseparable mixer matrix, indeed has shown superior performance to previous heuristic bidirectional extensions!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/bidirectionality_trans-480.webp 480w,/assets/img/2024-07-16-hydra/bidirectionality_trans-800.webp 800w,/assets/img/2024-07-16-hydra/bidirectionality_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/bidirectionality_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hydra-our-main-bidirectional-sequence-mixer">Hydra: Our Main Bidirectional Sequence Mixer</h2> <h3 id="implementation">Implementation</h3> <p>Now that we‚Äôve confirmed quasiseparable matrices as the go-to mixer matrices, we fully leverage them to propose the two-headed Mamba ‚Äì <strong><em>Hydra</em></strong>. Take a look at part (d) in the figure above, which illustrates the mixer matrix of Hydra, and also notice it‚Äôs also our previosly defined SAM! Utilizing an SSM, which is a semiseparable mixer, we can implement Hydra with the following formula: \(QS(\textbf{X}) = \texttt{shift}(SS(\textbf{X})) + \texttt{flip}(\texttt{shift}(SS(\texttt{flip}(\textbf{X})))) + \textbf{DX},\) where $\textbf{X}$ is the input sequence, $\texttt{flip}(\cdot)$ denotes a function that reverses the input, $\texttt{shift}(\cdot)$ denotes a right-shift function, and $\textbf{D} = \text{diag}(\delta_1, \cdots, \delta_L)$ represents the diagonal elements of $QS$. Here, $QS(\cdot)$ and $SS(\cdot)$ are the mixer matrix of Hydra and an SSM, respectively.</p> <p>Among the various iterations of SSMs, we adopt the latest one ‚Äì SSD from Mamba-2. Since SSMs are sub-quadratic, this simple implementation maintains the sub-quadratic cost. Compared to heuristic extensions that use two separate SSMs for bidirectionality, Hydra shares the input processing function $f_X$ for forward and reverse sequence processing, which nearly halves the number of parameters.</p> <p>You can check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py">the actual code</a>. To sum up:</p> <ul> <li>Hydra‚Äôs matrix mixer is meticulously parameterized to be a quasiseparable matrix with enhanced expressivity through shift operations.</li> <li>Hydra is sub-quadratic and super easy to implement using existing SSM implementations like Mamba.</li> <li>Hydra greatly reduces parameter counts compared to bidirectional extensions using two SSMs.</li> </ul> <h3 id="performance">Performance</h3> <p>We have seen that Hydra outperforms heuristic bidirectional extensions of SSMs, but how does it compare to state-of-the-art methods? Surprisingly, Hydra surpasses all previous models, including Transformer-based models such as BERT and ViT. When matched for the number of parameters, Hydra consistently shows the best performance across both NLP and Vision domains, highlighting its versatility.</p> <table> <tr> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #4dabf7">NLP</td> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #69db7c">Vision</td> </tr> <tr> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">GLUE Avg</td> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">Top-1 (%)</td> </tr> <tr> <td style="font-weight:bold;">BERT<d-cite key="bert"></d-cite></td> <td>110M</td> <td>83.5</td> <td style="font-weight:bold;">ViT-B<d-cite key="vit"></d-cite></td> <td>87M</td> <td>78.8</td> </tr> <tr> <td style="font-weight:bold;">MLP-Mixer<d-cite key="mlpmixer"></d-cite></td> <td>112M</td> <td>77.5</td> <td style="font-weight:bold;">S4-ViT-B<d-cite key="s4"></d-cite><d-cite key="s4d"></d-cite></td> <td>89M</td> <td>79.4</td> </tr> <tr> <td style="font-weight:bold;">FNet<d-cite key="fnet"></d-cite></td> <td>112M</td> <td>75.8</td> <td style="font-weight:bold;">Hyena-ViT-B<d-cite key="hyena"></d-cite></td> <td>88M</td> <td>78.4</td> </tr> <tr> <td style="font-weight:bold;">M2<d-cite key="m2"></d-cite></td> <td>116M</td> <td>80.9</td> <td style="font-weight:bold;">Mamba-ViT-B<d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite></td> <td>89M</td> <td>79.1</td> </tr> <tr> <td style="background-color: #f783ac; font-weight:bold;">Hydra</td> <td>112M</td> <td>84.3</td> <td style="background-color: #f783ac; font-weight:bold;">Hydra-ViT-B</td> <td>91M</td> <td>81.0</td> </tr> </table> <p>On the GLUE benchmark, Hydra outperforms BERT by 0.8 points. On ImageNet-1K, Hydra improves by 2.2 points over ViT. These results underscore Hydra‚Äôs capability to set new standards in both natural language processing and image classification tasks!</p> <h2 id="epilogue">Epilogue</h2> <p>Lately, the demand for large-scale computation has never been higher. Since the emergence of Mamba, interests in structured matrices has surged, and now is their time to shine. Structured matrices offer an exciting approach to efficient and powerful input processing, similar to how M2 improved over MLP-Mixer.</p> <p>In recent years, we‚Äôve seen numerous groundbreaking works showcasing promising results using structured matrices like Mamba. If the community strives together, just as we have spent about seven years investigating and improving Transformers, we believe there is enormous potential for further advancements through systematic exploration of different structured matrices, along with better optimized training settings (which have been fine-tuned for Transformers).</p> <p>A big shout-out to the recent <a href="https://arxiv.org/abs/2406.06248">BTT</a><d-cite key="btt"></d-cite> work, which systematically explores structured matrices for effective channel mixers. We were very excited to see this kind of systematic investigation, which is crucial for the continued advancement of better architectures.</p>]]></content><author><name>Sukjun Hwang*</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part I - The Model</title><link href="https://goombalab.github.io/blog/2024/mamba2-part1-model/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part I - The Model"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part1-model</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part1-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2405.21060">Paper</a>] [<a href="https://github.com/state-spaces/mamba">Code</a>]</p> <p><strong>This series is cross-posted at <a href="https://tridao.me/blog/2024/mamba2-part1-model/">tridao.me</a></strong></p> <ol> <li>Part I - The Model</li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>Since the release of <a href="https://arxiv.org/abs/2312.00752">Mamba</a> 6 months ago, we‚Äôve been pleasantly surprised by the overwhelming <a href="https://github.com/AvivBick/awesome-ssm-ml">community response</a>. It‚Äôs been incredibly gratifying to see the line of research on efficient sequence models we‚Äôve been pursuing for years really resonate with the machine learning community and take off more than we could have anticipated. We‚Äôve seen an enormous amount of exciting follow-up work, from direct applications (e.g. vision <d-cite key="zhu2024vision"></d-cite><d-cite key="ma2024u"></d-cite><d-cite key="liu2024vmamba"></d-cite>, genomics <d-cite key="schiff2024caduceus"></d-cite>, graphs <d-cite key="wang2024graph"></d-cite><d-cite key="behrouz2024graph"></d-cite>, and more) to understanding (e.g. on recall abilities <d-cite key="jelassi2024repeat"></d-cite>, in-context learning<d-cite key="akyurek2024context"></d-cite> <d-cite key="grazzi2024mamba"></d-cite> <d-cite key="park2024can"></d-cite>, and formal language expressivity <d-cite key="merrill2024illusion"></d-cite><d-cite key="sarrof2024expressive"></d-cite>), and an enormous number of <a href="https://jackcook.com/2024/02/23/mamba.html">online</a> <a href="https://srush.github.io/annotated-mamba/hard.html">blogs</a>, <a href="https://www.youtube.com/watch?v=dVH1dRoMPBc">tutorials</a>, <a href="https://www.youtube.com/watch?v=8Q_tqwpTpVU">and</a> <a href="https://www.youtube.com/watch?v=N6Piou4oYx8">videos</a>. We couldn‚Äôt be more excited about the direction of this research!</p> <p>Yet despite its potential so far, we weren‚Äôt completely satisfied with the first version of Mamba‚Ä¶</p> <h3 id="problem-1-understanding">Problem 1 (Understanding)</h3> <p>From a conceptual standpoint, one of the reasons we found SSMs so fascinating is how they just feel <em>fundamental</em>. One way this is exemplified is how they have rich ties to many major paradigms of sequence models. As developed in our earlier works on structured SSMs <d-cite key="gu2021combining"></d-cite><d-cite key="gu2023thesis"></d-cite>, they seem to capture the essence of continuous, convolutional, and recurrent sequence models ‚Äì all wrapped up in a simple and elegant model.</p> <p>But of course, aside from these, there‚Äôs another major sequence model paradigm: variants of the ubiquitous <strong>attention</strong> mechanism<d-cite key="bahdanau2015neural"></d-cite><d-cite key="vaswani2017attention"></d-cite>. SSMs always felt somewhat disjoint from attention, and we‚Äôve tried for a while to understand their relationship better.</p> <blockquote> <p>Question 1: <strong>What are the conceptual connections between state space models and attention?</strong> Can we combine them?</p> </blockquote> <h3 id="problem-2-efficiency">Problem 2 (Efficiency)</h3> <p>From a computational standpoint, despite the work that went into making Mamba fast (in particular, its hardware-aware selective scan implementation) it‚Äôs still much less hardware-efficient than mechanisms such as attention. The missing piece is that modern accelerators such as GPUs and TPUs are <em>highly</em> specialized for matrix multiplications. While this isn‚Äôt a problem for inference, which is bottlenecked by somewhat different considerations, this can be a big deal during training time.</p> <blockquote> <p>Question 2: <strong>Can we speed up the training of Mamba models by recasting them as matrix multiplications?</strong></p> </blockquote> <p>These are the main questions that Mamba-2 ‚Äì in particular, its new state space model variant ‚Äì tries to address.</p> <h2 id="the-ssd-model">The SSD Model</h2> <p>The main point of the Mamba-2 paper is what we call <strong>structured state space duality</strong> (SSD), which refers to several things:</p> <ol> <li>The <strong>SSD model</strong> refers to a specific standalone layer, like attention or an SSM, that can be incorporated into deep neural networks</li> <li>The <strong>SSD framework</strong> is a general framework for reasoning about this model (and many more theoretical connections)</li> <li>The <strong>SSD algorithm</strong> is an algorithm for computing SSD layers much more efficiently than previous SSMs</li> </ol> <p>The main SSD model or ‚Äústate space dual model‚Äù itself really isn‚Äôt so complicated! In this first part of a series of blog posts, we‚Äôll provide a self-contained description of the SSD layer (and Mamba-2) in isolation and how it compares to related models, particularly Mamba-1.</p> <p>In the next parts of this series, we‚Äôll describe the general framework and theoretical connections, which aren‚Äôt necessary to actually use Mamba-2.</p> <h3 id="the-linear-ssm-mode">The Linear (SSM) Mode</h3> <p>SSD starts from the same set of equations as Mamba:</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} h_t \end{aligned}\] <p>\begin{equation} \label{eq:ssm} (\text{Selective state space model (SSM)}) \end{equation}</p> <p>To recap, a <strong>structured state space model (SSM)</strong> <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> defines a map from $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$. Think of $x_t$ and $y_t$ as being scalars, and the hidden state $h_t$ as an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the <em>state size, state dimension, or state expansion factor</em>.</p> <p>A <em>selective</em> state space model allows the $(A, B, C)$ SSM parameters to vary across time <d-cite key="gu2023mamba"></d-cite>. We‚Äôll think of them as tensors with shapes $A \in \mathbb{R}^\mathtt{(T, N, N)}$, $B \in \mathbb{R}^\mathtt{(T, N)}$, and $C \in \mathbb{R}^\mathtt{(T, N)}$ respectively.<d-footnote>As with Mamba-1, we take everything over the reals $\mathbb{R}$, although complex variants as with other structured SSMs like the S4 lineage <d-cite key="gu2022efficiently"></d-cite> are also possible.</d-footnote></p> <p>Structured SSMs require $A$ to have structure to be efficiently computable, such as the most commonly used diagonal structure <d-cite key="gu2022parameterization"></d-cite><d-cite key="gupta2022diagonal"></d-cite><d-cite key="smith2023s5"></d-cite><d-cite key="gupta2022simplifying"></d-cite>. In this case $A$ has shape $\mathtt{(T, N)}$ where only the diagonal elements of the $\mathtt{N} \times \mathtt{N}$ matrices are stored.</p> <h4 id="ssd-scalar-structured-ssm">SSD: Scalar Structured SSM</h4> <p>The original Mamba (or more precisely its core ‚ÄúS6‚Äù layer) is exactly a selective SSM with diagonal structure.</p> <p><strong>The SSD layer of Mamba-2 makes only one small modification</strong>: it restricts the diagonal $A$ even further to a <em>scalar times identity</em> structure; in other words the diagonal elements of $A$ must all be the same value. In this case $A$ can be represented with shape just $\mathtt{(T)}$ and one can also identify $A_t$ as just a scalar (and so we‚Äôll sometimes denote it $a_t$).</p> <h4 id="multihead-ssms">Multihead SSMs</h4> <p>Equation \eqref{eq:ssm} is defined only for a single dimensional input $x \in \mathbb{R}^\mathtt{T}$. If $X \in \mathbb{R}^\mathtt{(T, P)}$ has $\mathtt{P}$ separate channels, we can use the same dynamics (i.e. the same SSM $(A, B, C)$) independently for each channel. This can be interpreted as a <em>single head</em> of the SSM model.</p> <p>Here, we think of $X$ as a tensor of shape $\mathtt{(T, P)}$ where $\mathtt{T}$ is the sequence (time) dimension and $\mathtt{P}$ is the ‚Äúhead dimension‚Äù.<d-footnote>Normally there's an additional batch dimension $\mathtt{B}$ when implementing these models, which we'll ignore throughout this presentation.</d-footnote></p> <p>Multiple heads can be constructed completely independently; for the remainder of this post, we assume that we‚Äôre working with a single head. Note that these heads are exactly analogous to how heads in multi-head attention models work, and in Mamba-2 we also choose similar dimensions as modern Transformers, e.g. $\mathtt{P} = 64$ or $\mathtt{P}=128$. (To scale to larger model widths $\mathtt{D} = \mathtt{d\_model}$, we keep this fixed and increase the number of independent heads.)</p> <p>We can notate the general (selective) state space model as \begin{equation} \label{eq:ssm-transformation} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T,‚Ä¶)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>Some axes of variation include</p> <ol> <li>The structure on $A$, which affects its parameter shape: <ul> <li><code class="language-plaintext highlighter-rouge">... = (N,N)</code> for general (unstructured) SSMs</li> <li><code class="language-plaintext highlighter-rouge">... = (N)</code> for diagonal SSMs (or other structures, such as diagonal-plus-low-rank <d-cite key="gu2022efficiently"></d-cite>)</li> <li><code class="language-plaintext highlighter-rouge">... = ()</code> for scalar SSMs (i.e. SSD)</li> </ul> </li> <li>The state dimension $\mathtt{N}$ (i.e. <code class="language-plaintext highlighter-rouge">d_state</code>)</li> <li>The head dimension $\mathtt{P}$ (i.e. <code class="language-plaintext highlighter-rouge">d_head</code>)</li> </ol> <p>There are other axes of variation of structured SSMs (e.g. time-invariance vs. selectivity, SISO vs. MIMO<d-cite key="smith2023s5"></d-cite>, real vs. complex, etc.), but we‚Äôre highlighting these so that we can contrast Mamba-2 to Mamba-1 in just a second‚Ä¶</p> <h3 id="the-quadratic-attention-mode">The Quadratic (Attention) Mode</h3> <p>But first, let‚Äôs switch tacks and forget about state space models for a moment. Given the same tensors above with the same shapes $(A^\mathtt{(T)}, B^\mathtt{(T, N)}, C^\mathtt{(T, N)})$, let‚Äôs define a different object.</p> <p>First, we‚Äôll define the following matrix (don‚Äôt worry, we‚Äôll explain more and give it a name in Part II of this series!)</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix} .\] <p>Then, let‚Äôs define the following matrix</p> <p>\begin{equation} \label{eq:ssd-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Finally, $M$ encodes a <em>sequence transformation</em> $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$ mapping a 1D input to a 1D output‚Äîjust as in equation \eqref{eq:ssm}‚Äîthrough basic matrix multiplication $y = Mx$.</p> <p>What‚Äôs special about this? Well, you may notice that it looks very similar to an attention computation. In fact, if all $a_t = 1$, then $L$ is simply the lower-triangular <em>causal mask</em> and \eqref{eq:ssd-attention} is equivalent to <strong>causal linear attention</strong> <d-cite key="katharopoulos2020transformers"></d-cite>:</p> \[Y = (L \circ Q K^\top) V\] <p>This is exactly the same as equation \eqref{eq:ssd-attention} if we rename $(C, B, X) \mapsto (Q, K, V)$!</p> <h2 id="state-space-duality">State Space Duality</h2> <p>The so-called ‚Äúduality‚Äù refers to the fact that the two models defined in equations \eqref{eq:ssm} (for the scalar-identity structured $A_t$ case) and \eqref{eq:ssd-attention} are actually <em>exactly the same model</em>, which we can view as a particular function</p> \[(A^\mathtt{(T)}, B^\mathtt{(T, N)}, C^\mathtt{(T, N)}, X^\mathtt{(T, P)}) \mapsto Y^\mathtt{(T, P)}\] <p>In the general <em>SSD Framework</em> (Part II of this series), we‚Äôll show this equivalence in two completely different ways, both of which are actually much more general and each quite illuminating.</p> <p>If you take our word for it, though, then SSD is relatively simple to contrast in relation to either SSMs or attention.</p> <h3 id="ssd-vs-state-space-models">SSD vs. State Space Models</h3> <p>Compared to previous SSMs, SSD is pretty much the same as the core layer of Mamba but with even more structure on the recurrent $A$ matrices.</p> <ol> <li>Mamba-1 (S6) uses diagonal structure on $A$, while Mamba-2 (SSD) uses scalar-times-identity structure on $A$.</li> <li>Mamba-1 has a head dimension of $\mathtt{P}=1$ (i.e. all channels are completely independently controlled by separate SSMs), while Mamba-2 uses a head dimension of $\mathtt{P}&gt;1$ (something like $\mathtt{P}=64$ by default).</li> </ol> <p>In particular, this can be viewed as weight-tied in two ways:</p> <ul> <li>By restricting the diagonal structure of $A$ to scalar-times-identity, the recurrence dynamics are shared across all $\mathtt{N}$ elements of the state space.</li> <li>These dynamics are also shared across all $\mathtt{P}$ channels of a given head.</li> </ul> <p>In other words, a single SSM head has total state size $\mathtt{P} \times \mathtt{N}$, which are each governed by separate scalar recurrences in Mamba-1 but are controlled by a single shared recurrence in Mamba-2.</p> <p>Why make these restrictions? The main motivation is efficiency: these changes are necessary to be able to view the model in its [<a href="#the-quadratic-attention-mode">dual attention form</a>], which allows matrix multiplications to be used.</p> <blockquote class="block-tip"> <h4 id="the-bottom-line-mamba-1-vs-mamba-2">The Bottom Line: Mamba-1 vs. Mamba-2</h4> <p>Compared to Mamba-1, Mamba-2 allows <strong>much larger state dimensions</strong> (from <code class="language-plaintext highlighter-rouge">N=16</code> in Mamba-1 to <code class="language-plaintext highlighter-rouge">N=64</code> to <code class="language-plaintext highlighter-rouge">N=256</code> or even higher in Mamba-2) while simultaneously being <strong>much faster during training</strong>.</p> </blockquote> <p>But can this hurt us? There‚Äôs some intuition to believe that it shouldn‚Äôt. One of the main reasons for the selectivity (e.g. $A$ that depends on the input $X$) introduced in Mamba is to let the SSM be able to control whether to remember or ignore particular pieces of information; for example, if a filler ‚Äúum‚Äù is encountered in a text transcript. But if such information should be ignored, then the entire state can ignore it together, and so it should be okay if the state‚Äôs dynamics are shared across all features.</p> <p>Empirically, we haven‚Äôt found evidence that the restricted expressivity of Mamba-2 might hurt, but the jury‚Äôs still out! From one perspective, Mamba-2 isn‚Äôt <em>strictly</em> better than Mamba-1: while it‚Äôs a dramatic improvement from a <em>training</em> perspective, Mamba-1 might be better from a pure <em>inference</em> perspective. Since inference speed of SSMs is entirely governed by the state dimension, if one wants to maximize performance for a target inference efficiency (i.e. for a particular state size $\mathtt{N}$), then the increased expressivity of Mamba-1 might be better. We haven‚Äôt fully analyzed the (theoretical or empirical) tradeoffs here, and think this would be a cool direction for the community to dig in more!</p> <h3 id="ssd-vs-attention">SSD vs. Attention</h3> <p>Compared, to standard (self-)attention, SSD also only has two differences:</p> <ol> <li>The softmax normalization is dropped.</li> <li>A separate elementwise mask matrix is applied multiplicatively.</li> </ol> <p>The first difference can be interpreted as what reduces the effective state size of the model from linear to constant, and improves its efficiency from quadratic to linear.</p> <p>The second difference is what distinguishes SSD from standard linear attention. One way to think of the mask is as <strong>input-dependent relative positional encodings</strong>. Because of the mask $L$ in \eqref{eq:ssd-attention}, the standard attention score $\langle Q_i, K_j \rangle$ is attenuated by a weight</p> \[a_{i:j}^\times = a_i \cdots a_{j+1}\] <p>which can be interpreted as a ‚Äúdiscount factor‚Äù based on how far apart the positions $i$ and $j$ are. (This interpretation was concurrently espoused by Tobias Katsch‚Äôs <a href="https://arxiv.org/abs/2311.01927">GateLoop</a> paper<d-cite key="katsch2023gateloop"></d-cite>.) In its attention form, this input-dependent positional mask can be interpreted as the key factor that encodes the ‚Äúselectivity‚Äù of Mamba!</p> <h2 id="best-of-both-worlds">Best of Both Worlds</h2> <p>So why do we care that there are two views of this model? Well, first of all, it‚Äôs extremely mathematically interesting, as we‚Äôll cover in <a href="/blog/2024/mamba2-part2-theory/">Part II</a>, and we hope will inspire future directions. But there are immediate practical benefits too!</p> <h3 id="the-ssm-and-attention-modes">The SSM and Attention Modes</h3> <p>The SSM \eqref{eq:ssm} and attention \eqref{eq:ssd-attention} modes represent two different ways of computing the same function, so let‚Äôs contrast them.</p> <p>First, remember that one main reason why SSMs are interesting to begin with is because computing \eqref{eq:ssm} as a recurrence requires maintaining a <em>constant-size state</em> (size $\mathtt{N}$ per channel) and scales <em>linearly in the sequence length</em> $\mathtt{T}$. The downside is that the raw FLOPs don‚Äôt reflect actual speed in practice because of hardware considerations‚Ä¶</p> <p>On the other hand, computing this sequence transformation $y = Mx$ through equation \eqref{eq:ssd-attention} takes quadratic time in the sequence length, because we‚Äôre materializing this $\mathtt{T} \times \mathtt{T}$ matrix. But it can be fast in practice because it only uses matrix multiplications, which are extremely optimized on GPUs and TPUs.</p> <h3 id="the-ssd-mode">The SSD Mode</h3> <p>So if there are two equivalent ways of computing the same model, when should we use one mode or the other? During inference, there‚Äôs no trade-off: the SSM mode is designed for fast autoregressive inference. But what about training? Here there‚Äôs a tension between FLOPs and hardware efficiency where the attention mode uses more FLOPs, but uses them more efficiently through matrix multiplications.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_algorithm-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_algorithm.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It turns out we can get the best of both worlds by combining the algorithms! There are two equivalent interpretations of this ‚Äústate space dual‚Äù algorithm, either as</p> <ol> <li>A block decomposition of a particular structured matrix that defines the SSD ‚Äútoken-mixing‚Äù sequence transformation.</li> <li>A ‚Äúchunkwise‚Äù algorithm that splits the sequence into segments, computes the quadratic attention form on each segment, and adjusts the result by passing the SSM states between segments.</li> </ol> <p>We‚Äôll leave the details of this algorithm to <a href="/blog/2024/mamba2-part3-algorithm/">Part III</a> (or Section 6 of the <a href="https://arxiv.org/abs/2405.21060">full paper</a>), as it requires a bit of machinery from the theory to derive. But we do emphasize that the implementation of this algorithm isn‚Äôt too complicated ‚Äì a minimal implementation that we provide is only ~30 lines of PyTorch!</p> <p>The benefits of the SSD algorithm is that it preserves the same efficient FLOP counts as SSMs (compared to quadratic attention), and also dramatically speeds up training compared to general state space models by utilizing matmuls.</p> <table> <thead> <tr> <th>¬†</th> <th>Attention</th> <th>SSM</th> <th>SSD</th> </tr> </thead> <tbody> <tr> <td>State size</td> <td>$\mathrm{T}$</td> <td>$\mathbf{N}$</td> <td>$\mathbf{N}$</td> </tr> <tr> <td>Training FLOPs</td> <td>$\mathrm{T}^2\mathrm{N}$</td> <td>$\mathbf{TN^2}$</td> <td>$\mathbf{TN^2}$</td> </tr> <tr> <td>Inference FLOPs</td> <td>$\mathrm{T}\mathrm{N}$</td> <td>$\mathbf{N^2}$</td> <td>$\mathbf{N^2}$</td> </tr> <tr> <td>(Naive) memory</td> <td>$\mathrm{T}^2$</td> <td>$\mathrm{TN}^2$</td> <td>$\mathbf{TN}$</td> </tr> <tr> <td>Matrix multiplications?</td> <td>:heavy_check_mark:</td> <td>:x:</td> <td>:heavy_check_mark:</td> </tr> </tbody> </table> <h2 id="the-mamba-2-architecture">The Mamba-2 Architecture</h2> <p>Although the core contribution of Mamba-2 is the new SSD layer and theory, we also make some small changes to Mamba‚Äôs neural network architecture.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/architecture_2-480.webp 480w,/assets/img/2024-05-31-mamba-2/architecture_2-800.webp 800w,/assets/img/2024-05-31-mamba-2/architecture_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/architecture_2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The main change is producing the $(A, B, C)$ SSM parameters in parallel with the $X$ input, instead of sequentially. This is partly motivated by the connections to attention; but more pragmatically, it‚Äôs simpler and more amenable to scaling techniques such as tensor parallelism, which will be discussed in Part IV of this series!</p> <p>There are some other small differences which are covered in more detail in the paper. However, we do want to emphasize that these architectural changes aren‚Äôt really the main point of the model.</p> <h3 id="language-modeling">Language Modeling</h3> <p>In terms of empirical results, we didn‚Äôt test Mamba-2 as extensively as Mamba-1, but believe it should generally be on par or better across the board. Our full language model results use the same protocol as Mamba, and found slightly better scaling at Chinchilla laws <d-cite key="hoffmann2022empirical"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-480.webp 480w,/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-800.webp 800w,/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/pile_8k_mamba2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fully trained models on the Pile dataset<d-cite key="pile"></d-cite> and the standard zero-shot downstream evaluations show similar trends. We emphasize that even when the performance is comparable, Mamba-2 is <em>much</em> faster to train than Mamba-1!</p> <h3 id="associative-recall">Associative Recall</h3> <p>More interestingly, we highlight the one synthetic task we tried. Since the original Mamba paper, which investigated synthetics such as Synthetic Copying and Induction Heads, many follow-up works have begun investigating harder associative recall tasks. The <strong>multi-query associative recall (MQAR)</strong> task introduced by the Zoology and Based <d-cite key="arora2024zoology"></d-cite><d-cite key="arora2024simple"></d-cite> line of work has become a de facto standard.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mqar-480.webp 480w,/assets/img/2024-05-31-mamba-2/mqar-800.webp 800w,/assets/img/2024-05-31-mamba-2/mqar-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mqar.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We ran a version of this task that‚Äôs much harder than the one usually reported in the literature, and found that Mamba-2 is substantially better than Mamba-1. One reason for the improved performance is the much larger state size (up to $16\times$ larger than Mamba-1 here), which was one of the primary motivations of Mamba-2 in the first place.</p> <p>Interestingly, Mamba-2 also appears to be noticeably better than Mamba-1 on this particular task even when the state size is controlled. We‚Äôre not quite sure why to be honest, and it would be great to ablate the other aspects of the model to investigate‚Ä¶ for example, could it be possible that the [<a href="#ssd-vs-state-space-models">restricted structure of SSD</a>] is actually <em>helpful</em> here?</p> <h2 id="next-up">Next Up</h2> <p>In <a href="/blog/2024/mamba2-part2-theory/">the next part of this series</a>, we‚Äôll go more into the full SSD framework, including how to prove the claimed ‚Äúduality‚Äù of the SSD layer, and strong generalizations of it.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part II - The Theory</title><link href="https://goombalab.github.io/blog/2024/mamba2-part2-theory/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part II - The Theory"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part2-theory</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part2-theory/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li>Part II - The Theory</li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>In <a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series, we defined the state space dual (SSD) <em>model</em>. In isolation, this model is relatively simple to define, and we claimed that it can be computed either as an SSM recurrence or with an attention-like pattern. If you just want to use the model, feel free to skip this post!</p> <p>In this post, we‚Äôll dive into the theory behind the model. We‚Äôll derive the SSD ‚Äúduality‚Äù in two completely separate ways, one starting from the SSM perspective and one from the attention perspective. Each method is actually much more broad than the SSD model itself, and the union of these two strong generalizations is what we call the SSD <em>framework</em>. This framework provides a rich body of connections between state space models, attention, and structured matrices. While the SSD model can be viewed as a specific instantiation of each prong of the framework, the SSD framework is much more general opens up many directions for future work.</p> <h4 id="the-state-space-duality-framework">The State Space Duality framework</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_venn-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_venn-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_venn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_venn.png" width="100%" height="auto" title="Structured State Space Duality" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SSD Framework (red, blue): State space models (i.e. semiseparable matrices) and structured masked attention encapsulate large classes of efficient sequence models. Their intersection is the SSD model (purple).</figcaption> </figure> <p>For each of the two parts of this framework, we‚Äôll</p> <ol> <li>Define the general concepts</li> <li>Show how the SSD model is an instantiation, and prove the duality</li> <li>Suggest future directions for how the framework can be used</li> </ol> <p>Note that this theory is <em>not necessary</em> to use the SSD model itself; this part of the series can be safely skipped for the practitioner that just wants to use SSD (Mamba-2).</p> <h2 id="recap-the-ssd-model">Recap: The SSD Model</h2> <p><a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series introduced the SSD layer, which is defined as a selective SSM</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} y_t \end{aligned}\] <p>\begin{equation} \label{eq:ssm} (\text{Selective state space model (SSM)}) \end{equation}</p> <p>with scalar-identity structure on $A$.</p> <p>More formally, we view it as a <em>sequence transformation</em> $X \mapsto Y$</p> <p>\begin{equation} \label{eq:ssm-transformation} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>The dual attention-like form of the SSD layer is</p> <p>\begin{equation} \label{eq:ssd-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Now let‚Äôs see how to prove this!</p> <h2 id="ssd-framework-1-structured-matrix-transformations">SSD Framework 1: Structured Matrix Transformations</h2> <p>The first framing of the duality will be from an SSM-centric perspective, where we‚Äôll prove the duality through the framework of <strong>matrix sequence transformations</strong> or ‚Äúmatrix mixers‚Äù.</p> <h3 id="matrix-transformations">Matrix Transformations</h3> <p>The idea is that many sequence models, i.e. <em>sequence transformations</em> $X \in \mathbb{R}^\mathtt{(T,P)} \mapsto Y \in \mathbb{R}^\mathtt{(T,P)}$, can be written in the form of a single matrix multiplication $Y = M(X) \cdot X$ where $M$ is a matrix which can itself depend on $X$. We call this a <em>matrix sequence transformation</em>, or matrix transformation for short. In the literature sequence transformations have also been referred to as ‚Äúsequence mixers‚Äù or ‚Äútoken mixers‚Äù, and matrix sequence transformations as ‚Äúmatrix mixers‚Äù. There are many examples of these, which are distinguished by the structure of the $M$ matrix. The de facto example is self-attention itself, where $M = \mathsf{softmax}(QK^\top)$ is the attention matrix. Other examples include MLP-Mixer<d-cite key="tolstikhin2021mlp"></d-cite>, FNet<d-cite key="lee2021fnet"></d-cite>, and Monarch Mixer<d-cite key="dao2022monarch"></d-cite><d-cite key="fu2024monarch"></d-cite>.</p> <p>Why do we care about these types of models?</p> <blockquote> <p>Writing a sequence model as a matrix transformation provides a powerful tool to understand the structure and characteristics of the model.</p> </blockquote> <p>And although general non-linear RNNs such as LSTMs <em>cannot</em> be written as matrix mixers, state space models can! In fact, this is pretty easy to see by just unrolling the definition of the SSM recurrence. The upshot is that the SSM \eqref{eq:ssm-transformation} can be written as a matrix transformation</p> \[Y = \mathsf{SSM}(A, B, C)(X) = MX\] <p>where $M_{ij} = 0$ for $i &lt; j$ (i.e. it‚Äôs lower triangular) and otherwise \begin{equation} \label{eq:semiseparable} M_{ij} = C_i^\top A_{i:j}^\times B_j := C_i^\top A_i \dots A_{j+1} B_j \end{equation}</p> <p>Drawing it out, this matrix looks like</p> \[\begin{bmatrix} C_0^\top B_0 &amp; \\ C_1^\top A_1 B_0 &amp; C_1^\top B_1 &amp; \\ C_2^\top A_2A_1 B_0 &amp; C_2^\top A_2 B_1 &amp; C_2^\top B_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_1 B_0 &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_2 B_1 &amp; \dots &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1} B_{\mathtt{T}-2} &amp; C_\mathtt{T}^\top B_{\mathtt{T}-1} \\ \end{bmatrix}\] <p>\begin{equation} \label{eq:ssm-matrix} (\text{Matrix Transformation Representation of State Space Models}) \end{equation}</p> <h3 id="semiseparable-matrices">Semiseparable Matrices</h3> <p>This type of matrix in fact has a name: it‚Äôs called a (triangular) <strong>semiseparable matrix</strong>, and has been studied in other fields of engineering and computational linear algebra<d-cite key="vandebril2005bibliography"></d-cite>. These matrices are (IMO) quite fundamental and beautiful, and the full paper talks about more of their properties. For example, an alternative characterization of semiseparable matrices is their <em>structured rank property</em>, which says that every submatrix contained in the lower-triangular portion is low rank.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/semiseparable-480.webp 480w,/assets/img/2024-05-31-mamba-2/semiseparable-800.webp 800w,/assets/img/2024-05-31-mamba-2/semiseparable-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/semiseparable.png" width="100%" height="auto" title="State Space Models are Semiseparable Matrices" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">All submatrices contained on-and-below the diagonal of a semiseparable matrix are low-rank.</figcaption> </figure> <p>For our purposes, we‚Äôll care about this form mainly for the algorithmic considerations. One of the central messages of this SSD paper is that:</p> <blockquote class="block-tip"> <h4 id="takeaway-computing-ssms-through-matrix-multiplication">Takeaway: Computing SSMs Through Matrix Multiplication</h4> <p>All algorithms for computing state space models can be viewed as structured matrix multiplication algorithms on semiseparable matrices.</p> </blockquote> <p>Let‚Äôs see an easy instantiation of this, focusing on our main objective!</p> <h3 id="deriving-the-duality-ssm-to-attention">Deriving the Duality: SSM to Attention</h3> <p>To show that equation \eqref{eq:ssd-attention} follows from equation \eqref{eq:ssm} (in the case of the SSD model, i.e. scalar SSM), we directly use the matrix form of the state space model \eqref{eq:semiseparable}. Because the $A_t$ are all scalars in this case, they can be factored out of the entries</p> \[C_i^\top A_{i:j}^\times B_j = A_{i:j}^\times \cdot (C_i^\top B_j)\] <p>which directly implies equation \eqref{eq:ssd-attention}.</p> <p>In summary:</p> <blockquote class="block-tip"> <h4 id="duality-representation-1-ssm">Duality Representation 1 (SSM)</h4> <p>The duality for the SSD model can be seen as two <strong>different matrix multiplication algorithms</strong> on the semiseparable matrix.</p> </blockquote> <ul> <li>The linear form is a <em>structured matrix multiplication algorithm</em> that computes the outputs $Y_0, Y_1, \dots$ sequentially, leveraging the structure of the semiseparable matrix.</li> <li>The quadratic form is the <em>naive matrix multiplication algorithm</em> that materializes the full matrix.</li> </ul> <h3 id="going-beyond-the-ssd-layer-1">Going Beyond the SSD Layer 1</h3> <p>The power of the semiseparable matrix representation applies to <em>all</em> state space models, with various downstream implications.</p> <h4 id="algorithms">Algorithms</h4> <p>Algorithmically, the Mamba-2 paper explores several consequences, such as:</p> <ol> <li>The above duality result for the SSD model, i.e. a scalar-identity structured SSM.</li> <li>New asymptotic efficiency results for state space models (<a href="https://arxiv.org/abs/2405.21060">Theorem 3.7</a>), which follow from applying known results from the semiseparable matrix literature <d-cite key="pernet2016computing"></d-cite><d-cite key="pernet2018time"></d-cite><d-cite key="pernet2023exact"></d-cite>.</li> <li>A more general hybrid algorithm that can be viewed as combining both the linear and quadratic forms to get the best of both worlds. This can be derived as a new matrix multiplication algorithm utilizing <em>block decompositions</em> of the semiseparable matrix. This is the subject of Part III of this blog series!</li> </ol> <h4 id="understanding">Understanding</h4> <p>Conceptually, the matrix transformation viewpoint helps provide a unifying view of sequence models. Some example downstream ideas include</p> <ul> <li><strong>New sequence models</strong>: Restricting ourselves to matrix transformations reduces the problem of developing new sequence models to that of finding structured matrix classes with target properties. In ongoing work by my students, we study this point of view, and use it to derive the most natural bidirectional extension of Mamba (coming very soon!).</li> <li><strong>Expressivity</strong>: Looking at the matrix transformation representation can help us understand what different models can represent from a linear algebraic perspective. In another ongoing work, we use this as a tool to study which subquadratic models are the most amenable to being distilled from Transformers.</li> <li><strong>Interpretability</strong>: A concurrent work <d-cite key="ali2024hidden"></d-cite> derived the matrix formulation of SSMs and use it to probe the internal representations of Mamba models.</li> </ul> <p>We‚Äôre excited to see what algorithmic and conceptual ideas from the structured matrix literature can be applied to further improve state space models!</p> <h2 id="ssd-framework-2-structured-attention">SSD Framework 2: Structured Attention</h2> <p>The second framing of the duality is from an attention-centric perspective, where we‚Äôll prove the duality through the framework of <strong>tensor contractions</strong>.</p> <p>Note that this is entirely independent of the previous [<a href="#ssd-framework-1-structured-matrix-transformations">matrix transformation viewpoint</a>].</p> <h3 id="warm-up-kernel-attention">Warm-up: Kernel Attention</h3> <p>For our purposes, we‚Äôll define attention as a function</p> \[(Q^\mathtt{(T,N)}, K^\mathtt{(S,N)} , V^\mathtt{(S,P)} ) \mapsto Y^\mathtt{(T,P)}\] <p>given by the pairwise matrix multiplications</p> \[Y = (QK^\top) \cdot V\] <details><summary>On Dimensions</summary> <p>Think of $\mathtt{P} = \mathtt{N}$ as the head dimension; technically speaking, in attention the $V$ head dimension $\mathtt{P}$ can differ from the $QK$ head dimension $\mathtt{N}$. Think of $\mathtt{T}$ as the <em>target</em> sequence dimension and $\mathtt{S}$ as the <em>source</em> sequence dimension. Giving these two axes different names will make the math more clear and also covers more general forms of attention such as cross-attention, where the source and target are separate sequences with different lengths. However, for our purposes we‚Äôll assume the self-attention setting where $\mathtt{S}=\mathtt{T}$.</p> </details> <details><summary>Why can we assume this form?</summary> <p>The usual form of attention $Y = f(QK^\top) \cdot V$ (e.g. where $f$ is the softmax function) can, for essentially all functions $f$<d-footnote>And up to some additional massaging such as row-wise normalization, which is easy to handle</d-footnote>, be written as $Y = \psi(Q)\psi(K)^\top \cdot V$ for some appropriate feature map $\psi$ (which may be infinite dimensional). In this case, we can simply redefine $Q \leftarrow \psi(Q)$ and define $\mathtt{N}$ to be the <strong>feature dimension</strong> of the attention kernel to begin with. Softmax attention, for example, can be represented with a particular infinite-dimensional feature map ($\mathtt{N}=\infty$) which represents the exponential kernel.</p> </details> <p>We‚Äôll restrict ourselves to the case when $\psi$ is finite, which is sometimes called <strong>kernel attention</strong>. Many, many variants have been proposed before!<d-cite key="katharopoulos2020transformers"></d-cite><d-cite key="peng2021random"></d-cite><d-cite key="choromanski2021rethinking"></d-cite><d-cite key="qin2022cosformer"></d-cite><d-cite key="zheng2022linear"></d-cite><d-cite key="wang2020linformer"></d-cite><d-cite key="xiong2021nystromformer"></d-cite></p> <p>Why do we care about this formulation? When the sequence length $\mathtt{T}$ grows and the feature dimension $\mathtt{N}$ is small‚Äîcommonly, in the regime when $\psi$ is simple such as an elementwise transform and so $\mathtt{N}$ is constant‚Äîthen the cost of attention can be reduced from quadratic in $\mathtt{T}$ to linear. This follows from simply computing the matrix multiplications in a different order</p> \[Y = Q \cdot (K^\top V)\] <p>This is a somewhat ‚Äúfolklore‚Äù interpretation of linear attention.<d-footnote>At least, one lineage of efficient attention; other varieties exist, such as those based on sparsity or hashing. We reserve the term "linear attention" to those related to Katharopoulos et al.<d-cite key="katharopoulos2020transformers"></d-cite>, or more broadly low-rank attention.</d-footnote></p> <blockquote> <p>The most common way of linearizing attention is usually viewed as a consequence of the <strong>associativity of matrix multiplication</strong></p> </blockquote> <h3 id="causal-linear-attention">(Causal) Linear Attention</h3> <p>However, once the basic kernel attention is slightly modified, we can no longer use the associativity of matrix multiplication directly.</p> <p>The seminal <strong>Linear Attention (LA)</strong> framework of Katharopoulos et al. <d-cite key="katharopoulos2020transformers"></d-cite> shows that it can still be extended to the important case of incorporating causality into attention, for autoregressive settings such as language modeling.</p> <p>Let‚Äôs be a lot more explicit about how it works. The quadratic form of <strong>causal linear attention</strong> is \begin{equation} \label{eq:quadratic-kernel-attention} Y = (L \circ QK^\top) \cdot V \end{equation} where</p> \[L = \begin{bmatrix} 1 \\ \vdots &amp; \ddots \\ 1 &amp; \dots &amp; 1 \end{bmatrix}\] <p>is the <strong>causal mask</strong> matrix.</p> <p>The issue is: once the $L$ mask is incorporated into \eqref{eq:quadratic-kernel-attention}, we can no longer directly apply matrix associativity! This is the problem that the original Linear Attention paper addresses. What they show is that \eqref{eq:quadratic-kernel-attention} is equivalent to a different form which avoids materializing the quadratic $QK^\top$ attention matrix and has linear time complexity</p> \[Y = Q \cdot \mathsf{cumsum}(K^\top V)\] <p>As far as we‚Äôre aware this wasn‚Äôt explicitly proved in the paper, although it isn‚Äôt too hard to write out the summation to show it.</p> <p>What we‚Äôll do is prove this equivalence in essentially one line, while revealing <em>exactly</em> where the ‚Äúlinear‚Äù part of Linear Attention comes from, and how to strongly generalize it.</p> <p>Spoiler alert:</p> <blockquote class="block-tip"> <h4 id="where-does-the-cumsum-in-linear-attention-come-from">Where does the cumsum in Linear Attention come from?</h4> <p>The appearance of the <em>cumulative sum</em> in linear attention is exactly equivalent to the fact that the causal mask $L$, as a matrix multiplication, encodes cumulative sums:</p> \[y = L \cdot x \iff y = \mathsf{cumsum}(x)\] </blockquote> <h3 id="a-tensor-contraction-proof-of-linear-attention">A Tensor Contraction Proof of Linear Attention</h3> <p>Let‚Äôs write out the quadratic form of linear attention \eqref{eq:quadratic-kernel-attention} very explicitly in <strong>tensor contraction</strong> or <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a> notation, with shape annotations:</p> \[\begin{aligned} G &amp;= \mathsf{contract}(\mathtt{TN, SN} \to \mathtt{TS})(Q, K) \\ M &amp;= \mathsf{contract}(\mathtt{TS, TS} \to \mathtt{TS})(G, L) \\ Y &amp;= \mathsf{contract}(\mathtt{TS, SP} \to \mathtt{TP})(M, V) \end{aligned}\] <p>\begin{equation} \label{eq:sma-quad} (\text{Structured Masked Attention - Quadratic Form}) \end{equation}</p> <p>With this notation, we can notice that this sequence of contractions can be written as a <em>single four-way contraction</em></p> <p>\begin{equation} \label{eq:sma} y = \mathsf{contract}(\mathtt{TN},\mathtt{SN},\mathtt{SP},\mathtt{TS} \to \mathtt{TP})(Q, K, V, L) . \end{equation}</p> <p>And finally, it can be computed with any other contraction ordering. In particular, we can perform pairwise reductions on the order $V, K, L, Q$ instead of $Q, K, L, V$</p> \[\begin{aligned} Z &amp;= \mathsf{contract}(\mathtt{SP},\mathtt{SN} \to \mathtt{SPN})(V, K) \\ H &amp;= \mathsf{contract}(\mathtt{TS},\mathtt{SPN} \to \mathtt{TPN})(L, Z) \\ Y &amp;= \mathsf{contract}(\mathtt{TN},\mathtt{TPN} \to \mathtt{TP})(Q, H) \end{aligned}\] <p>\begin{equation} \label{eq:sma-lin} (\text{Structured Masked Attention - Linear Form}) \end{equation}</p> <p>Now the key observation is that the second line of \eqref{eq:sma-lin} is simply a matrix multiplication by $L$, which can be computed with a cumulative sum.</p> <p>That‚Äôs the entire proof of linear attention! The beauty of it is that we didn‚Äôt have to write out a single summation, which was abstracted out into a tensor contraction combined with the structure of $L$.</p> <p>This immediately proves our claim about the <a href="#where-does-the-cumsum-in-linear-attention-come-from">cumsum in linear attention</a>. Moreover, this immediately reveals that the efficiency of linear attention can be made much more general‚Ä¶</p> <h3 id="structured-masked-attention">Structured Masked Attention</h3> <p>The critical observation is that in order for \eqref{eq:sma-lin} to be fast, all that is necessary is for $L$ to be <em>any structured matrix</em> ‚Äì in other words any matrix that has subquadratic matrix-vector multiplication.</p> <p>This immediately motivates one of the main prongs of the SSD framework, which can be seen as a strong generation of LA.</p> <blockquote class="block-tip"> <h4 id="definition-structured-masked-attention">Definition: Structured Masked Attention</h4> <p><strong>Structured masked attention (SMA)</strong> is defined as the <em>four-way tensor contraction</em> \eqref{eq:sma} using an attention mask $L$ that is a structured matrix.</p> </blockquote> <blockquote class="block-tip"> <h4 id="duality-representation-2-sma">Duality Representation 2 (SMA)</h4> <p>SMA has <strong>dual quadratic and linear</strong><d-footnote>Assuming that the structured matrix $L$ has linear time matrix-vector multiplication</d-footnote> <strong>modes</strong> which are simply <em>two different pairwise reduction orders</em> \eqref{eq:sma-quad} and \eqref{eq:sma-lin}.</p> </blockquote> <p>Finally, let‚Äôs just connect this back to the commonly held view of linear attention as matrix multiplication associativity.</p> <blockquote> <p>Although it is commonly believed that incorporating attention masks $L$ prevents matrix multiplication reordering, it turns out to still be compatible. In particular, <strong>associativity of matrix multiplication</strong> is a special case of <strong>tensor contraction reduction orders</strong>; although the former no longer applies, the latter can integrate the attention mask $L$.</p> </blockquote> <p>Next, let‚Äôs look at some consequences of the structured attention framework.</p> <h3 id="deriving-the-duality-attention-to-ssm">Deriving the Duality: Attention to SSM</h3> <p>Recall that the SSD model is defined as either a scalar-identity SSM in equation \eqref{eq:ssm}, or through the attention-like form in equation \eqref{eq:ssd-attention}.</p> <p>To show the equivalence of these forms, we simply recognize that \eqref{eq:ssd-attention} is a special case of structured masked attention where the mask matrix is</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix} .\] <p>\begin{equation} \label{eq:1-ss} (\text{1-semiseparable (1-SS) matrix}) \end{equation}</p> <p>We call this a <strong>1-semiseparable (1-SS) matrix</strong>, for reasons that are explained in more detail in the Mamba-2 paper.</p> <p>Thus, we can also say that the SSD model is <strong>1-semiseparable masked attention</strong> or <strong>1-SS SMA</strong>.</p> <p>To prove that this can be written as an SSM, we simply appeal to the SMA framework, which says that the dual form of this model can be computed through matrix multiplication by $L$. So how fast is that? It‚Äôs not too hard to see that multiplication $y = Lx$ can be computed in linear time through a scalar recurrence:</p> \[\begin{aligned} y_0 &amp;= x_0 \\ y_1 &amp;= a_1 x_0 + a_1 \\ y_2 &amp;= a_2a_1 x_0 + a_2 x_1 + x_2 = a_2 y_1 + x_2 \\ \vdots &amp; \qquad \vdots \end{aligned}\] <p>This corresponds exactly to the original SSM recurrence!</p> <p>(In fact, multiplication by 1-SS matrices $L$ can be computed in a <em>lot</em> more ways, which we compile in the full paper! Alternative algorithms can reveal more insights: for example, the associative scan algorithm used by S5 <d-cite key="smith2023s5"></d-cite> and Mamba can also be shown to be a structured matrix multiplication algorithm on 1-SS matrices.)</p> <h3 id="going-beyond-the-ssd-layer-2">Going Beyond the SSD Layer 2</h3> <p>Structured masked attention not only helps define the SSD model and prove its duality, but it is a much broader framework of efficient attention models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/sma-480.webp 480w,/assets/img/2024-05-31-mamba-2/sma-800.webp 800w,/assets/img/2024-05-31-mamba-2/sma-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/sma.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prior examples include the original linear attention as well as the recent Retentive Network (RetNet) model<d-cite key="sun2023retentive"></d-cite>. These can be viewed as direct special cases of SSD. But beyond SSD, we can define classes of efficient attention by replacing the mask $L$ with <em>any structured matrix</em>. As a suggestion, we think that Toeplitz or Fourier structured attention may be interesting to consider because they might encode different forms of positional information.</p> <p>Additionally, other forms of structure can be incorporated into the $L$ mask. For example, another extension my students are developing is viewing SSD (and recurrences in general) as an algorithm operating on <em>directed line graphs</em>, and generalizing it to incorporate arbitrary graph structures.</p> <h2 id="state-space-duality">State Space Duality</h2> <p>We‚Äôll end this post with a brief recap of what we‚Äôve covered.</p> <p>The <strong>SSD framework</strong> consists of the two broad approaches covered in this post, which is summarized by the two areas of the [<a href="#the-state-space-duality-framework">Venn diagram</a>]:</p> <ol> <li>Viewing state space models through [<a href="#ssd-framework-1-structured-matrix-transformations">structured matrix transformations</a>]</li> <li>Generalizing linear attention through [<a href="#ssd-framework-2-structured-attention">tensor contractions</a>]</li> </ol> <p>The [<a href="#recap-the-ssd-model">SSD layer</a>] is a particular model which is the purple intersection in the figure, which can be viewed as an instance of either part of the SSD framework, and in particular has dual quadratic and linear forms that can be derived from either representation.</p> <table> <thead> <tr> <th><em>SSD Framework</em></th> <th>Structured SSMs</th> <th>Structured Attention</th> </tr> </thead> <tbody> <tr> <td>The main representation is‚Ä¶</td> <td>Structured matrix \eqref{eq:ssm-matrix} <br/> sequence transformations</td> <td>The 4-way \eqref{eq:sma} <br/> tensor contraction</td> </tr> <tr> <td>This generalizes‚Ä¶</td> <td>State space models</td> <td>Linear attention</td> </tr> <tr> <td>The SSD model is <br/> an instantiation as‚Ä¶</td> <td>Scalar state space model <br/> ($A_t$ is a scalar-identity matrix)</td> <td>1-semiseparable masked attention <br/> ($L$ mask is a 1-SS matrix)</td> </tr> <tr> <td>The linear-quadratic duality is <br/> revealed through‚Ä¶</td> <td>Structured matrix <br/> multiplication algorithms</td> <td>Tensor contraction <br/> reduction orderings</td> </tr> </tbody> </table> <h2 id="next-up">Next Up</h2> <p>In <a href="/blog/2024/mamba2-part3-algorithm/">the next part of this series</a>, we‚Äôll see how to use some of the SSD framework (in particular, the <a href="#takeaway-computing-ssms">structured matrix algorithm</a> point of view) to derive the more efficient hybrid SSD algorithm that leverages both of the dual forms.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry></feed>