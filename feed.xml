<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://goombalab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://goombalab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-08T16:21:15+00:00</updated><id>https://goombalab.github.io/feed.xml</id><title type="html">Goomba Lab</title><subtitle>Homepage of the Goomba AI Lab @ CMU MLD. </subtitle><entry><title type="html">On the Tradeoffs of SSMs and Transformers</title><link href="https://goombalab.github.io/blog/2025/tradeoffs/" rel="alternate" type="text/html" title="On the Tradeoffs of SSMs and Transformers"/><published>2025-07-08T00:00:00+00:00</published><updated>2025-07-08T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2025/tradeoffs</id><content type="html" xml:base="https://goombalab.github.io/blog/2025/tradeoffs/"><![CDATA[<p>This blog post was adapted from a talk I‚Äôve given a handful of times over the last year. It was meant to be a high-level talk accessible to a fairly broad audience, but hopefully has some interesting insights, opinions, and intuitions around sequence models for the dedicated researchers too.</p> <h2 id="state-space-models">State Space Models</h2> <p>Just so we‚Äôre on the same page, I‚Äôll start by defining what I mean by a state space model. (This section isn‚Äôt strictly necessary to get to the main part of this post though; feel free to skip directly to <a href="#states-brains-and-databases">the next section</a>.)</p> \[\begin{equation} \label{eq:ssm} \begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} h_t \end{aligned} \end{equation}\] <p>These equations define the (structured) state space model (SSM) as developed in a line of work <d-cite key="gu2023thesis"></d-cite> culminating in Mamba <d-cite key="gu2023mamba"></d-cite>. They can be viewed as a modern version of a recurrent neural network (RNN) with a few key characteristics. While a lot of technical work was involved in getting this family of models to work, I‚Äôll start by trying to abstract away what I view as the main high-level ingredients that made these models successful, e.g. match the performance of Transformers on language modeling.</p> <h3 id="the-three-ingredients">The three ingredients</h3> <h4 id="1-state-size">1. State size</h4> <p>A characteristic of the SSM is that its hidden state $h_t$ has a larger size than the the inputs and outputs $x_t, y_t$. The key idea is that the hidden state of any recurrent model is its only access to the model‚Äôs context (in an autoregressive setting). So for modeling information-dense modalities such as language, the model needs a large enough state to store the relevant information that it wants to access later.</p> <p>In SSMs, if each input $x_t$ is a 1-dimensional scalar, then the hidden state $h_t$ is an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the <em>state size, state dimension, or state expansion factor</em>. This is also known as a SISO (single-input single-output) SSM and allows the models to store $\mathtt{N}$ times as much information as older RNNs such as LSTMs and GRUs <d-cite key="lstm"></d-cite><d-cite key="chung2014empirical"></d-cite>.</p> <h4 id="2-state-expressivity">2. State expressivity</h4> <p>Not only does the model need to have a large enough state to <em>theoretically</em> store relevant context, it needs to have an expressive enough state update function to encode and access exactly the information it needs.</p> <p>Earlier versions of ‚Äúlinear time-invariant‚Äù SSMs used simple recurrences $h_{t} = A h_{t-1} + B x_t$ whose updates are constant at every time step <d-cite key="gu2023thesis"></d-cite>. While this works great for compressible data like audio, it doesn‚Äôt provide enough flexibility for sequences with variable information rates like language, where the model may have to selectively choose what information to remember. <strong>Selective SSMs</strong> like Mamba fix this by making the recurrence more expressive by letting the transition matrices vary through time and depend on the data itself. These mechanisms are closely related to the gating mechanisms of classical RNNs!</p> <p>This is the area with the most active research on modern recurrent models, which are focused on understanding the theoretical expressivity of different parameterizations of the transition matrix $A_t$ and what they allow the model to remember in its state.</p> <h4 id="3-training-efficiency">3. Training efficiency</h4> <p>Having a larger and more expressive recurrent state is important, but comes with a critical trade-off ‚Äì the model becomes much harder to compute. Mamba addressed this with careful parameterization of the recurrence and utilizing the classic parallel scan algorithm<d-cite key="blelloch1990prefix"></d-cite><d-cite key="martin2018parallelizing"></d-cite>.</p> <p>Many other algorithmic innovations have appeared, all with a few shared characteristics:</p> <ul> <li><strong>Parallelization</strong>: They aim to be parallelizable and practically efficient on accelerators like GPUs and TPUs ‚Äì preferably leveraging matrix multiplications (matmuls) as the workhorse.</li> <li><strong>Memory management</strong>: They have to control memory usage carefully. In particular, any model that uses state expansion can‚Äôt actually materialize the state in main memory! While Mamba brute-forced the problem using clever awareness of the GPU memory hierarchy <d-cite key="gu2023mamba"></d-cite>, most alternatives find ways of rewriting the equations entirely to use different computation paths that don‚Äôt need to compute the state explicitly during a parallel training pass.</li> <li><strong>Linearity</strong>: The model generally has to be linear in $x_t$, leading some to call this whole family of models <em>linear recurrent models</em>. Linearity plays a role in both computational efficiency as well as modeling/optimization ability, which I won‚Äôt get into here.</li> </ul> <h3 id="mamba---putting-it-all-together">Mamba - putting it all together</h3> <p>None of these three ingredients is new:</p> <ol> <li>Linear attention <d-cite key="katharopoulos2020transformers"></d-cite><d-cite key="sun2023retentive"></d-cite> and earlier SSMs <d-cite key="gu2021combining"></d-cite><d-cite key="gu2022efficiently"></d-cite> had similar equations utilizing state expansion.</li> <li>Selectivity was inspired by, and closely related to, gating mechanisms in classical RNNs like the LSTM and GRU <d-cite key="lstm"></d-cite><d-cite key="chung2014empirical"></d-cite>.</li> <li>Parallel scans were utilized in earlier SSMs/linear RNNs like S5 <d-cite key="smith2023s5"></d-cite> and LRU <d-cite key="orvieto2023resurrecting"></d-cite>. Linear attention variants also used parallelizable training algorithms leveraging matmuls.</li> </ol> <p>What Mamba did was show that <strong>combining all of these together</strong> was the key to a step change in empirical performance and approaching Transformers on language modeling.</p> <h3 id="modern-recurrent-models">Modern recurrent models</h3> <p>Since then, there‚Äôs been a flurry of activity on continuing to understand and improve recurrent models. Many of them come from different motivations with different nomenclatures and terminologies.</p> <ul> <li>Some models such as RWKV <d-cite key="peng2023rwkv"></d-cite><d-cite key="peng2024eagle"></d-cite><d-cite key="peng2025rwkv"></d-cite>, xLSTM <d-cite key="katharopoulos2020transformers"></d-cite>, and Griffin <d-cite key="de2024griffin"></d-cite> come from an <strong>RNN-centric</strong> point of view and call Ingredient 1 <em>matrix-valued states</em> and Ingredient 2 <em>gating</em>.</li> <li><strong>Linear attention</strong> <d-cite key="katharopoulos2020transformers"></d-cite> first combined Ingredients 1 and 3; later variants such as GLA<d-cite key="yang2024gated"></d-cite> and Gated DeltaNet<d-cite key="yang2025gated"></d-cite> incorporate various forms of selectivity (data-dependent recurrence) and use attention-based terminology such as using $(K, Q, V)$ instead of $(B, C, X)$. Mamba-2 can also be simultaneously seen as either an SSM or a linear attention <d-cite key="dao2024transformers"></d-cite>.</li> <li>Recently, many of these models have been cast into a framework of <strong>test-time training/regression</strong><d-cite key="liu2024longhorn"></d-cite><d-cite key="sun2024learning"></d-cite><d-cite key="wang2025test"></d-cite><d-cite key="von2025mesanet"></d-cite>, which views the recurrent update as online optimization on some objective for remembering the context. The state is viewed as an <em>associative memory</em> and parallelization happens through a notion of <em>minibatch gradient descent</em>.</li> </ul> <p>A core commonality is that almost all of these models can be cast into the same SSM equation \eqref{eq:ssm}, with the main axes of variations being in the structure of $A_t$ (Ingredient 2) and corresponding efficient training algorithms (Ingredient 3). So I‚Äôll use the term <strong>state space model</strong> (or just ‚Äúmodern recurrent model‚Äù) to refer broadly to this large class of new models, as it captures their main shared characteristics (e.g. SISO linear recurrence with state expansion). But of course, there are many other reasonable names given the closely related ideas!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/recurrent_models-480.webp 480w,/assets/img/2025-07-08-tradeoffs/recurrent_models-800.webp 800w,/assets/img/2025-07-08-tradeoffs/recurrent_models-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/recurrent_models.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">This figure is from Songlin Yang's excellent <a href="https://arxiv.org/abs/2406.06484">DeltaNet</a> paper, which shows how the huge proliferation of modern recurrent models all fits into this framework (using linear attention notation).</figcaption> </figure> <p>Despite the accelerating amount of research into this direction and steady stream of new models, however, I think that all of them are still quite similar to each other and have roughly similar empirical performance, for the most part. In particular, <strong>all of these models are much more similar to each other than they are to quadratic attention</strong>. So in the rest of this post, we‚Äôre going to try to get a grasp on the higher-level tradeoffs between SSMs and Transformers.</p> <h2 id="states-brains-and-databases">States, Brains, and Databases</h2> <p>I claim that we can understand the trade-offs of different models better by looking at what they store in (and how they manipulate) their <strong>autoregressive state</strong>.</p> <p>What does that mean? In some sense, every <em>autoregressive model</em> ‚Äì one that generates data sequentially left-to-right like modern LLMs ‚Äì is a ‚Äústate space model‚Äù that holds some state in memory and evolves it on every time step (e.g. in between every generated word for an LLM).</p> <h3 id="autoregressive-states-of-sequence-models">Autoregressive states of sequence models</h3> <p>(Causal) self-attention, the core component of autoregressive Transformers, is often defined through a specific operation involving computing the pairwise interactions between every element of the sequence <d-cite key="vaswani2017attention"></d-cite>. Consequently, its computation cost scales <em>quadratically</em> in the sequence length, which is often viewed as the main drawback of attention.</p> <p>On the other hand, because computing one step of the recurrence \eqref{eq:ssm} takes constant time, processing an entire sequence scales <em>linearly</em> with the length of the sequence, which is often viewed as the main advantage of state space models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/state-480.webp 480w,/assets/img/2025-07-08-tradeoffs/state-800.webp 800w,/assets/img/2025-07-08-tradeoffs/state-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/state.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>But instead of thinking of the training cost of these models, I find it more illuminating to think about what happens at inference time when they process a new input.</p> <ul> <li>When a self-attention layer receives a new token, it needs to compare it to all the previously seen elements of the sequence, which means that <em>it must have cached a representation for every single prior token in the context</em>. Every new input it sees must get added to the cache, which therefore grows linearly in the context size.</li> <li>On the other hand, a state space model has always summarized its context $x_1, \cdots, x_t$ into the hidden state $h_t$ (equation \eqref{eq:ssm}), which always has a constant size. This fixed-size state is the only means by which the model can interact with data: it streams data in, compresses it into its state, and uses that to make decisions or produce new outputs.</li> </ul> <p>Without even getting into the details of the definitions of these various models, I think it‚Äôs roughly accurate to say that we could have defined them from first principles through their autoregressive states:</p> <ul> <li><strong>Transformers (self-attention) are characterized by a state that caches every element of its history</strong>, and interacts with new data by doing a pass over every element of the cache.</li> <li><strong>SSMs are characterized by a state that compresses all its history</strong>, and interacts with new data in an online streaming fashion.</li> </ul> <details><summary>Aside: The ‚ÄúKV‚Äù cache</summary> <p>The Transformer cache is, of course, more formally known as the <strong>KV cache</strong>, where ‚ÄúKV‚Äù refers to specific parts of how attention was first defined and named (key and value).</p> <p>But the point of this description is that I think that rather than defining the KV cache as a consequence of attention; perhaps in an alternative universe, (causal) self-attention could have been derived from first principles as the canonical model that stores a cache (‚ÄúKV‚Äù or not) of its context. So in this post, I mainly call it a ‚Äúcontext cache‚Äù or ‚Äútoken cache‚Äù instead to abstract out the main principle instead of implementation detail.</p> <p>As an aside, it‚Äôs rather interesting/amusing to me that often when I talk to LLM researchers, they call the recurrent state of SSMs a ‚Äútype of KV cache‚Äù rather than calling the KV cache a type of state, which IMO is much more accurate and descriptive.</p> </details> <h3 id="a-coarse-analogy">A coarse analogy</h3> <p>Although SSMs are often viewed as more efficient but somewhat weaker versions of Transformers, it‚Äôs not as simple as that. Even ignoring computational efficiency, these models do have different tradeoffs in their inductive biases (or modeling power). Given the nature of the way they process data, here‚Äôs a rough analogy that I like.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/analogy-480.webp 480w,/assets/img/2025-07-08-tradeoffs/analogy-800.webp 800w,/assets/img/2025-07-08-tradeoffs/analogy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/analogy.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Transformers are like databases</strong>: they treat every new observation as an important item that is filed away for future reference.</p> <p>On the other hand, <strong>SSMs are like brains</strong>: finite-sized memories that are always on, processing new inputs and producing new outputs in real-time.</p> <p>This analogy is a bit superficial, but does help intuitively explain some of the empirical behaviors that are observed. For example, SSMs can‚Äôt memorize a phonebook in one pass and then recite it back, or recall an arbitary person‚Äôs phone number from memory <d-cite key="jelassi2024repeat"></d-cite><d-cite key="waleffe2024empirical"></d-cite>. But then of course, neither can humans ‚Äì we‚Äôre hopelessly bad at exact memorization and retrieval ‚Äì but that doesn‚Äôt seem to hinder intelligence from arising! On the other hand, Transformers have a fundamental hard limit on context length (once the cache size is exceeded), while recurrent models like SSMs can hypothetically maintain an infinitely long (but fuzzy) memory of the past like humans have.</p> <details><summary>Aside: Context compression</summary> <p>The aforementioned limitation on context length might be circumvented by newer context compression techniques, which involve a more clever iterative process of throwing out the entire cache and trying to compress it into a shorter summary, so that new information can be processed that otherwise would overflow the cache. This of course must be lossy, and makes the whole system start resembling an SSM more.</p> <p>Similarly, the limitations of SSM may be alleviated by more clever iterative techniques of interacting with the data. For example, issues with recall might be remedied by giving them another pass over the data ‚Äì just as how humans will look things up in external references.</p> <p>The theme here is that sometimes limitations of methods are not so black-and-white. They can depend on the way in which models are used and more generally on higher system-level changes. But we‚Äôre not going to get into these nuances for the purposes of this post.</p> </details> <details><summary>Aside: Long context</summary> <p>Something worth pointing out is that ‚Äúlong context‚Äù is a very popular, but horribly overloaded and ill-defined term. Both Transformers and SSMs have been touted as having better ‚Äúlong-context abilities‚Äù as a blanket statement, which can‚Äôt both be accurate.</p> <p>The reason is because they have very different <em>types</em> of memory. Going back to the analogy, I wouldn‚Äôt say that there is a clear winner comparing, say, my own memory vs. my research notes. They‚Äôre both just different: my notes lets me refer back to specific details I may have forgotten, but my brain remembers a much longer history of fuzzy context. Transformers and SSMs probably have similar qualitative differences that are difficult to measure.</p> <p>I‚Äôm very curious, for example, if large-scale SSMs (if trained properly with modern <a href="https://goombalab.github.io/blog/2025/improving-length-generalization/">length extrapolation techniques</a> <d-cite key="buitrago2025understanding"></d-cite>) would overcome the finite context problem that some chatbot users have complained about. Maintaining a continual conversation with an assistant is much more like human conversations and relationships: what matters is a long, persistent <em>summary</em> of the context, remembering the <em>shape and flow</em> of the interactions without needing to recall every specific detail. No one needs a scratchpad to have a continual relationship with their friend. This is exactly where the more brain-like nature of SSMs is more suitable than the database-like nature of Transformers, which instead may be better suited for AI tasks requiring precision and retrieval.</p> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-480.webp 480w,/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-800.webp 800w,/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/intelligence_hybrid.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A more intriguing empirical finding that might be predicted from the analogy is that combining both types of information processing may be even more capable! Just as human intelligence is augmented by having explicit scratch pads and external references, language models get better when combining SSMs with attention layers by a simple interleaving strategy.</p> <p>And what‚Äôs even more intriguing is that the optimal ratio of these layers, as independently verified by dozens of research groups by now (<a href="https://arxiv.org/abs/2212.14052">H3</a>, <a href="https://arxiv.org/abs/2403.19887">Jamba</a>, <a href="https://arxiv.org/abs/2405.16712">Zamba</a>, <a href="https://arxiv.org/abs/2406.07522">Samba</a>, and many more that followed after)<d-cite key="dao2023hungry"></d-cite><d-cite key="lieber2024jamba"></d-cite><d-cite key="glorioso2024zamba"></d-cite><d-cite key="ren2025samba"></d-cite>, is somewhere between a roughly 3:1 to 10:1 ratio of SSM:attention layers.<d-footnote>Note that this isn't factoring in computation cost (which is usually what's highlighted when comparing Transformers vs SSMs) - we're just talking about raw modeling ability. Put another way, taking a pure Transformer model and replacing some (or most) of the layers with SSM layers would both improve efficiency *and* performance.</d-footnote> This might track the coarse analogy if one believed that human intelligence is mostly in the brain and augmented by lightweight access to external databases! These hybrid models have now been scaled up to very serious sizes (e.g. MoE with 560B total parameters) by major labs, like NVIDIA‚Äôs <a href="https://research.nvidia.com/labs/adlr/nemotronh/">Nemotron-H</a> <d-cite key="blakeman2025nemotron"></d-cite> and Tencent‚Äôs <a href="https://tencent.github.io/llm.hunyuan.T1/README_EN.html">T1/TurboS</a> <d-cite key="liu2025hunyuan"></d-cite> with state-of-the-art performance.<d-footnote>Fun fact: both of these models were announced on the same day, my birthday üòÇ (completely by coincidence)</d-footnote></p> <details><summary>Aside: Perplexity</summary> <p>When I talk about performance here, I‚Äôm specifically referring to perplexity. As a community, we now know that there are more nuances to the downstream performance, in particular <em>algorithmic capabilities</em> of different types of models<d-cite key="bick2025understanding"></d-cite>. But perplexity is still perhaps the most pure metric of the <em>statistical ability to model language as a distribution of sequences</em>, the original definition of language modeling.</p> <p>I actually believe that pound-for-pound (or FLOP-for-FLOP), SSMs are better than Transformers at modeling language, in this sense. But of course, there are many other downstream capabilities that have other differences and are important to understand.</p> </details> <h2 id="is-attention-all-you-need">Is Attention All You Need?</h2> <p>So <a href="https://arxiv.org/abs/1706.03762">attention is all you need</a>, right? There‚Äôs a perception of Transformers being the ultimate architecture that can learn anything from raw data, the more the better, with having enough compute being the only bottleneck.</p> <blockquote class="block-danger"> <h4 id="myth">Myth</h4> <p>Just throw your data at a Transformer <em>üôÇ</em></p> </blockquote> <p>Well, not quite. Attention is indeed amazing and has become an effective backbone for pretty much all modalities, from its original applications in language to <a href="https://arxiv.org/abs/2010.11929">vision</a> and <a href="https://arxiv.org/abs/2005.08100">audio</a> and beyond<d-cite key="dosovitskiy2021image"></d-cite><d-cite key="gulati2020conformer"></d-cite>. But there is some more nuance to it.</p> <blockquote class="block-tip"> <h4 id="reality">Reality</h4> <p>Attention is most effective on<br/> <strong>pre-compressed data</strong> at the ‚Äú<strong><em>right level of abstraction</em></strong>‚Äù</p> </blockquote> <p>I claim instead that in order to use a Transformer effectively, the <em>data has to be substantially processed</em>. To support this claim, let‚Äôs look at how they‚Äôre actually used in practice.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/patches-480.webp 480w,/assets/img/2025-07-08-tradeoffs/patches-800.webp 800w,/assets/img/2025-07-08-tradeoffs/patches-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/patches.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/tokenizers-480.webp 480w,/assets/img/2025-07-08-tradeoffs/tokenizers-800.webp 800w,/assets/img/2025-07-08-tradeoffs/tokenizers-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/tokenizers.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In pretty much all real pipelines, raw data is processed by an encoder before being fed to a Transformer, for example:</p> <ul> <li>The <strong>patchification</strong> step in vision pipelines (whether <a href="https://arxiv.org/abs/2010.11929">classification</a> or <a href="https://arxiv.org/abs/2212.09748">generation</a>)<d-cite key="dosovitskiy2021image"></d-cite><d-cite key="peebles2023scalable"></d-cite>.</li> <li>The <strong>tokenization</strong> step of language modeling.</li> </ul> <p>This may seem intuitive: after all, because of the quadratic complexity of attention, of course it makes sense to try to simplify the data (such as shortening input sequences).</p> <p>But my claim is <em>not just about computational efficiency</em>; I‚Äôm making a stronger statement about limitations in <em>modeling power</em>.</p> <p>Let‚Äôs dig in more here.</p> <h3 id="should-we-get-rid-of-tokenization">Should we get rid of tokenization?</h3> <p>Tokenization is a notorious step of all language modeling pipelines (most commonly the ‚ÄúBPE‚Äù algorithm <d-cite key="sennrich2016neural"></d-cite>, which I‚Äôll use interchangeably with ‚Äútokenization‚Äù), where textual data is processed into contiguous chunks, essentially encoding them into coarser features than the raw character-level data. It has a number of failure modes such as the <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">SolidGoldMagikarp</a> edge case and the infamous ‚ÄúHow many R‚Äòs are there in the word ‚Äòstrawberry‚Äô?‚Äù test.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/karpathy-480.webp 480w,/assets/img/2025-07-08-tradeoffs/karpathy-800.webp 800w,/assets/img/2025-07-08-tradeoffs/karpathy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/karpathy.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Taken with permission from the most prominent <a href="https://x.com/karpathy/status/1657949234535211009">hater</a> <a href="https://x.com/karpathy/status/1759996551378940395">of</a> <a href="https://x.com/karpathy/status/1816637781659254908">tokenizers</a>. The enemy of my enemy is a friend of mine!</figcaption> </figure> <p>So why do we use it?</p> <p>From polling a lot of opinions, almost everyone agrees that tokenizers are clunky and ugly, but a ‚Äúnecessary evil‚Äù.<d-footnote>It's kind of uncanny how many people use this exact phrasing!</d-footnote> Practically speaking, they sub-sample the sequence by a factor of around $5\times$ which dramatically improves the efficiency of the core language model. Despite the edge cases ‚Äì which are gradually being understood and patched out ‚Äì they <em>just work</em>, for the most part. It would be <em>nice</em> to get rid of them, but it‚Äôs not worth a dedicated effort.</p> <p>I, on the other hand, <strong>deeply believe that we should get rid of tokenization</strong>. I‚Äôm driven by aesthetics much more than the average person, I‚Äôd guess, and it‚Äôs because I think that they are rooted in intuition and intangible reasons that usually lead to deeper consequences down the line, even if we can‚Äôt predict them. In this case, I think that the consequences of overcoming tokenization <em>will extend far beyond the surface-level implications</em>.</p> <blockquote> <p>We should care about removing tokenization, not (just) for the practical reasons, but for the aesthetic and intangible reasons.</p> </blockquote> <p>Besides fixing the edge cases, removing tokenization simply <strong>adheres closer to the spirit of deep learning</strong>. Deep learning has always been about replacing handcrafted feature engineering with powerful end-to-end neural networks that can learn patterns automatically from data. From CNNs replacing manually engineered edge detectors in computer vision, to Transformers replacing linguistic features in NLP, major advances in AI have always happened with <strong>less data processing and more automatic learning</strong> (as popularly espoused by <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>).</p> <p>I believe that replacing tokenization with end-to-end models will have huge consequences for</p> <ul> <li><strong>scaling laws</strong>: learning better patterns from raw data always leads to more powerful models;</li> <li><strong>multilingual and multimodal</strong>: tokenization is notoriously hard or impossible for certain languages and other types of sequential data;</li> <li><strong>reasoning</strong>: because models can learn more semantically meaningful patterns from the data, and reason over higher levels of abstraction;</li> </ul> <p>and much more, including probably a lot of implications I haven‚Äôt foreseen yet.</p> <p>(As I was writing this post up, Luca Periƒá released a parallel blog post focused specifically on tokenization and tokenizer-free architectures. <a href="https://lucalp.dev/bitter-lesson-tokenization-and-blt/">Check it out</a>!)</p> <h3 id="so-what-happens-without-tokenization">So what happens without tokenization?</h3> <p>In the modern era of LLMs, there‚Äôve been astonishingly few papers that have thought about or tried to address this problem. It‚Äôs hard to even find trustworthy benchmarks about the performance of tokenizer-free models.</p> <p>So here‚Äôs a plot from our upcoming paper where we carefully ran standard architectures on byte-level language modeling (essentially, treating each English character as a separate token). (Note: Byte-level modeling with Mamba was first attempted by <a href="https://arxiv.org/abs/2401.13660">MambaByte</a> <d-cite key="wang2024mambabyte"></d-cite> from Sasha Rush‚Äôs group. This is a reproduction.)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/bpb_curve-480.webp 480w,/assets/img/2025-07-08-tradeoffs/bpb_curve-800.webp 800w,/assets/img/2025-07-08-tradeoffs/bpb_curve-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/bpb_curve.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Byte-level models trained on FineWeb-Edu (context length 8192). Sliding window attention (width=1024) is FLOP matched to Mamba, while global attention uses $2\times$ the FLOPs.</figcaption> </figure> <p>There are a number of implications here that most LLM researchers I‚Äôve talked to find surprising.</p> <p>The first thing to note is that the SSM performs <em>much</em> better than the FLOP-matched Transformer. This might not seem surprising to many because byte sequences are much longer than BPE-token sequences, and the quadratic complexity of Transformers kicks in.</p> <p>But as I said earlier, the weakness of Transformers is not (just) about efficiency, but about modeling power. And what‚Äôs notable about this plot (in particular, focusing on global attention) is that <strong>when matching for <em>data</em> instead of compute, allowing the Transformer to use many more FLOPs, the SSM still outperforms it consistently</strong>!<d-footnote>This plot is the result after we specifically tuned for the global Transformer baseline; In other settings (e.g. different combinations of network width/depth/optimizer hyperparameters), there was a much larger gap between Mamba and global attention.</d-footnote></p> <p>For contrast: if we compared these models on the <em>exact same data, but tokenized</em><d-footnote>This experiment used sequences of 8k characters, which would be roughly 2k tokens long, a standard length.</d-footnote>, their perplexity curves would look approximately the same (or the Transformer would be slightly better). So keeping the <em>same models</em> and the <em>same data</em>, but simply untokenizing the inputs, simultaneously <strong>lets the Transformer use much more compute</strong> but also <strong>decreases its performance relative to the SSM</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/dna_scaling-480.webp 480w,/assets/img/2025-07-08-tradeoffs/dna_scaling-800.webp 800w,/assets/img/2025-07-08-tradeoffs/dna_scaling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/dna_scaling.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Here‚Äôs another example. This plot is from the original Mamba paper, where we showed that Mamba scaled substantially better than Transformer out-of-the-box on DNA language modeling. Once again, this is a ‚Äútokenization-free‚Äù language with high-resolution input and small vocabulary size (just 4!), and the SSM strongly outperforms the Transformer when <em>data-matched</em> (while using less compute).</p> <p>(By the way, I hypothesize that these results for tokenizer-free models would hold for any reasonable variant of SSMs, such as probably most of the [<a href="#modern-recurrent-models">modern recurrent models</a>].)</p> <h3 id="a-heuristic-explanation">A heuristic explanation</h3> <p>A useful model of what‚Äôs happening is to turn back to the autoregressive state. In a nutshell, because Transformers have an explicit cache of all prior tokens, they have an <strong>inductive bias to pay attention to individual tokens</strong>.<d-footnote>To be precise, though, while the first layer sees individual tokens, token boundaries then get blurred in later layers as each sequence position can represent some combination of all tokens before it. This is meant to be an intuitive heuristic that I believe is useful, not a mechanistic explanation.</d-footnote> Or, perhaps more succinctly:</p> <blockquote> <p>The <strong>inductive bias</strong> of soft attention is <strong>hard attention</strong>.</p> </blockquote> <p>Here are some useful heuristics for when attention is naturally suited to the task:</p> <ul> <li>Does caching a representation for every ‚Äútoken‚Äù of data make sense?</li> <li>Does hard attention (focusing on or recalling an individual token) make sense?</li> </ul> <p>These questions point at the following idea: <strong>is each individual token semantically meaningful?</strong> For example, when reading language, we pay attention to units at the level of words (or subwords like prefixes/suffixes), which have <em>meaning</em>. But on the other hand, when this doesn‚Äôt hold ‚Äì for example, it‚Äôs rare that we‚Äôd ever want to pay attention to an individual <em>character</em> when reading ‚Äì the performance of attention suffers.<d-footnote>A slightly different explanation that some would propose is that attention simply gets confused by distractors in general, which is exacerbated when the data is too high-resolution, like at the character level. This explanation is also useful and I think actually points to the same underlying principle as mine.</d-footnote><d-footnote>On a related note, another researcher hypothesized that SSMs may be less prone to hallucination than Transformers; it hasn't been fleshed out, but if true would make sense from this intuition.</d-footnote></p> <p>What‚Äôs interesting is thinking about many other types of data which lie somewhere in between. For example, image patches can be quite meaningful when they capture some feature, but often can be useless or only partially meaningful.</p> <table> <thead> <tr> <th>Data</th> <th>Is a token ‚Äúmeaningful‚Äù?</th> </tr> </thead> <tbody> <tr> <td>Words / subword tokens</td> <td>:heavy_check_mark:</td> </tr> <tr> <td>Characters</td> <td>:x:</td> </tr> <tr> <td>DNA base-pairs</td> <td>:x:</td> </tr> <tr> <td>Image, video, audio patches</td> <td>:question:</td> </tr> <tr> <td>Time series datapoints</td> <td>:question:</td> </tr> </tbody> </table> <p>This is why I do think that attention is indispensable for data like tokenized language, which has largely been processed to a degree of meaning.<d-footnote>Many people will nitpick about whether BPE tokens represent any meaning. For sure they don't -- which is again a major reason I think tokenization needs to go. But to some approximation they do tend to find important repeated subwords like prefixes; and moreover there are a lot of hacks built-in, such as first segmenting on whitespace so that tokens can't cross word boundaries (which is very important to its performance -- another indicator of just how broken tokenization is). So in practice, LLM vocabularies tend to contain lots of actual words, which could be considered "meaningful".</d-footnote></p> <p>On the other hand, when the data is generally not meaningful (in the sense of requiring a model to pay attention to individual units), such as character-level language or DNA<d-footnote>I'm aware that sometimes you do need to pay attention to individual characters or base pairs, and that understanding the interactions of single base pairs is actually a big problem for machine learning on DNA. This heuristic is a deliberate oversimplification that I still think is generally useful.</d-footnote>, Transformers don‚Äôt work well, and other models like SSMs hold a clear edge. SSMs in particular, with their compressed states, may be particularly suited for these because when data appears at resolutions that are too high to be useful, what the model needs to do is <strong>compress the data into more meaningful abstractions</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/applications-480.webp 480w,/assets/img/2025-07-08-tradeoffs/applications-800.webp 800w,/assets/img/2025-07-08-tradeoffs/applications-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/applications.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Mamba applications in the first 6 months after its release.</figcaption> </figure> <p>The above figure, which was helpfully sent to me by the hosts of <a href="https://www.cognitiverevolution.ai/">The Cognitive Revolution</a> podcast, shows the breakdown of where Mamba was actually used after being published. Despite being motivated by and focusing on language modeling in the paper, the majority of its applications were actually in other modalities!<d-footnote>I don't work in computer vision, and part of me is unsure how much of Mamba's popularity there is just trend following üòú but I've been told, at least, that SSMs work pretty well!</d-footnote><d-footnote>Without giving away too much, I can also say that they are certainly <a href="https://cartesia.ai">very powerful for audio</a> if understood well and used correctly.</d-footnote> I think this is probably related to the above explanation: it‚Äôs very hard to find good ‚Äútokenizers‚Äù that provide meaning in data like time series, audio, and vision. And models that naturally compress, like SSMs, may have an advantage in inductive bias over Transformers.</p> <p>These heuristics are, of course, very unrefined, and I‚Äôm sure many researchers would take issue with this depiction. But I‚Äôve found it helpful for intuition and has been pretty good at predicting when various models are effective in practice.</p> <details><summary>Aside: Theories of tokenization</summary> <p>As people start thinking about tokenization more, there are some interesting theoretical results that have emerged which support this central thesis (that Transformers require meaningful tokens).</p> <ol> <li> <p><a href="https://arxiv.org/abs/2402.18376">Tokenization Is More Than Compression</a> examined the hypothesis that <em>the primary role of tokenization is to shrink the input sequence length</em>. They invented a new tokenizer that has even higher compression rates than BPE (actually, they even keep the same vocabulary, but simply find different segmentations that are more compressed) yet leads to worse language models, providing evidence against the hypothesis<d-cite key="schmidt2024tokenization"></d-cite>.</p> </li> <li> <p><a href="https://openreview.net/forum?id=wm9JZq7RCe">An Analysis of Tokenization: Transformers under Markov Data</a> showed that for certain data distributions, applying tokenization qualitatively changes what Transformers can learn. Intuitively, commonly used tokenizers like BPE and Unigram are somewhat based in information-theoretic heuristics, and play a particular role in smoothing out the non-uniform information rate of raw data into a form that‚Äôs more easily processed by a Transformer<d-cite key="rajaraman2024analysis"></d-cite>.</p> </li> </ol> </details> <details><summary>Aside: Do SSMs not need meaningful input?</summary> <p>Of course, working on more meaningful inputs would benefit all models, not just Transformers. But I hypothesize that Transformers particularly rely on it.</p> <p>In one of the iterations that I gave this talk, an audience member asked me the question of what I thought would happen if Transformers or SSMs were run on ‚Äú$n$-gram tokenized‚Äù language (instead of using BPE tokens, divide up the text into fixed windows of $n$ characters) or some other suboptimal tokenization.</p> <p>I predicted that both models would get worse on poorly segmented data, but it would affect SSMs less: in order of performance,</p> <p><code class="language-plaintext highlighter-rouge">Transformer (bad tokens) &lt; SSM (bad tokens) &lt; SSM (good tokens) &lt;= Transformer (good tokens)</code></p> <p>Byte/character-level modeling (equivalent to $n$=1) certainly provides some evidence for this.</p> </details> <h3 id="a-hypothetical-litmus-test">A hypothetical litmus test</h3> <p>Another thought experiment that‚Äôs intrigued me is what happens in the presence of noise. LLM data notoriously requires immense amounts of processing, filtering, and cleaning, but real-world data (and other modalities) aren‚Äôt like that. Humans also learn just fine from noisy data!</p> <p>So, what happens in a very simple scenario where information-less filler tokens are inserted into the sequence?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/thoughtexperiment-480.webp 480w,/assets/img/2025-07-08-tradeoffs/thoughtexperiment-800.webp 800w,/assets/img/2025-07-08-tradeoffs/thoughtexperiment-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/thoughtexperiment.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This figure illustrates a redundancy factor of $2\times$, but of course this can be arbitrarily increased to $k\times$ in the thought experiment. I think this shows another clear failure mode of standard attention: the compute shouldn‚Äôt be scaling as $k^2$, and the (inference) memory certainly shouldn‚Äôt scale in $k$ either ‚Äì caching the noise tokens is pointless.</p> <p>SSMs are much better: as $k$ increases, the memory doesn‚Äôt grow. But it actually doesn‚Äôt fully fix the problem either, as <em>any</em> standard architecture would have compute scaling with $k$ (since every token is processed by the entire model). And so all LLMs suffer from this sort of noise and redundancy.<d-footnote>More recent ideas like mixture-of-depths and other conditional compute approaches may make some progress here, but I think don't sufficiently address it yet and I'm guessing would be brittle.</d-footnote></p> <p>In fact, I think thought experiments like this provide useful litmus tests for what <strong>‚Äúthe right architecture‚Äù</strong> should look like. And I‚Äôll informally propose this one as a goal for the future of architecture design (maybe someone will help me formalize it in a paper someday?).</p> <blockquote> <h4 id="a-litmus-test">A Litmus Test</h4> <p>An ideal architecture should be able to process this sequence-with-fillers task <strong>without (substantially) increased compute or memory usage</strong>.</p> </blockquote> <p>More generally, suppose we have two copies of a dataset, one of which contains a lot of extra noise, but overall they have essentially the same ‚Äúinformation content‚Äù. We should expect ‚Äúthe right architecture‚Äù to behave essentially identically on both of these data sets.</p> <details><summary>Aside: Convolutions for language modeling</summary> <p>On a somewhat tangential note, I originally came up with the thought experiment in the figure above years ago, as a means to convince myself that convolutions don‚Äôt work for language modeling. When <a href="https://arxiv.org/abs/2111.00396">S4</a> was published, the community was excited about its potential on various modalities, and it spawned a wave of follow-up work on <a href="https://arxiv.org/abs/2302.10866">pure convolutional language models</a>.<d-cite key="gu2022efficiently"></d-cite><d-cite key="poli2023hyena"></d-cite></p> <p>But over the course of working on linear time-invariant SSMs, I quickly realized they were hopeless for language modeling. This example shows why: because language doesn‚Äôt have an intrinsic ‚Äúsampling rate‚Äù, tokens can be spaced somewhat arbitrarily. Clearly, even simple mis-spacings would drastically change what features a convolution could pick up on ‚Äì in the above example, the convolution could not possibly output the same feature on both of those input sequences, in contrast to input-dependent sequence mixing layers like attention or selective SSMs.</p> <p>On the other hand, convolutions exhibit strong inductive bias exactly when there‚Äôs a notion of sampling rate that spaces inputs out at a consistent rate. This is another way of phrasing the ‚Äúshift equivariant‚Äù inductive bias that makes them so great for (raw) perceptual modalities like vision and audio.</p> </details> <h3 id="is-attention-all-you-need-redux">Is attention all you need? (redux)</h3> <p>So through these discussions and examples, hopefully I‚Äôve made a case for my original claim, which I‚Äôll repeat here:</p> <blockquote> <p>Attention is most effective on<br/> <strong>pre-compressed data</strong> at the ‚Äú<strong><em><span style="color:red">right level of abstraction</span></em></strong>‚Äù</p> </blockquote> <p>This is, of course, an oversimplification of the picture ‚Äì and I wouldn‚Äôt even know how to try to formally define a ‚Äúlevel of abstraction‚Äù ‚Äì but I do believe this is true in some fuzzy sense.</p> <h2 id="the-tradeoffs-of-state-space-models-and-transformers">The Tradeoffs of State Space Models and Transformers</h2> <p>Let‚Äôs finally return to the main topic for this blog post.</p> <h3 id="state-space-models-1">State space models</h3> <p>The trade-offs of SSMs are pretty clear from thinking intuitively about its autoregressive state.</p> <blockquote class="block-tip"> <h4 id="the-strength">The Strength</h4> <p>SSMs are the natural <strong>stateful model</strong> with efficient, interactive, online processing.</p> </blockquote> <blockquote class="block-danger"> <h4 id="the-weakness">The Weakness</h4> <p>SSMs lack fine-grained <strong>recall and retrieval</strong> abilities.</p> </blockquote> <p>Both of these are two sides of the same coin ‚Äì consequences of its compressed state.</p> <p>I want to note, however, that I think there are strengths that are more subtle, and difficult to measure or even articulate.</p> <p>Going back to the [<a href="#a-coarse-analogy">brain analogy</a>], one question that intrigues me is whether <strong>compression is actually fundamental to intelligence</strong>.<d-footnote>My student Isaac has explored this hypothesis from a different angle: <a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html">[link]</a></d-footnote> Is it possible that forcing information into a smaller state forces a model to learn more useful patterns and abstractions? While compressed states are often viewed as a <a href="https://arxiv.org/abs/2402.01032">drawback</a> in the literature<d-cite key="jelassi2024repeat"></d-cite>, I think it might be because it‚Äôs very easy to measure these particular weaknesses but very hard to measure more subtle qualitative effects.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/bugfeature-480.webp 480w,/assets/img/2025-07-08-tradeoffs/bugfeature-800.webp 800w,/assets/img/2025-07-08-tradeoffs/bugfeature-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/bugfeature.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>At any rate, there are certainly many interesting applications where SSMs are the right tool for the job right now. And in my lab‚Äôs next release, we‚Äôll show another interesting and important use case (for language!) where the <em>compressive inductive bias of SSMs turns out to be essential</em>. Stay tuned!</p> <h3 id="transformers">Transformers</h3> <p>Transformers perform exceptionally well, and in fact are pretty much the only tool for the job, on tasks that require paying attention to individual tokens in the context.</p> <blockquote class="block-tip"> <h4 id="the-strength-1">The Strength</h4> <p>Transformers have <strong>perfect recall</strong> and <strong>fine-grained manipulation</strong> of individual tokens in their context.</p> </blockquote> <p>And what about the downsides? Everyone knows that the main weakness of Transformers is their quadratic complexity, right?</p> <p>Not exactly. The main theme of this post is to convey that Transformers <em>do have inductive biases</em> that gives them weaknesses in terms of modeling power, not just efficiency. And just as with SSMs, both the high-level strengths and weaknesses of Transformers are two sides of the same coin, consequences of the structure of their autoregressive state: the token cache <strong>maintains the granularity of the input resolution</strong> it‚Äôs given.</p> <blockquote class="block-danger"> <h4 id="the-weakness-1">The Weakness</h4> <p>Transformers are <strong><em>beholden</em></strong> to the <strong>tokens</strong> they are given.</p> </blockquote> <p>In other words, they are more sensitive to the <em>resolution</em> and <em>semantic content</em> of the data. Transformers are characterized by their context cache, which stores a separate representation for every element of the sequence, which means that every element better be useful.</p> <details><summary>Aside: What about efficient attention?</summary> <p>Many variants of attention exist, which have been primarily motivated by the efficiency angle. I think my framing gives us better intuition of how these variants might behave. For example, I hypothesize that the same weakness is present for any variant of attention that maintains an explicit token cache; in particular, for example, any type of sparse attention. The core weakness is still there (and perhaps even exacerbated in the case of sparse attention): the model is biased toward <em>attending</em> to individual tokens.</p> <p>On the other hand, some variants of efficient attention ‚Äúblur‚Äù the boundaries of tokens, including <a href="https://arxiv.org/abs/2006.04768">low-rank approximations</a><d-cite key="wang2020linformer"></d-cite> and any variant of linear attention. (More abstractly, these belong to a larger family of attention variants that make <em><a href="https://arxiv.org/abs/2405.21060">structured approximations</a></em> to the quadratic attention matrix<d-cite key="dao2024transformers"></d-cite>, any of which would have similar properties, I think.) Because of lacking a token-level cache, these models would not have the same weakness and would instead inherit properties much closer to SSMs.</p> <p>Incidentally, this is another more subtle reason why I somewhat prefer using ‚Äústate space model‚Äù or ‚Äúrecurrent model‚Äù as a descriptive term over ‚Äúlinear attention‚Äù. To me, the term ‚Äúattention‚Äù is <em>characterized</em> by maintaining a token-resolution state and having access to individual elements ‚Äì in other words, being able to <strong>pay attention</strong> to a single token.</p> </details> <h2 id="scaling-laws">Scaling Laws</h2> <p>To end, let‚Äôs talk about one of the major drivers of the current wave of progress in AI:<br/> <strong>scaling laws</strong>, or the phenomenon that spending more compute on models consistently leads to more capabilities.</p> <p>These laws are always plotted with FLOPs on the x-axis and some measure of performance on the y-axis, with the idea being that the slope of this line measures ‚Äúthe rate at which <strong>compute</strong> is converted into <strong>capabilities</strong>‚Äù. Indeed, I think there‚Äôs a popular viewpoint that Transformers are simply a vehicle that optimally performs this conversion.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/scaling-480.webp 480w,/assets/img/2025-07-08-tradeoffs/scaling-800.webp 800w,/assets/img/2025-07-08-tradeoffs/scaling-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/scaling.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And I think this is a great depiction of the goal of architecture research. We‚Äôre simply looking for <strong>the black box that performs this conversion in the best way possible</strong>. From this perspective, there is only one central question:</p> <blockquote> <p>Is my model using its compute wisely?</p> </blockquote> <p>In other words, we want every FLOP to count. And as is hopefully clear after this post (at least, I‚Äôve convinced myself!), Transformers are far from optimal.</p> <details><summary>Aside: Does it actually matter?</summary> <p>There‚Äôs another layer to the picture that I haven‚Äôt touched on, which is the practical efficiency of models. As <a href="https://tridao.me/">Tri</a> likes to frame it, what we actually care about is ‚Äúdollars-to-capabilities‚Äù, which can be factored into (1) ‚Äúdollars-to-FLOPs‚Äù and (2) ‚ÄúFLOPs-to-capabilities‚Äù. One might need to balance these two, for example, by accepting a suboptimal architecture for (2) in return for much more efficient (1). And some might say that Transformers have optimized the combination of these two.</p> <p>I still care primarily about question (2), partly because I personally find it more intellectually interesting, but also because I truly believe there are substantial advances to be made that change the balance even factoring in (1).</p> <p>A second higher-level question touching on whether it actually matters is: do we need to improve on anything to get to AGI/ASI? The answer here might be no ‚Äì tokenized Transformers may very well represent a viable path ‚Äì but I think that finding improvements may either get us there faster, or ultimately lead to more intelligent models.</p> </details> <p>Don‚Äôt get me wrong: despite being known as a leader of the Transformer-alternatives direction, I think Transformers are amazing and <em>attention is truly a fundamental modeling primitive</em>. But I also think it‚Äôs clear that they, by themselves, are not the final solution. We still have work to do.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/meme-480.webp 480w,/assets/img/2025-07-08-tradeoffs/meme-800.webp 800w,/assets/img/2025-07-08-tradeoffs/meme-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-08-tradeoffs/meme.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="whats-next">What‚Äôs next</h3> <p>Part of my reason for writing this post was to broadcast this content to a wider audience, and so this talk can be sunsetted :)</p> <p>But it‚Äôs also setting up for the next major architecture advancement‚Ä¶</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>Thanks to Tri Dao and Luca Periƒá for feedback on this post.</p> <h3 id="cite-this-post">Cite this post</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@online{gu2025tradeoffs,
  author  = {Albert Gu},
  title   = {On the Tradeoffs of State Space Models and Transformers},
  year    = {2025},
  url     = {https://goombalab.github.io/blog/2025/tradeoffs/},
}
</code></pre></div></div>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[(or - tokens are bs)]]></summary></entry><entry><title type="html">Understanding and Improving Length Generalization in Recurrent Models</title><link href="https://goombalab.github.io/blog/2025/improving-length-generalization/" rel="alternate" type="text/html" title="Understanding and Improving Length Generalization in Recurrent Models"/><published>2025-07-06T00:00:00+00:00</published><updated>2025-07-06T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2025/improving-length-generalization</id><content type="html" xml:base="https://goombalab.github.io/blog/2025/improving-length-generalization/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2507.02782">Paper</a>]</p> <h2 id="existing-recurrent-models-still-fall-short">Existing Recurrent Models Still Fall Short</h2> <div style="text-align: justify; margin-bottom: 1em;"> Linear recurrent models such as Mamba <d-cite key="mamba"></d-cite><d-cite key="mamba2"></d-cite> and linear attention <d-cite key="LA_katharopoulos2020transformers"></d-cite><d-cite key="RQKV"></d-cite><d-cite key="gated_linear_attention_yang2024gla"></d-cite><d-cite key="delta_net"></d-cite> possess <strong>a remarkable feature: they can process extremely long sequences</strong>, which is key for applications that require long context reasoning (like summarizing long texts or agents with long term memory). Indeed, this is their key advantage over their main competitor, the Transformers <d-cite key="attention_is_all_you_need"></d-cite>, which are bottlenecked by their finite context window and quadratic complexity over the sequence length. </div> <div style="text-align: justify; margin-bottom: 1em;"> Previously, the issue with recurrent models was their performance: on short sequences they were less capable than Transformers. But recent architecture breakthroughs have improved the performance of recurrent models and brought them on par with Transformers, to the point that they are currently used in several industry applications like audio modeling <d-cite key="goel2024sonic"></d-cite> or code completion <d-cite key="mistral2024codestral"></d-cite>. However, several recent works have found out that <em>recurrent models still fall short</em>: they might have comparable performance to Transformers, <strong> but in many cases they struggle to generalize past the training length.</strong> </div> <div style="text-align: justify; margin-bottom: 1em;"> Indeed, we show the performance of the official Mamba-2 checkpoints <d-cite key="mamba2"></d-cite> as a function of the sequence position $t$ (using perplexity, the lower the better). It can be seen that for positions $t$ beyond the training context $T=2048$, these models become virtually useless: they fail to <em>length generalize</em>. </div> <div style="max-width: 500px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2-480.webp 480w,/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2-800.webp 800w,/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/mamba2_poswise_reduced2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: justify; margin-bottom: 1em;"> This is an issue: existing recurrent models have low performance on long sequences, and are not much more efficient than Transformers in shorter sequences; so they seem to be falling short on both sides. </div> <div style="text-align: justify; margin-bottom: 1em;"> Does this mean that recurrent models are useless? Not at all! In our work, we show that <strong>length generalization is easily achievable in recurrent models through simple training interventions: post-training for 500 steps (~0.1% of the pre-training budget) enables length generalization in up to 256k sequences!</strong> Therefore, recurrent models possess an <em>unrealised potential</em> rather than a <em>fundamental limitation</em>. </div> <h2 id="why-do-recurrent-models-fail-to-length-generalize-the-unexplored-states-hypothesis">Why Do Recurrent Models Fail to Length Generalize? The <em>Unexplored States Hypothesis</em></h2> <div style="text-align: justify; margin-bottom: 1em;"> For an input sequence with $t$ elements $(x_1, x_2, ..., x_{t-1}, x_t)$, recurrent models compress the input context $(x_1, x_2, ..., x_{t-1})$ into a fixed-size <em>recurrent state</em> $h_{t-1}$. At time $t=0$, the state is initialized with some value $h_{-1}$, and then it is updated at each $t$ with an update function $f$: </div> <div style="text-align: center;"> $ h_t = f(h_{t-1}, x_t) $ </div> <div style="text-align: justify; margin-bottom: 1em;"> Similarly, the output at time $t$ only depends on the state $h_t$ and the current input $x_t$, i.e. for some other function $g$ the output $y_t$ can be written as </div> <div style="text-align: center;"> $y_t = g(h_t, x_t)$ </div> <div style="text-align: justify; margin-bottom: 1em;"> The functions $f$ and $g$ do not depend on the position $t$, so in theory recurrent models can naturally process any sequence length. But then, how can it be that they fail when $t$ is large? </div> <div style="text-align: justify; margin-bottom: 1em;"> In our work we show that <strong>the distribution of the state $h_t$ changes over time</strong>. Therefore, even if $g$ and $f$ work correctly up to some $T$, other $h_t$ with $t&gt;T$ might be significantly different, and thus the model fails to produce the correct output. Indeed, in the following figure we show how the norm of the state of Mamba-2 <d-cite key="mamba2"></d-cite> increases significantly over time: </div> <div style="max-width: 400px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/statemetrics_base-480.webp 480w,/assets/img/2025-07-06-length-generalization/statemetrics_base-800.webp 800w,/assets/img/2025-07-06-length-generalization/statemetrics_base-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/statemetrics_base.png" width="0.1" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: justify; margin-bottom: 1em;"> This explains why recurrent models fail to length generalize: when processing sequences longer than those seen during training, they encounter states $h_t$ that have not been explored during training, and thus they have not learnt to process them. Based on this insight, we propose the <strong>unexplored states hypothesis</strong> to explain the failure to length generalize: </div> <blockquote> <h4 id="unexplored-states-hypothesis">Unexplored States Hypothesis</h4> <div style="text-align: justify; margin-bottom: 1em;">Recurrent models fail to length generalize when they are trained only on a <strong>subset of all attainable state distributions</strong>&mdash;i.e. on a subset of the states that would be attained if the state recurrence was rolled out indefinitely. </div> <div style="text-align: justify; margin-bottom: 1em;"> When trained for long enough, the models <strong>overfit to this subset</strong> and perform poorly on long sequences because they <strong>encounter unexplored state distributions</strong>. </div> </blockquote> <h2 id="interventions-to-enable-length-generalization">Interventions to Enable Length Generalization</h2> <div style="text-align: justify; margin-bottom: 1em;"> The unexplored states hypothesis indicates that length generalization can be achieved not by changing the architecture or its mechanisms, but by training the model on a more diverse set of state distributions&mdash;in particular, on the distributions that arise when rolling out the state recurrence on long sequences. To do so, we could directly train the model on longer sequences, but this might not always be possible due to GPU memory constraints or due to lack of sufficiently long training sequences. </div> <blockquote class="block-tip"> <h4 id="the-recipe-to-achieve-length-generalization-interventions-on-the-initial-state">The recipe to achieve length generalization: interventions on the initial state</h4> <div style="text-align: justify; margin-bottom: 1em;"> Most modern architectures assume a zero initial state ($h_{-1}=0$). In our work, we consider four simple interventions on the <strong>initial state</strong> $h_{-1}$, which increase the diversity of states that the model explores during training without the need of training on longer sequences.</div> </blockquote> <div style="text-align: justify; margin-bottom: 0.5em;"> The four training interventions can be seen as sampling the initial state $h_{-1}$ from four different distributions that progressively get closer to the distribution of attainable states: </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 1. <strong>Random Noise</strong>: The state is initialized with an IID Gaussian with zero mean and a constant standard deviation (using the same mean / standard deviation for all layers and heads). </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 2. <strong>Fitted Noise</strong>: During training, we record the mean and standard deviation of the final states of the sequences across all layers and heads. Then, we initialize the state with an IID Gaussian distribution with mean and standard deviation fitted to the ones seen during training (using a different mean / standard deviation for each layer and head). </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 3. <strong>State Passing (SP)</strong><sup id="fnref1"><a href="#fn1">1</a></sup>: We use the final state of a previous (unrelated) sequence as the initial state. These final states are obtained by applying the state recurrence on a given sequence, <em>attaining</em> $h_T$ and using it as $h_{-1}$ for another sequence. This is similar to what happens at validation: the model doesn't stop at $T$, but rather keeps rolling the state and producing outputs from $h_T$. </div> <div style="text-align: justify; margin-bottom: 0.5em;"> 4. <strong>Truncated Backpropagation Through Time (TBTT)</strong> <d-cite key="TBTT_1990"></d-cite> <d-cite key="TBTT_sutskever"></d-cite>: In this case, we split a long sequence into smaller chunks, and use the final state of each chunk as the initial state of the next one. This is equivalent to processing the whole sequence, yet stopping the gradient propagation between chunks. </div> <details><summary>Difference between SP and TBTT</summary> <p>For simplicity, we implement SP by using the final state of the previous batch of sequences as the initial state of the new one. Thus, in practice the only difference between SP and TBTT is that TBTT requires carefully setting up the dataloader so that the sequences of the previous batch correspond to the prior parts of the sequences in the new batch.</p> </details> <div style="text-align: justify; margin-bottom: 1em;"> The following figures show the results of post-training the official Mamba-2 models for 500 steps (~0.1% of pre-training budget) with each intervention: </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/interventions_2-480.webp 480w,/assets/img/2025-07-06-length-generalization/interventions_2-800.webp 800w,/assets/img/2025-07-06-length-generalization/interventions_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/interventions_2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/interventions_1-480.webp 480w,/assets/img/2025-07-06-length-generalization/interventions_1-800.webp 800w,/assets/img/2025-07-06-length-generalization/interventions_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/interventions_1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="takeaway-1-sp-and-tbtt-enable-length-generalization">Takeaway #1: SP and TBTT enable length generalization</h3> <p>State Passing and TBTT ‚Äì which are the interventions that are closer to realistic states ‚Äì allow length generalization in sequences much longer than those seen during training. Thus:</p> <blockquote class="block-tip"> <h4 id="takeaway">Takeaway</h4> <div style="text-align: justify; margin-bottom: 0.5em;"> Length generalization is expected to be <strong>readily achievable in recurrent models</strong> through <strong>simple training interventions</strong>.</div> </blockquote> <p>Note that our results were achieved <em>with only ~0.02% of the original pre-training budget</em>!</p> <h3 id="takeaway-2-properties-of-the-state-of-recurrent-models">Takeaway #2: Properties of the state of recurrent models</h3> <blockquote class="block-tip"> <h4 id="takeaway-1">Takeaway</h4> <div style="text-align: justify; margin-bottom: 0.5em;"> We can infer properties of the <strong>distribution of the state</strong> of recurrent models by looking at the <strong>performance of the interventions</strong></div> <p>.</p> </blockquote> <div style="text-align: justify; margin-bottom: 1em;"> The Random Noise intervention fails to length generalize in the 370m, whereas Fitted Noise works. This suggests that for the 370m model the distribution of attainable states cannot be approximated with a Gaussian with fixed variance, but it can be approximated with an IID Gaussian with fitted variance in each layer and head of the state. However, the Fitted Noise intervention fails to achieve length generalization in the 1.3b model, indicating that the state of large models probably has complex dependency relationships among its elements and thus cannot be approximated with IID values. </div> <div style="text-align: justify; margin-bottom: 1em;"> Additionally, the interventions also fix the increasing state norm behavior we showed before, by making the model output states with similar norm at all timesteps: </div> <div style="max-width: 400px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/statemetrics_full-480.webp 480w,/assets/img/2025-07-06-length-generalization/statemetrics_full-800.webp 800w,/assets/img/2025-07-06-length-generalization/statemetrics_full-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/statemetrics_full.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <details><summary>SP in prior works</summary> <p id="fn1"> <sup>1</sup> Prior works have used the State Passing technique <d-cite key="longssm-wang2024longssmlengthextensionstatespace"></d-cite><d-cite key="end_to_end_bansal2022end"></d-cite>, yet it was applied to different recurrent architectures (e.g. time-invariant ones) or to tasks different to text modeling. To the best of our knowledge, we are the first to show that this technique used as a training intervention can greatly improve the length generalization of several recurrent models, and that it is as effective as TBTT in text modeling. <a href="#fnref1">‚Ü©</a> </p> </details> <h2 id="performance-on-long-context-tasks">Performance on Long Context Tasks</h2> <div style="text-align: justify; margin-bottom: 1em;"> We have seen that the interventions enable length <em>robustness</em> (i.e. not having decreased peformance after the training context $T$), but it is not clear whether they enable length <em>generalization</em> (i.e. solving tasks that require exploiting relationships between tokens that are separated by more than $T$ positions). One may wonder whether the interventions enable length robustness by simply preventing the model from reasoning beyond the training context length&mdash;similar to sliding window attention, which can't reason over tokens separated by more than the sliding window&mdash;in which case the models would have constant performance for all evaluation contexts $t &gt; T$, but could not solve tasks that require long context reasoning. In our work we show that <strong>the interventions do enable length generalization</strong> by showing results on three long context tasks. </div> <div style="text-align: justify; margin-bottom: 1em;"> <strong>BABILong</strong><d-cite key="babilong"></d-cite>. BABILong is a challenging benchmark which tests both the common sense understanding of a model as well as its ability to capture long range dependencies in text. In the figure below it can be observed that <strong>State Passing enhances the length generalization of the model in both the few-shot and finetuned settings</strong> (we recall that the model is trained and finetuned on sequences of length 2048). Therefore, State Passing is not only useful in fixing the diverging perplexity of established language models, but also in enhancing their ability to solve long context reasoning tasks. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/babilong-480.webp 480w,/assets/img/2025-07-06-length-generalization/babilong-800.webp 800w,/assets/img/2025-07-06-length-generalization/babilong-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/babilong.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div style="text-align: justify; margin-bottom: 1em;"> <strong>Passkey retrieval</strong><d-cite key="landmark_attention_mohtashami2023randomaccess"></d-cite>. The passkey retrieval task requires the model to retrieve a 5-digit passkey inserted at a given depth of a long context. In the figure below we show the performance of the Mamba-2 370m and 780m official checkpoints in three settings: zero shot, regular finetuning, and finetuning with fitted noise<sup id="fnref2"><a href="#fn2">2</a></sup>. The models finetuned with fitted noise are capable of exploiting relationships between tokens that are much more than 2048 positions apart (the training context length). In particular, <strong>the 780m model can solve the passkey perfectly for sequences of length 256k</strong>. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/passkey_2-480.webp 480w,/assets/img/2025-07-06-length-generalization/passkey_2-800.webp 800w,/assets/img/2025-07-06-length-generalization/passkey_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/passkey_2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <details><summary>Choice of intervention for passkey retrieval</summary> <p id="fn2"> <sup>2</sup> Contrary to typical language modeling datasets, the distribution of tokens in the passkey task is not stationary (in other words, there is not a well defined behavior for what the model should do after revealing the passkey). This is why we show results for the fitted noise intervention, as it does not require using the final state of a sequence (i.e., right after revealing the passkey), which might not be appropriate as the initial state.. <a href="#fnref2">‚Ü©</a> </p> </details> <div style="text-align: justify; margin-bottom: 1em;"> <strong>Synthetic Copying</strong><d-cite key="transformers-better-copying-pmlr-v235-jelassi24a"></d-cite>. The synthetic copying task consists in copying an arbitrary sequence of tokens. In the table below we show that using State Passing during training greatly improves the validation performance in sequences more than three times longer. Thus, <strong>state passing helps the model length generalize, solving long context tasks that are harder than those seen during training</strong>. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/synthetic_copying-480.webp 480w,/assets/img/2025-07-06-length-generalization/synthetic_copying-800.webp 800w,/assets/img/2025-07-06-length-generalization/synthetic_copying-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/synthetic_copying.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="a-deeper-look-into-how-recurrent-models-process-context">A Deeper Look into How Recurrent Models Process Context</h2> <div style="text-align: justify; margin-bottom: 1em;"> We have shown that the interventions on the initial state enable length robustness and allow solving long context tasks. On top of these findings, we now present a metric that sheds light on how sequence models process their context. </div> <div style="text-align: justify; margin-bottom: 1em;"> Ideally, in the case of text modeling we would like the model to pay attention to the recent context, and not focus too much on tokens that are too far away. But how can we quantify this behavior? We introduce <strong>Effective Remembrance</strong> to measure <strong>how much an autoregressive model is "effectively" remembering previous tokens</strong>. Denote by $q(\cdot \| \text{context})$ the probabilities that an autoregressive sequential model outputs for the next token given a context. Then, we define: </div> <div style="text-align: center;"> $ \text{EffRem}_T(t) = d(q(\cdot | x[0:T],q(\cdot | x[t:T])) $ </div> <div style="text-align: justify; margin-bottom: 1em;"> Where \( d(p,\bar{p}) \) is a distance between probability distributions (e.g., Total Variation). \(\text{EffRem}_T(t)\) roughly measures how much the model "effectively remembers" the tokens \( x[0:t-1] \) at time \( T \). If \( \text{EffRem}_T(t) = 0 \), this means that the predictions using \( x[t:T] \) and using \( x[0:T] \) are the same, meaning that <strong>the model does not "effectively remember" any of the past tokens \( x[0:t-1] \)</strong>. Conversely, if \( \text{EffRem}_T(t) \) is high, <strong>the model is substantially influenced by the tokens \( x[0:t-1] \)</strong>, since removing them from the context changes the prediction significantly. </div> <div style="text-align: justify; margin-bottom: 1em;"> The following figure shows $\text{EffRem}_T(t)$ for two official Mamba-2 checkpoints (<strong>which fail to length generalize</strong>) for varying $t$ and $T=8192$ (four times the training context): </div> <div style="max-width: 500px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/effrem_undesirable-480.webp 480w,/assets/img/2025-07-06-length-generalization/effrem_undesirable-800.webp 800w,/assets/img/2025-07-06-length-generalization/effrem_undesirable-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/effrem_undesirable.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div style="text-align: justify; margin-bottom: 1em;"> Intuitively we would expect that while every token contributes to the model‚Äôs output, the most recent tokens should have a significantly stronger influence. However, notice how the $\text{EffRem}$ curves immediately jump up and then gradually taper off. This behavior is clearly problematic: the next-token prediction at time $T=8192$ shouldn't change drastically depending on whether the model sees only the recent tokens \( x[4096:8192] \) or the full sequence \( x[0:8192] \). In natural language, the model should primarily rely on recent context, and earlier tokens \( x[0:4096] \) shouldn't completely alter the prediction&mdash;especially not to the extent that the total variation between the two output probability distributions approaches 1. This means that the model is disproportionately influenced by tokens at the beginning of the sequence. </div> <blockquote class="block-tip"> <h4 id="intuition">Intuition</h4> <div style="text-align: justify; margin-bottom: 1em;">We hypothesize that when a model is always trained with a zero initial state, it uses the <strong>first few tokens it sees</strong> to rapidly differentiate the state, which in turn causes <strong>overfitting to these tokens</strong>.</div> </blockquote> <h3 id="state-passing-fixes-effective-remembrance">State Passing fixes Effective Remembrance</h3> <p>After post-training with State Passing, the $\text{EffRem}$ curves show a gradual increase, indicating that the model places minimal weight on distant tokens and places progressively more weight on recent ones. In particular, tokens in the immediate context (e.g. the previous words in a sentence) have a critical impact on the next token predictions, which is the desired behavior in text modeling.</p> <div style="max-width: 500px; margin: 0 auto; text-align: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced-480.webp 480w,/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced-800.webp 800w,/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-07-06-length-generalization/mamba2-effrem-reduced.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <blockquote class="block-tip"> <h4 id="takeaway-2">Takeaway</h4> <div style="text-align: justify; margin-bottom: 1em;"> Through Effective Remembrance, we can check that <strong>State Passing helps the models prioritize recent context</strong> and not be needlessly disrupted by tokens that are far away in the past. </div> </blockquote> <h2 id="conclusion">Conclusion</h2> <div style="text-align: justify; margin-bottom: 1em;"> We have shown that <strong>length generalization is expected to be achievable in recurrent models</strong> through simple training interventions, without the need of changing the architecture nor the internal mechanisms of the model. Moreover, these interventions <strong>improve their performance on long context reasoning tasks</strong>, suggesting that existing recurrent models are not realising their full potential and can be easily improved. </div> <div style="text-align: justify; margin-bottom: 1em;"> Secondly, we believe that this work has significant implications for architecture research. For example, it has become very popular for modern recurrent architecture works to compare out-of-length extrapolation abilities <d-cite key="rwkv-v6-peng2024eaglefinchrwkvmatrixvalued"></d-cite><d-cite key="gated_delta_net_yang2024gateddeltanetworksimproving"></d-cite><d-cite key="beck2024xlstm"></d-cite>. In our work we show that <strong>simple training interventions substantially improve length generalization across several recurrent architectures</strong>, and thus research can focus mostly on the in-length performance (or if directly studying length generalization, it would be important to account for these interventions). </div> <div style="text-align: justify; margin-bottom: 1em;"> Lastly, <strong>we have proposed Effective Remembrance as a tool to understand how any autoregressive sequence model processes its context</strong>, thus making it easy to quantify how much models are "effectively remembering" parts of the context. </div>]]></content><author><name>Ricardo Buitrago Ruiz</name></author><summary type="html"><![CDATA[[Paper]]]></summary></entry><entry><title type="html">Cross-Architecture Distillation Part I - The MOHAWK Framework</title><link href="https://goombalab.github.io/blog/2024/distillation-part1-mohawk/" rel="alternate" type="text/html" title="Cross-Architecture Distillation Part I - The MOHAWK Framework"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/distillation-part1-mohawk</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/distillation-part1-mohawk/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2408.10189">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba">Code</a>]</p> <ol> <li>Part I - MOHAWK</li> <li><a href="/blog/2024/distillation-part2-phi-mamba/">Part II - Phi-Mamba</a></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>We start off by summarizing some important aspects from <d-cite key="ssd"></d-cite>, specifically the sequence transformation/mixer viewpoint and the Mamba-2 SSM variant.</p> <p><strong>Definition:</strong> A <em>sequence transformation/mixer</em> refers to a parameterized map on sequences $Y = f_{\theta}(X)$ where $X, Y \in \mathbb{R}^{(T, P)}$ and $\theta$ is an arbitrary collection of parameters. $T$ represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_t, Y_t \in \mathbb{R}^P$.</p> <p>In layman‚Äôs terms, <em>sequence mixers</em> aggregate tokens across various time steps. This ability to learn temporal interactions and information forms the foundation of modern deep sequence models, like Transformers.</p> <p><strong>Definition:</strong> <em>Matrix mixers</em> are a specific type of sequence mixers that can be represented as $Y = MX$ for matrix $M \in \mathbb{R}^{(T,T)}$.</p> <p>Examples of <em>matrix mixers</em> which fall under this definition include vanilla self-attention, where $M = \text{Softmax}(\mathbf{Q}\mathbf{K}^\top)$ <d-cite key="vaswani2023attention"></d-cite>, linear attention <d-cite key="katharopoulos2020transformers"></d-cite>, and Toeplitz matrices <d-cite key="qin2023toeplitz"></d-cite>.</p> <h3 id="mamba-2">Mamba-2</h3> <p>Mamba-2 <d-cite key="ssd"></d-cite> is a recent variant of Structured State Space Models (SSMs) <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> which can be viewed as a matrix mixer that can be applied onto an input sequence in subquadratic time due to structured matrix multiplication. Mamba-2 is a time-varying SSM, defined as</p> \[\begin{aligned} h_{t+1} &amp;= A_t h_t + B_t x_t \\ y_t &amp;= C_t h_t \end{aligned}\] <p>where $B_t$ and $C_t$, like in Mamba-1 <d-cite key="gu2023mamba"></d-cite>, are input-dependent projections, but $A_t$ is the identity matrix $I$ multiplied by a scalar $\alpha_t$. Importantly, Mamba-2 identified the <em>Structured State Space Duality (SSD)</em> connection which found that specific variants of SSMs can be viewed as a form of causal linear attention <d-cite key="katharopoulos2020transformers"></d-cite>.</p> <p>Formally, the Mamba-2 SSD matrix mixer can be represented as</p> \[\begin{equation} \label{eq:ssd-matrix-mixer} \begin{aligned} \begin{bmatrix} \alpha_{1} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{2:1} &amp; \alpha_{2} &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{3:1} &amp; \alpha_{3:2} &amp; \alpha_{3} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \alpha_{n:1} &amp; \alpha_{n:2} &amp; \alpha_{n:3} &amp; \cdots &amp; \alpha_{n} \end{bmatrix} \circ (C \cdot B^\top) \cdot X \end{aligned} \end{equation}\] <p>where $\alpha_{t:i} = \alpha_{t-1} \cdot \alpha_{t-2} \cdots \alpha_{i}$.</p> <p>From this representation, one can see that Mamba-2 can be viewed as causal linear attention with a learnable causal mask!</p> <h2 id="mohawk-method">MOHAWK Method</h2> <p>Inspired by the <em>matrix mixer</em> viewpoint which provides a common lense for viewing the key components of various architectures, we introduce the <strong>MOHAWK</strong> framework for cross-architectural distillation, which is composed of three stages:</p> <ol> <li><strong>M</strong>atrix <strong>O</strong>rientation</li> <li><strong>H</strong>idden-State <strong>A</strong>lignment</li> <li><strong>W</strong>eight-Transfer and <strong>K</strong>nowledge Distillation</li> </ol> <p>These three sequential stages distill the student model from the bottom up, steadily increasing the number of components distilled into at each stage until the end student model has been distilled end-to-end. We find that this multi-stage process is much more effective than traditional knowledge distillation.</p> <p>Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba ‚Äìbased on the Phi-1.5 model <d-cite key="gunasekar2023textbooks"></d-cite>‚Äì along with the specifics of its distillation process.</p> <p>For clarity, we refer to the term <em>block</em> as a repeating component that forms the backbone of the end-to-end model. <em>Blocks</em> are composed of layers, for instance the Llama block is composed of a self-attention layer followed by a MLP layer. <em>Layers</em> can be composed of numerous subcomponents, like the self-attention layer, which encompasses the projections and the self-attention mechanism, and the Mamba layer, which includes the projections, convolution, and SSM mixer, etc.</p> <h3 id="stage-1-matrix-orientation">Stage 1: Matrix Orientation</h3> <p>We begin the first stage of MOHAWK by matching the matrix mixer of both the student and teacher. Prior to directly aligning the matrix mixers themselves, we first adjust the <em>matrix mixer layer</em> to be analogous to that of the teacher‚Äôs, i.e., structurally both layers are the same except the matrix mixer component. We then minimize the distance between the matrix mixer of the teacher and student layers, which can be expressed as the following equation:</p> <p>\(\begin{equation} \label{eq:matrix-orientation-minimization} \min_{\mathbf{\phi}} \|\mathrm{TeacherMixer}(\mathbf{u}) - \mathrm{StudentMixer}_{\boldsymbol{\phi}}(\mathbf{u})\|_F \end{equation}\) where $\phi$ represents the parameters in the layer and $\mathbf{u}$ is the shared input derived from the teacher model. The stage ensures that the student can closely approximate the teacher‚Äôs matrix mixer layer which sets a strong foundation for teacher matching in subsequent stages of the MOHAWK process.</p> <p>For Phi-Mamba: Because the student model uses the Mamba-2 mixer, we initialize the convolution to identity and discarded the nonlinear activation after the convolution to ensure the components upstream of the matrix mixers roughly equivalent to the self-attention layer. The loss calculate was between the self-attention matrix of the teacher and the ‚Äúunraveled‚Äù SSM matrix as shown in Equation \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="stage-2-hidden-state-alignment">Stage 2: Hidden-State Alignment</h3> <p>After optimizing Equation \eqref{eq:matrix-orientation-minimization} in Stage 1, Stage 2 proceeds to match the outputs of the student and teacher blocks.</p> <p>\(\begin{equation} \label{eq:hidden-state-minimization} \min_{\mathbf{\phi}} \|\mathrm{AttnBlock}(\mathbf{u}) - \mathrm{StudentMixerBlock}_{\boldsymbol{\phi}}(\mathbf{u})\|_2 \end{equation}\) where once again the inputs are the same. Like Stage 1, Stage 2 can be run in parallel. We find that the distance between the layer outputs is strongly correlated with the student model‚Äôs ability to recover the teacher model‚Äôs knowledge.</p> <p>For Phi-Mamba: To keep the block architectures as similar as possible, we initialized the Mamba-2 gate to be a value of 1 to simulate Phi‚Äôs lack of gating and removed the norm prior to the output projection.</p> <h3 id="stage-3-weight-transfer-and-knowledge-distillation">Stage 3: Weight-Transfer and Knowledge Distillation</h3> <p>The final stage aims to fine-tune the entire student model to match the performance of the teacher. This stage is critical for mending the potential discrepancies between post-Stage 2 blocks. We also initialize information dense components of the student model, in particular the MLPs, embedding, and LM head, before fine-tuning the student end-to-end. Given the weight transfer of critical architectural components, the overall block structure of the student mirror that of the teacher model, e.g., our student model has the MLPs and matrix mixer layers in parallel. Finally, we use knowledge distillation loss <d-cite key="hinton2015distilling"></d-cite> to encourage the student to imitate the teacher‚Äôs distribution:</p> \[\begin{equation} \min_{\mathbf{\phi}} \mathbf{\mathcal{L}}_{\mathrm{CE}}\big(\mathrm{Teacher}(\mathbf{x}), \mathrm{Student}_{\boldsymbol{\phi}} (\mathbf{x})\big) \end{equation}\] <p>For Phi-Mamba: We create a new Phi-Mamba block that has the same parallel MLP-matrix mixer layer structure as the original Phi-1.5 block. We copy over the MLP and norm weights, token embeddings, and language model head and pre-head norm as it has been hypothesized that much of a model‚Äôs information is stored in these components. We also find that the MLPs can be frozen after the transfer with only a slight decrease in performance but reduce the number of trainable parameters by more than half!</p> <h2 id="approximating-self-attention">Approximating Self-Attention</h2> <p>With the MOHAWK method we can now distill from any quadratic self-attention model to any model that utilizes a <em>matrix mixer</em> for sequential modeling. But, a caveat is that the performance of the student model is inherently constrained by the expressivity of its matrix mixer. So why did we decide to use the Mamba-2 mixer instead of an alternative like linear attention or gated convolution? In this next section, we will empirically explore Mamba-2‚Äôs ability to approximate the self-attention matrix $\text{Softmax}(QK^\top)$ and compare it to some other popular sub-quadratic matrix mixer families. We describe a couple of them below.</p> <h3 id="linear-attention-and-ssd">Linear Attention and SSD</h3> <p>When describing linear attention matrices, we can utilize the fact that both $Q$ and $K$ are token-dependent projections of some input $x \in \mathbb{R}^{d_{in}}$ onto $\mathbb{R}^{d_{out}}$, and therefore the rank of $Q$ and $K$ are bounded by $\min{ { d_{in}, d_{out} } }$ For multi-head linear attention, $d_{out}$, which corresponds to the head dimension, is typically a small value (e.g., $64$ and $128$ for Phi-1.5 and Llama2-7b-Chat respectively). Thus, we approximate linear attention matrix mixers using causal low-rank matrices $\mathbf{L \circ Q K}^\top$, where $\mathbf{L}$ is a lower-triangular causal mask of 1s, and $\mathbf{Q}$, $\mathbf{K}$ are in $\mathbb{R}^{n \times d}$ with $d \ll n$.</p> <p>For the multi-head Mamba-2 matrix family, we utilize the state space dual (SSD) layer in a manner similar to the previous linear attention class, but imbuing the causal matrix $\mathbf{L}$ with an $n$-degree rolling multiplicative structure for $\mathrm{SSD}$. This can be seen as a more expressive mask that generalizes the lower-triangular, ones-only causal mask \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="general-semi-separable-and-toeplitz">General Semi-separable and Toeplitz</h3> <p>To approximate the general class of semi-separable matrices (abbreviated as ‚ÄúSSM‚Äù in the following table), we utilize <em>balanced truncation</em>. This method is used in the field of time-invariant Dynamical System model reduction <d-cite key="BTSurvery"></d-cite> and has been modified for use in time-varying systems <d-cite key="TVBTSurvery"></d-cite>. Similarly, for the family of Toeplitz matrices, which represent a convolution operation, we apply a causal mask, the same one used for causal low-rank matrices, on top a Toeplitz matrix.</p> <h3 id="empirical-approximation">Empirical Approximation</h3> <p>To empirically validate the expressiveness of the four aforementioned families, we sample 1,000 attention matrices, each consisting of 512 tokens, from the Llama2-7B-Chat <d-cite key="touvron2023llama"></d-cite> model on four different datasets. One attention head, and its respective attention matrix, from each layer was chosen at random. Both (causal) low-rank (LR) and SSD matrix families were approximated with 10,000 steps of gradient descent per sample. SSM and Toeplitz were both calculated without using gradient descent using balanced truncation and a simple heuristic respectively. We calculate the Frobenius distance between each ‚Äúground truth‚Äù self-attention matrix and the approximated matrix of each family.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-480.webp 480w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-800.webp 800w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx.png" width="100%" height="auto" title="Matrix Approximation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Self Attention matrix approximation by structured matrix mixers</figcaption> </figure> <p>Given the previous table‚Äôs experiment was conducted in a very controlled setting, we further explore the ability of the various families‚Äô abilities to approximate the self-attention matrix within a language model. We replace the self-attention matrix mixers of a Phi-1.5 model with either input-dependent Toeplitz, causal low-rank, or SSD (our Mamba-2 variant) matrix mixers, and ran the second and third stages of our MOHAWK procedure for 1B tokens each.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl.png" width="100%" height="auto" title="Matrix Structure Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Evaluations of various structured matrices on downstream tasks</figcaption> </figure> <p>We find that there is a constant correlation between the self attention approximation abilities (measured via projection distances) of a matrix family and the downstream performance metrics (accuracy) of the matrix mixer integrated into an end-to-end language model. This finding that more expressive matrix mixers lead to more effective models is echoed in <d-cite key="hwang2024hydrabidirectionalstatespace"></d-cite>.</p> <h2 id="next-up">Next Up</h2> <p>The <a href="/blog/2024/distillation-part2-phi-mamba/">following section</a> will cover MOHAWK in action, distilling our final Phi-Mamba and Hybrid-Phi-Mamba models, and explore the training laws regarding each stage of MOHAWK.</p>]]></content><author><name>Aviv Bick*</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws</title><link href="https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba/" rel="alternate" type="text/html" title="Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2408.10189">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba">Code</a>]</p> <ol> <li><a href="/blog/2024/distillation-part1-mohawk/">Part I - MOHAWK</a></li> <li>Part II - Phi-Mamba</li> </ol> <p>In <a href="/blog/2024/distillation-part1-mohawk/">Part I</a> of this series, we covered important terminology, the Mamba-2 architecture, and the MOHAWK architecture. We also demonstrated Mamba-2‚Äôs ability to match the self-attention matrix of Transformers, which influenced our choice to use it as the student model for validating our MOHAWK method.</p> <p>In this section, we will explore the training laws regarding each of the three stages of MOHAWK and empirically validate the importance of all stages. We use the cumulative insights gained to then distill a <strong>fully subquadratic Mamba model using only 3B tokens</strong> - less than 1% of many of the other models‚Äô token budget - while being <strong>competitive with many of the current state-of-the-art open-source subquadratic models</strong>! We also distill a strong Mamba-Attention hybrid.</p> <h2 id="final-results">Final Results</h2> <p>We empirically validate the MOHAWK framework by distilling the pretrained Phi-1.5 model into a 1.5B Mamba variant, dubbed Phi-Mamba. Our final model was distilled with <strong>only 3B tokens</strong>, with a 80M/160M/2.76B token split among Stage 1/2/3, from the C4 dataset with a context length of 2048. The choices for these token splits were influenced by our verification of the importance of all three stages and training laws that determined, given a fixed token budget, how to allocate resources, which we detail in the following sections.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance.png" width="100%" height="auto" title="Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance of Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <h2 id="importance-of-each-mohawk-stage">Importance of Each MOHAWK Stage</h2> <p>A brief recap of the three stages of MOHAWK are</p> <p>1) <strong>Matrix Orientation</strong>: matches the matrix mixer of each respective block.</p> <p>2) <strong>Hidden-State Alignment</strong>: independently compares the block output given the same input across all layers of the student model.</p> <p>3) <strong>Weight-Transfer and Knowledge Distillation</strong>: performs knowledge distillation of logits from teacher to student and copies over crucial weights from the teacher model.</p> <p><strong>Each stage plays a crucial role</strong> as shown in our ablations below. All the runs were performed with a fixed total token count.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl.png" width="100%" height="auto" title="MOHAWK Ablations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Effects of various MOHAWK stage ablations on downstream performance</figcaption> </figure> <p>As expected, Stage 3‚Äôs end-to-end alignment is important as the <strong>previous stages only match the block outputs</strong>, leaving the blocks disjoint if the hidden state cannot be completely matched, as shown with both the Phi-Mamba and Hybrid-Phi-Mamba trained on Stage 3 outperform their counterparts trained with Stage 2. Of course, student models that have more mixing layers similar to the teacher may see a diminished impact of Stage 3 as the layers may be aligned more with only Stage 2.</p> <p>The addition of a Stage 2 initialization provides additional synergy, <strong>boosting performance significantly compared to Stage 3 only</strong>. We also note that the effects of adding Stage 2 is more pronounced in cases where the student architecture is less similar to the teacher architecture, e.g., the improvement for Phi-Mamba which has zero attention layers is larger than Hybrid-Phi-Mamba which has four.</p> <p>Stage 1 also provides a good in downstream performance. For example, only with the addition of Stage 1 on top of Stage 2 and 3 can a Phi-to-Phi distillation <strong>recover the original teacher Phi‚Äôs overall performance</strong>. And, we see in the two other architectures that performance gains can also be observed.</p> <h2 id="training-laws-for-mohawk">Training Laws for MOHAWK</h2> <p>We aimed to evaluate the impact the preceding stage had on the current stage‚Äôs performance.</p> <p>For the Stage 2 + 3 pair, we trained Phi-Mamba instances from scratch using Stage 2 to various checkpoints. These checkpoints were then used to initialize Phi-Mamba instances that were trained using Stage 3 to different total budgets. The figure below shows that given an adequate training budget, <strong>student models initialized from Stage 2 outperform students trained only with Stage 3</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage23-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage23.png" width="100%" height="auto" title="Stage 2 + 3 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Training Laws for Stage 2 and 3 of MOHAWK</figcaption> </figure> <p>Given the previous finding, we then analyze how matrix mixer matching (Stage 1) can impact the student‚Äôs ability to match the overall mixer block with the teacher (Stage 2). Similar to before, we train numerous Phi-Mamba models using Stage 1 and use them as initializations for Stage 2 and compare them against each other and also a Stage 2 only model. Here, we find that <strong>even a small budget allocated to Stage 1 can help the subsequent stage</strong> perform better than random initialization.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage12-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage12.png" width="100%" height="auto" title="Stage 1 + 2 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Training Laws for Stage 1 and 2 of MOHAWK</figcaption> </figure> <h3 id="training-the-final-phi-mamba-model">Training the Final Phi-Mamba Model</h3> <p>Using the insights gained in the training laws above, we finalized our training regime given a fixed budget of 3B tokens. Stage 1 was allocated 80M due to the strong performance on matrix distance and hidden state distance. Stage 2 was trained for 160M tokens given the seeming saturation of both hidden state distance and perplexity when compared to the other initialization states, e.g., 10M, 20M, 40M, etc. We train Stage 3 to reach 3B tokens in total, but reduced the learning rate of the last stage to alleviate training instabilities. We hypothesize that this is due to the Stage 1 + 2 initialization‚Äôs Mamba component being quite similar to that of the teacher model, so a large learning rate coupled with disconnect between blocks, which are addressed in Stage 3, can cause training instabilities.</p> <h2 id="hybrid-phi-mamba-model">Hybrid Phi-Mamba Model</h2> <p>There has been a growing body of work that combines both Attention and SSM mechanisms, leading to improved performance over either one used by itself <d-cite key="Samba"></d-cite><d-cite key="jamba2024"></d-cite><d-cite key="MambaVision"></d-cite>. Although incorporating Attention layers does make the model quadratic, limiting their number allows us to mitigate the efficiency drawbacks while increasing expressivity and performance!</p> <p>Thus, we distill the Phi-1.5 model into a Mamba-Attention hybrid model that maintains only four quadratic Attention layers. The remaining layers use the Mamba-2 layer variant also used in our Phi-Mamba model. Trained with 5B tokens using the MOHAWK method, our model achieves an average score of $66.0$ on downstream metrics, <strong>outperforming Phi-Mamba</strong>‚Äôs $65.1$ and <strong>approaching Phi-1.5</strong>‚Äôs $67.2$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance.png" width="100%" height="auto" title="Hybrid-Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance of Hybrid-Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <p>Our Hybrid-Phi-Mamba model is <strong>performs comparably</strong> to strong Attention-Mamba hybrids at the 1.5B range <strong>while using less Attention layers</strong> than Samba (12) and Mamba-SWA-MLP (18). We find that interleaving the Attention layers with the Mamba layers resulted in the strongest model, an observation that was also seen in <d-cite key="Samba"></d-cite>. We also find that increasing the number of Attention layers improved performance.</p>]]></content><author><name>Aviv Bick*</name></author><summary type="html"><![CDATA[[Paper] [Code]]]></summary></entry><entry><title type="html">Hydra Part I - Matrix Mixer Framework</title><link href="https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer/" rel="alternate" type="text/html" title="Hydra Part I - Matrix Mixer Framework"/><published>2024-07-16T00:00:00+00:00</published><updated>2024-07-16T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer/"><![CDATA[<p>[<a href="https://arxiv.org/abs/2407.09941">Paper</a>] [<a href="https://github.com/goombalab/hydra">Code</a>]</p> <ol> <li>Part I - Matrix Mixer Framework</li> <li><a href="/blog/2024/hydra-part2-model/">Part II - Hydra: The Model</a></li> </ol> <p>Attention mechanisms<d-footnote>In this work, Attention<d-cite key="attention"></d-cite> exclusively refers to Self-Attention<d-cite key="transformer"></d-cite></d-footnote> have taken center stage in the world of sequence mixing, celebrated for their significant flexibility and performance. However, this power comes with a price: high computational and memory demands. Despite these challenges, attention has become the go-to solution for many applications.</p> <p>In modern state-of-the-art models, architectural designs typically split into two main components: the sequence mixer and the channel mixer. To illustrate, let‚Äôs look at the Transformer encoder architecture. It consists of two key elements: Multi-Head Attention and a Feed-Forward Network (FFN). The Multi-Head Attention serves as the sequence mixer, efficiently managing interactions across the input sequence. Meanwhile, the FFN acts as the channel mixer, processing information within each sequence element.</p> <p>Take a glance at the figure below to see this architecture in action. You‚Äôll notice how these components work together to create the robust models we rely on today.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_mixer_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/matrix_mixer_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In our work, we study the large and important class of sequence mixers that can be represented as basic matrix multiplications: $\textbf{Y} = \textbf{M}\textbf{X}$. We call this approach <strong><em>the matrix mixer framework</em></strong>. This framework includes diverse and important classes of sequence models such as Attention, convolutions<d-cite key="ckconv"></d-cite><d-cite key="tnn"></d-cite>, and state-space models<d-cite key="s4"></d-cite><d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite>. For example, the typical self-attention mechanism, $\textbf{Y} = \text{softmax}(\textbf{Q}\textbf{K}^T)\textbf{V}$, can be seen as a special case where the matrix $\textbf{M}$ is defined as $\text{softmax}(\textbf{Q}\textbf{K}^T)$.</p> <p>Viewing sequence mixers through this lens has a significant advantage: designing new sequence mixers becomes a matter of finding the optimal matrix $\textbf{M}$. This perspective opens up a systematic way to explore and innovate in the field of sequence modeling.</p> <p>So, now the question is, what is a good $\textbf{M}$? Key desiderata for such a matrix would include:</p> <ul> <li>Efficiency: We want sub-quadratic matrix multiplication and parameterization to ensure our models run swiftly and handle long sequences with ease.</li> <li>Performance: The matrix mixer should match the high standards of Attention mechanisms in modeling diverse sequence data across various modalities.</li> <li>Flexibility: The solution should work well with sequences of different lengths (+ capable of both causal and bidirectional sequence modeling, which we will tackle in <a href="/blog/2024/hydra-part2-model/">Part II</a>)</li> </ul> <p>Check out the table below to see how various sequence mixers measure up. While several models like MLP-Mixer<d-cite key="mlpmixer"></d-cite>, FNet<d-cite key="fnet"></d-cite>, TNN<d-cite key="tnn"></d-cite>, LA<d-cite key="la"></d-cite>, and M2<d-cite key="m2"></d-cite> have been introduced, none of them fully meet all our criteria.</p> <table> <thead> <tr> <th>¬†</th> <th>Sub-quadratic</th> <th>Performance</th> <th>Flexibility</th> </tr> </thead> <tbody> <tr> <td>MLP-Mixer</td> <td>üò≠</td> <td>üò≠</td> <td>üò≠</td> </tr> <tr> <td>FNet</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>TNN</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>LA</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>M2</td> <td>ü§ó</td> <td>üò≠</td> <td>üò≠</td> </tr> <tr> <td>Transformer</td> <td>üò≠</td> <td>ü§ó</td> <td>ü§ó</td> </tr> </tbody> </table> <p>As you can see, each of these models has its strengths and weaknesses, but none perfectly hit all the marks. This gap highlights the need for another approach in developing sequence mixers.</p> <blockquote> <p><strong>So, is it even possible to meet all three key criteria?</strong></p> </blockquote> <p>We believe the answer lies in examining <strong><em>the structures</em></strong> of the mixer matrix $\textbf{M}$. Our work begins with an in-depth theoretical and empirical analysis of various sequence mixers using the matrix mixer framework. We then extend this idea, offering a systematic approach to designing new sequence mixers. By fully leveraging this framework, we have developed <strong>multiple</strong> novel architectures, including a new bidirectional mixer named <strong><em>Hydra</em></strong>.</p> <p>Let‚Äôs dive into more details, which is outlined as follows:</p> <ul> <li>We study and formalize the matrix mixer framework, introducing new theoretical concepts about structures of $\textbf{M}$ that can capture such desiderata.</li> <li>Guided by the properties of different matrix classes, we introduce a series of sequence models with strong and predictable performance.</li> <li>We provide careful systematic studies on these matrix classes, comparing empirical performances by varying only the matrix mixer</li> </ul> <h2 id="formalization-of-the-matrix-mixer-framework">Formalization of the Matrix Mixer Framework</h2> <p>We begin by further formalizing our matrix mixer framework. While this framework can be applied to multi-head architectures, we will focus on the single-headed scenario here for simplicity.</p> <p>In essence, a sequence mixer transforms an input $\textbf{X} \in \mathbb{R}^{L \times C}$ into an output $\textbf{Y} \in \mathbb{R}^{L \times C}$, where $L$ is the sequence length and $C$ is the number of channels.</p> <ol> <li>Input preprocessing function: Denoted as $f_X \colon \mathbb{R}^{L \times C} \rightarrow \mathbb{R}^{L \times D}$, this function handles common data transformations before the mixing process.</li> <li>Matrix construction function: Denoted as $f_{\mathcal{M}} \colon \mathbb{R}^{L \times C} \times \Theta \rightarrow \mathcal{M} \subseteq \mathbb{R}^{L \times L}$, this function maps input data to mixer matrices. Here, $\Theta$ represents the space of learnable parameters, and $\mathcal{M}$ represents the class of mixer matrices.</li> </ol> <p>Given these functions, we denote the mixer matrix as $\textbf{M} = f_{\mathcal{M}}(\textbf{X}, \theta)$. The matrix mixer framework is then defined by the equation: \(\textbf{Y} = \textbf{M} (f_X(\textbf{X})).\)</p> <p>Using this framework, we are now playing a game of finding the optimal $\textbf{M}$ that satisfies all three requirements: efficiency, performance, and flexibility! This systematic approach allows us to analyze the characteristics of different sequence mixers and formalize the properties needed to meet our criteria.</p> <p>Let‚Äôs break down these objectives step-by-step and explore which matrices work best in achieving them.</p> <h2 id="solution-for-sub-quadratic-complexity-structured-matrices">Solution for Sub-quadratic Complexity: Structured Matrices</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_classes_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_classes_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_classes_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/matrix_classes_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To meet our first key requirement ‚Äì sub-quadratic matrix multiplication ‚Äì we can focus on a special type of matrices known as <strong>structured matrices</strong>. For a general matrix $\textbf{M}$, matrix multiplication typically incurs a computational cost of $O(L^2)$. However, structured matrices, with their compressed representation, allow us to perform these operations much more efficiently, achieving sub-quadratic complexity. We refer to sequence mixers using these matrices as <strong><em>structured matrix mixers</em></strong>.</p> <p>Structured matrices provide a broad array of options for our matrix mixer $\mathcal{M}$, as illustrated in the figure above. By leveraging these matrices, we can significantly reduce computational overhead while maintaining an efficient parameter count.</p> <p>All previous versions of sub-quadratic sequence mixers fit within the matrix mixer framework. This categorization by the class of mixer matrices helps us systematically analyze and understand the strengths and weaknesses of different approaches.</p> <details><summary>Notations</summary> <p>Think of bold capital letters like $\textbf{X}$ as matrices, bold small letters like $\textbf{x}$ as vectors, and regular small letters like $x$ as scalars. When we talk about elements in a matrix, we‚Äôll use subscripts. So, if we have a matrix $\textbf{X} \in \mathbb{R}^{M \times N}$, the element in the $i$-th row and $j$-th column is $x_{ij}$. If we‚Äôre looking at the whole $i$-th row, it‚Äôs $\textbf{x}_i$.</p> </details> <table> <thead> <tr> <th>Matrix Structure $\mathcal{M}$</th> <th>Formulation (\(ùëö_{ij}\))</th> <th>Complexity</th> <th>Method Instantiations</th> </tr> </thead> <tbody> <tr> <td>Dense</td> <td>$m_{ij}$</td> <td>$O(L^2)$</td> <td>MLP-Mixer<d-cite key="mlpmixer"></d-cite></td> </tr> <tr> <td>Dense (Softmax Attention)</td> <td>$\text{softmax}_j(q^T_i k_j)$</td> <td>$O(L^2)$</td> <td>Transformer<d-cite key="transformer"></d-cite></td> </tr> <tr> <td>Low-rank (Linear Attention)</td> <td>$q^T_i k_j$</td> <td>$O(L)$</td> <td>Linear Attention<d-cite key="la"></d-cite>, Linformer<d-cite key="linformer"></d-cite></td> </tr> <tr> <td>Butterfly</td> <td>Refer to <d-cite key="kaleidoscope"></d-cite><d-cite key="monarch"></d-cite></td> <td>$O(L \log L)$</td> <td>Kaleidoscope<d-cite key="kaleidoscope"></d-cite>, Monarch<d-cite key="monarch"></d-cite></td> </tr> <tr> <td>Toeplitz (Convolution)</td> <td>$m_{j-i}$</td> <td>$O(L \log L)$</td> <td>S4<d-cite key="s4"></d-cite>, H3<d-cite key="h3"></d-cite>, TNN<d-cite key="tnn"></d-cite>, CKConv<d-cite key="ckconv"></d-cite></td> </tr> <tr> <td>Discrete Fourier Transform</td> <td>$w^{ij}$</td> <td>$O(L \log^2 L)$</td> <td>FNet<d-cite key="fnet"></d-cite></td> </tr> <tr> <td>Semiseparable</td> <td>\(\textbf{c}^T_i \textbf{A}^{\times}_{i:j} \textbf{b}_j \mathbb{1}_{\{i \geq j\}}\)</td> <td>$O(L)$</td> <td>Mamba (S6, SSD) <d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite></td> </tr> </tbody> </table> <p>As shown in the table above, using structured matrices (all but the dense variants) as the mixer matrix directly leads to sub-quadratic computational complexity.</p> <h2 id="solution-for-all-desiderata-sequence-aligned-matrices">Solution for All Desiderata: Sequence Aligned Matrices</h2> <p>So, can we simply choose any structured matrix as our sequence mixer matrix and expect it to meet all our requirements for efficiency, performance, and flexibility? Unfortunately, not all structured matrix mixers are up to the task. This begs the question: Is there a class of mixer matrices that can satisfy all three requirements? Fortunately, the answer is yes!</p> <p>We introduce a special subset of structured matrices called <strong><em>Sequence Aligned Matrices (SAM)</em></strong>. SAMs are designed to achieve efficiency, high performance, and flexibility all at once.</p> <h4 id="what-are-sequence-aligned-matrices-sam">What are Sequence Aligned Matrices (SAM)?</h4> <p>In simple terms, SAMs ensure that the parameters for every submatrix $\textbf{M}[: i+1, : i+1]$ are only functions of the tokens up to index $i$. Here is a formal definition of SAM.</p> <details><summary>Formal definition of Sequence Alignment</summary> <p><strong>Definition</strong> <em>(Sequence Aligned Matrices)</em> Let $L$ be the sequence length and let $\textbf{M} \in \mathbb{R}^{L \times L}$ denote a matrix with a parameter set $\mathcal{P}$. Then, we say that $\textbf{M}$ is a Sequence Aligned Matrix if there exists a partition $\Pi$ of $\hat{\mathcal{P}} \subseteq \mathcal{P}$, and $\hat{\mathcal{P}} \neq \phi$, such that for all sets $\mathcal{E} \in \Pi$, there exists a bijective map $f_{\mathcal{E}} : [L] \rightarrow \mathcal{E}$, and, for each $i \in [L]$, the sub-matrix $\textbf{M}[:i+1,:i+1]$ is composed solely from the parameters in the subset $\cup_{\mathcal{E}, k \le i} f_{\mathcal{E}}(k) \subseteq \mathcal{P}$.</p> </details> <h4 id="properties-of-sam">Properties of SAM</h4> <p>SAM matrices come with two crucial properties that make them stand out:</p> <ul> <li><strong>Data Dependency</strong>: SAM matrices are dynamically generated from the input data. This means they adapt in real-time based on the information they process.</li> <li><strong>Extendability</strong>: SAM matrices can handle inputs of arbitrary lengths, making them versatile for various applications.</li> </ul> <p>Take, for instance, the Attention mechanism in Transformers. It‚Äôs a perfect example of a SAM matrix: the Query-Key-Value components are all dynamically projected from the input data, and the mechanism itself adapts seamlessly to different sequence lengths.</p> <p>These two properties are not just nice-to-haves; they are essential for the flexibility and performance of modern models. Our experimental results strongly highlight the necessity of SAM, showing that SAM-based mixer matrices significantly enhance the performance of models.</p> <h3 id="sam-variations">SAM Variations</h3> <p>Let‚Äôs dive into a series of new SAM-based models we developed: <em>Toeplitz, Cauchy, Vandermonde, and quasiseparable</em> sequence mixers. By making these mixer matrices SAM, we achieved significant improvements. To make this explanation easier, we‚Äôll assume that Query-Key-Value are projected from an input sequence.</p> <h4 id="cauchy-code">Cauchy <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/cauchy.py">(Code)</a></h4> <p>We begin with our Cauchy variant, as it shares a significant similarity with the Attention mechanism: the norm of $m_{ij}$ represents the magnitude of correlations between the $i$-th and $j$-th tokens. Following the definition of Cauchy matrices, our SAM Cauchy mixer works as follows:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d} \frac{1}{(q_{id} - k_{jd} + c)} \space, \end{equation}\] <p>where $\textbf{Q}, \textbf{K} \in \mathbb{R}^{L \times D}$, and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected matrices from $\textbf{X}$, and $c$ is a trainable constant that stabilizes training by preventing divide-by-zero errors.</p> <h4 id="vandermonde-code">Vandermonde <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/vandermonde.py">(Code)</a></h4> <p>Recall the definition of Vandermonde matrices: $m_{rs} = (m_r)^s$. Due to the exponential values, this can lead to instability during training. Therefore, we use the formulation $q_{rs} = \mathfrak{R}(e^{i \cdot r \cdot q_s})$ and $k_{rs} = \mathfrak{R}(e^{i \cdot s \cdot k_r})$ for $\textbf{Q}$ and $\textbf{K}$. This technique, taking the real part of complex numbers, is commonly used in SSMs. Under the same setting as our SAM Cauchy mixer, our SAM Vandermonde mixer $\textbf{M}$ is parameterized as:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d}(\cos(2 \pi q_{id}^j) - \cos(2 \pi k_{jd}^i)) \space, \end{equation}\] <p>where the cosine function comes from <a href="https://en.wikipedia.org/wiki/Euler's_formula">Euler‚Äôs formula</a>.</p> <h4 id="toeplitz-code">Toeplitz <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/toeplitz.py">(Code)</a></h4> <p>A Toeplitz matrix mixer is inherently a convolution between weights $\textbf{w} \in \mathbb{R}^{2L-1}$ and an input sequence $\textbf{V} \in \mathbb{R}^{L \times C}$. Usually, a general convolution adopts input-independent $\textbf{w}$, which does not satisfy the definition of SAM. Therefore, we extend our Toeplitz matrix mixer to be SAM as follows:</p> \[\begin{equation} \textbf{Y} = \mathcal{F}^{-1}(\mathcal{F}_\textbf{w} \odot \mathcal{F}_\textbf{V}), \qquad \qquad \textbf{w}_{i} = \begin{cases} q_{i-L+1} &amp; \text{if } i \geq L \\ k_{L-i+1} &amp; \text{if } i \lt L \\ \end{cases} \space , \end{equation}\] <p>where the convolution is implemented using FFT $\mathcal{F}$, and $\textbf{q}, \textbf{k} \in \mathbb{R}^{L}$ and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected from $\textbf{X}$.</p> <h4 id="quasiseparable-code">Quasiseparable <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/quasiseparable.py">(Code)</a></h4> <blockquote class="block-tip"> <p><strong>This variant has a separate name, Hydra. Stay tuned for <a href="/blog/2024/hydra-part2-model/">Part II</a> ü§≠</strong></p> </blockquote> <h2 id="impact-of-sam-parameterization">Impact of SAM Parameterization</h2> <p>Now, we validate that the SAM matrix mixers are better than non-SAM mixers. To prove this claim, we conducted strictly controlled systematic albations where the only variable was the mixer matrix. Check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixer.py">our efforts</a> for a comprehensive and fair comparison!</p> <table> <tr> <td style="font-weight:bold;">Structure</td> <td style="text-align:center;">Data Dependent</td> <td style="text-align:center;"># Params</td> <td style="text-align:center;">GLUE Avg</td> <td style="text-align:center;">Œî</td> </tr> <tr> <td style="font-weight:bold;">Dense</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.7</td> <td style="text-align:center;"></td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Toeplitz</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">75.8</td> <td rowspan="2" style="text-align:center;">+1.9</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">77.7</td> </tr> <tr> <td style="font-weight:bold;">DFT</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.7</td> <td rowspan="3" style="text-align:center;">+5.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Vandermonde</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">70.8</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">76.0</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Cauchy</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.2</td> <td rowspan="2" style="text-align:center;">+4.0</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Low-rank</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.9</td> <td rowspan="2" style="text-align:center;">+3.5</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.4</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Attention</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.9</td> <td rowspan="2" style="text-align:center;">+6.9</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.8</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Quasiseparable</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">75.1</td> <td rowspan="2" style="text-align:center;">+4.6</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">79.7</td> </tr> </table> <p>The results in the table above clearly demonstrate the importance of SAM. Regardless of the matrix class, incorporating the SAM property always leads to a significant performance boost. Additionally, our SAM-based Toeplitz, Cauchy, and low-rank mixers perform remarkably well, with quasiseparable mixers even surpassing Attention. These findings underscore the immense potential of structured matrix mixers as efficient yet powerful sequence mixers.</p> <h2 id="next-up">Next Up</h2> <p>Curious about the quasiseparable matrix mixer? In <a href="">the next part</a>, we‚Äôll introduce Hydra, our bidirectional extension of SSMs that not only surpasses Attention but also achieves sub-quadratic complexity. Stay tuned!</p>]]></content><author><name>Sukjun Hwang*</name></author><summary type="html"><![CDATA[[Paper] [Code]]]></summary></entry><entry><title type="html">Hydra Part II - The Model</title><link href="https://goombalab.github.io/blog/2024/hydra-part2-model/" rel="alternate" type="text/html" title="Hydra Part II - The Model"/><published>2024-07-16T00:00:00+00:00</published><updated>2024-07-16T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/hydra-part2-model</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/hydra-part2-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/logo_trans-480.webp 480w,/assets/img/2024-07-16-hydra/logo_trans-800.webp 800w,/assets/img/2024-07-16-hydra/logo_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/logo_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2407.09941">Paper</a>] [<a href="https://github.com/goombalab/hydra">Code</a>]</p> <ol> <li><a href="/blog/2024/hydra-part1-matrix-mixer/">Part I - Matrix Mixer Framework</a></li> <li>Part II - Hydra: The Model</li> </ol> <p>In our previous post, we systematically compared various sequence models with different mixer matrices, and the quasiseparable SAM mixer emerged as the top performer. So, what exactly is it?</p> <h2 id="recap-ssms-are-semiseparable-matrix-mixers">Recap: SSMs Are Semiseparable Matrix Mixers</h2> <p>Before diving into the details of quasiseparable SAM mixers, let‚Äôs briefly revisit some key findings from <a href="https://arxiv.org/abs/2405.21060">Mamba-2</a><d-cite key="ssd"></d-cite>. Recently, Mamba-2 has shown that the mixer matrices of SSMs are inherently parametrized to one of the fundamental structured matrix classes ‚Äì semiseparable matrices.</p> <blockquote> <p><strong>Defintion</strong> of Semiseparable Matrices <br/> A lower triangular matrix $\textbf{M}$ is $N$-semiseparable iff any submatrix from the lower triangle (on or below the diagonal) has a rank of at most $N$. See (a) in the figure below.</p> </blockquote> <p>So why are SSMs semiseparable matrix mixers? Using our previously defined matrix mixer framework, we can represent SSMs as follows:</p> \[\begin{align} \textbf{y}_t &amp;= \sum^{t}_{s=0} \textbf{C}^T_t \left(\prod_{k=s+1}^{i} \textbf{A}_{k}\right) \textbf{B}_s \textbf{x}_s \\ \\ \textbf{Y} &amp;= \text{SSM}(\textbf{A}, \textbf{B}, \textbf{C})(\textbf{X}) = \textbf{M} \textbf{X} \space ,\\ \\ m_{ij} &amp; = \textbf{c}^T_i \textbf{A}_i \cdots \textbf{A}_{j+1} \textbf{b}_j \end{align}\] <p>where each matrix $\textbf{A}_i \in \mathbb{R}^{N \times N}$ and vector $\textbf{c}_i, \textbf{b}_i \in \mathbb{R}^{N \times 1}$. This decomposition shows that SSMs are indeed semiseparable mixers. [If you are not familiar with this concept, we recommend checking out this <a href="/blog/2024/mamba2-part2-theory/">blog post</a> for a great explanation.]</p> <p>Semiseparable matrices are an excellent choice for mixer matrices ‚Äì they are sub-quadratic, performant, and can be extended to handle sequences of various lengths. However, there‚Äôs one significant limitation: due to their definition, the upper right triangle of semiseparable matrices is filled with zeros, making them inevitably causal. This limitation makes SSMs incapable of <strong>bidirectional sequence processing</strong>.</p> <p>Why is bidirectionality important? Bidirectional processing is crucial for several reasons. One major reason is its importance in handling multiple modalities, such as processing 2D images. Without bidirectionality, models can‚Äôt fully leverage information from both past and future contexts within a sequence, which is essential for comprehensive data analysis across various applications.</p> <p>A straightforward way to make SSMs bidirectional is to use two separate SSMs: one for forward sequence processing and one for reverse sequence processing. There are several approaches to combine their outputs, such as adding, multiplying, or concatenating them <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite><d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite>. While these heuristics can work, they lack a principled design philosophy, leading to different heuristics being used for different tasks without a systematic approach.</p> <p>But what if we could use the matrix mixer framework to systematically derive the optimal $\textbf{M}$? Absolutely, we can! In addition to the three desiderata we discussed previously ‚Äì sub-quadratic complexity, extendability, and high-performance ‚Äì let‚Äôs add one more requirement: <strong>bidirectionality</strong>. For the mixer matrix to achieve bidirectionality, it must feature upper triangular components. So, how should we fill them?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/semiquasi_trans-480.webp 480w,/assets/img/2024-07-16-hydra/semiquasi_trans-800.webp 800w,/assets/img/2024-07-16-hydra/semiquasi_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/semiquasi_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="structured-matrix-of-our-choice-quasiseparable-matrices">Structured Matrix of Our Choice: Quasiseparable Matrices</h2> <p>For our bidirectional sequence mixer, we choose quasiseparable matrices. So, what makes quasiseparable matrices stand out? Let‚Äôs start by looking at their definition.</p> <blockquote> <p><strong>Defintion</strong> of Quasiseparable Matrices by the Rank Characterization. <br/> A matrix $\textbf{M}$ is $N$-quasiseparable iff any submatrix from either the strictly upper or lower triangle (off from the diagonal) has a rank of at most $N$. See (b) in the figure above.</p> </blockquote> <p>At first glance, this definition might seem similar to that of semiseparable matrices. To clarify, let‚Äôs highlight the key differences between quasiseparable and semiseparable matrices:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Semiseparable</strong></th> <th><strong>Quasiseparable</strong></th> </tr> </thead> <tbody> <tr> <td>(I)</td> <td>any submatrix from <em>the lower triangle</em></td> <td>any submatrix from either the strictly <em>upper or lower triangle</em></td> </tr> <tr> <td>(II)</td> <td><em>on or below</em> the diagonal</td> <td><em>off</em> from the diagonal</td> </tr> </tbody> </table> <h3 id="quasiseparable-matrices-supset-semiseparable-and-low-rank-matrices">Quasiseparable Matrices $\supset$ Semiseparable and Low-Rank Matrices</h3> <p>Although the differences between quasiseparable and semiseparable matrices might seem subtle, they lead to significant improvements in expressivity. According to difference <strong>(I)</strong>, semiseparable matrices zero out the upper triangular elements, while quasiseparable matrices extend to include these elements, enabling bidirectionality. Consequently, semiseparable matrices can only generalize mixers that use causal low-rank matrices, such as Linear Attention, whereas quasiseparable matrices generalize typical low-rank matrices. Moreover, both differences <strong>(I)</strong> and <strong>(II)</strong> mean that quasiseparable matrices not only generalize but also extend semiseparable matrices.</p> <ul> <li><strong><em>Quasiseparable matrices generalize low-rank matrices.</em></strong></li> <li><strong><em>Quasiseparable matrices generalize and extend semiseparable matrices.</em></strong></li> </ul> <h3 id="quasiseparable-matrices-supset-two-separate-ssms">Quasiseparable Matrices $\supset$ Two Separate SSMs</h3> <p>We now understand that for bidirectional processing scenarios, quasiseparable mixers are indeed better than semiseparable matrices. But what makes quasiseparable mixers superior to the bidirectional extensions using two separate SSMs?</p> <p>Heuristic variants that use the Hadamard product and concatenation <d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite> are difficult to analyze systematically within the matrix mixer framework. Moreover, concatenation variants double the number of output channels, necessitating additional parameters for reducing the number of channels.</p> <p>In contrast, addition-based variants <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite> can be formulated using the matrix mixer framework, as shown in (c) of the figure above, which resembles quasiseparable matrices in (d). However, difference <strong>(II)</strong> highlights that the diagonals of semiseparable matrices are also constrained by the rank characterization, and consequently, so are the diagonals of addition-based extensions. Quasiseparable matrices, on the other hand, do not have this constraint on the diagonals, allowing them to be complete free parameters. This flexibility makes quasiseparable matrices more mathematically expressive than addition-based bidirectional extensions.</p> <ul> <li><strong><em>Quasiseparable matrices are strictly more expressive than mixer matrices of addition-based bidirectional SSMs.</em></strong></li> </ul> <p>This property of complete freedom in the diagonals of quasiseparable matrices is more evident in another definition of quasiseparable matrices:</p> <blockquote> <p>A matrix $\textbf{M}$ is $N$-quasiseparable if each element $m_{ij}$ satisfies:</p> \[\begin{equation} m_{ij} = \begin{cases} \overrightarrow{\textbf{c}^{T}_{i}} \overrightarrow{\textbf{A}_i} \cdots \overrightarrow{\textbf{A}_{j+1}} \overrightarrow{\textbf{b}_{j}}, &amp; \text{if } i &gt; j \\ \delta_{i}, &amp; \text{if } i = j \\ \overleftarrow{\textbf{c}^{T}_{i}} \overleftarrow{\textbf{A}_{i}} \cdots \overleftarrow{\textbf{A}_{j-1}} \overleftarrow{\textbf{b}_{j}}, &amp; \text{if } i &lt; j\\ \end{cases},\\ \end{equation}\] <p>where each $\delta_i$ is a scalar, $\textbf{b}_i, \textbf{c}_i \in \mathbb{R}^{N \times 1}$, and $\textbf{A}_i \in \mathbb{R}^{N \times N}$.</p> </blockquote> <p>These are the actual results we obtained for the C4 and GLUE benchmark, along with the validation loss curve. Supported by these theoretical claims, our Hydra model, which uses a quasiseparable mixer matrix, indeed has shown superior performance to previous heuristic bidirectional extensions!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/bidirectionality_trans-480.webp 480w,/assets/img/2024-07-16-hydra/bidirectionality_trans-800.webp 800w,/assets/img/2024-07-16-hydra/bidirectionality_trans-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-16-hydra/bidirectionality_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hydra-our-main-bidirectional-sequence-mixer">Hydra: Our Main Bidirectional Sequence Mixer</h2> <h3 id="implementation">Implementation</h3> <p>Now that we‚Äôve confirmed quasiseparable matrices as the go-to mixer matrices, we fully leverage them to propose the two-headed Mamba ‚Äì <strong><em>Hydra</em></strong>. Take a look at part (d) in the figure above, which illustrates the mixer matrix of Hydra, and also notice it‚Äôs also our previosly defined SAM! Utilizing an SSM, which is a semiseparable mixer, we can implement Hydra with the following formula: \(QS(\textbf{X}) = \texttt{shift}(SS(\textbf{X})) + \texttt{flip}(\texttt{shift}(SS(\texttt{flip}(\textbf{X})))) + \textbf{DX},\) where $\textbf{X}$ is the input sequence, $\texttt{flip}(\cdot)$ denotes a function that reverses the input, $\texttt{shift}(\cdot)$ denotes a right-shift function, and $\textbf{D} = \text{diag}(\delta_1, \cdots, \delta_L)$ represents the diagonal elements of $QS$. Here, $QS(\cdot)$ and $SS(\cdot)$ are the mixer matrix of Hydra and an SSM, respectively.</p> <p>Among the various iterations of SSMs, we adopt the latest one ‚Äì SSD from Mamba-2. Since SSMs are sub-quadratic, this simple implementation maintains the sub-quadratic cost. Compared to heuristic extensions that use two separate SSMs for bidirectionality, Hydra shares the input processing function $f_X$ for forward and reverse sequence processing, which nearly halves the number of parameters.</p> <p>You can check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py">the actual code</a>. To sum up:</p> <ul> <li>Hydra‚Äôs matrix mixer is meticulously parameterized to be a quasiseparable matrix with enhanced expressivity through shift operations.</li> <li>Hydra is sub-quadratic and super easy to implement using existing SSM implementations like Mamba.</li> <li>Hydra greatly reduces parameter counts compared to bidirectional extensions using two SSMs.</li> </ul> <h3 id="performance">Performance</h3> <p>We have seen that Hydra outperforms heuristic bidirectional extensions of SSMs, but how does it compare to state-of-the-art methods? Surprisingly, Hydra surpasses all previous models, including Transformer-based models such as BERT and ViT. When matched for the number of parameters, Hydra consistently shows the best performance across both NLP and Vision domains, highlighting its versatility.</p> <table> <tr> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #4dabf7">NLP</td> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #69db7c">Vision</td> </tr> <tr> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">GLUE Avg</td> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">Top-1 (%)</td> </tr> <tr> <td style="font-weight:bold;">BERT<d-cite key="bert"></d-cite></td> <td>110M</td> <td>83.5</td> <td style="font-weight:bold;">ViT-B<d-cite key="vit"></d-cite></td> <td>87M</td> <td>78.8</td> </tr> <tr> <td style="font-weight:bold;">MLP-Mixer<d-cite key="mlpmixer"></d-cite></td> <td>112M</td> <td>77.5</td> <td style="font-weight:bold;">S4-ViT-B<d-cite key="s4"></d-cite><d-cite key="s4d"></d-cite></td> <td>89M</td> <td>79.4</td> </tr> <tr> <td style="font-weight:bold;">FNet<d-cite key="fnet"></d-cite></td> <td>112M</td> <td>75.8</td> <td style="font-weight:bold;">Hyena-ViT-B<d-cite key="hyena"></d-cite></td> <td>88M</td> <td>78.4</td> </tr> <tr> <td style="font-weight:bold;">M2<d-cite key="m2"></d-cite></td> <td>116M</td> <td>80.9</td> <td style="font-weight:bold;">Mamba-ViT-B<d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite></td> <td>89M</td> <td>79.1</td> </tr> <tr> <td style="background-color: #f783ac; font-weight:bold;">Hydra</td> <td>112M</td> <td>84.3</td> <td style="background-color: #f783ac; font-weight:bold;">Hydra-ViT-B</td> <td>91M</td> <td>81.0</td> </tr> </table> <p>On the GLUE benchmark, Hydra outperforms BERT by 0.8 points. On ImageNet-1K, Hydra improves by 2.2 points over ViT. These results underscore Hydra‚Äôs capability to set new standards in both natural language processing and image classification tasks!</p> <h2 id="epilogue">Epilogue</h2> <p>Lately, the demand for large-scale computation has never been higher. Since the emergence of Mamba, interests in structured matrices has surged, and now is their time to shine. Structured matrices offer an exciting approach to efficient and powerful input processing, similar to how M2 improved over MLP-Mixer.</p> <p>In recent years, we‚Äôve seen numerous groundbreaking works showcasing promising results using structured matrices like Mamba. If the community strives together, just as we have spent about seven years investigating and improving Transformers, we believe there is enormous potential for further advancements through systematic exploration of different structured matrices, along with better optimized training settings (which have been fine-tuned for Transformers).</p> <p>A big shout-out to the recent <a href="https://arxiv.org/abs/2406.06248">BTT</a><d-cite key="btt"></d-cite> work, which systematically explores structured matrices for effective channel mixers. We were very excited to see this kind of systematic investigation, which is crucial for the continued advancement of better architectures.</p>]]></content><author><name>Sukjun Hwang*</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part I - The Model</title><link href="https://goombalab.github.io/blog/2024/mamba2-part1-model/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part I - The Model"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part1-model</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part1-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2405.21060">Paper</a>] [<a href="https://github.com/state-spaces/mamba">Code</a>]</p> <p><strong>This series is cross-posted at <a href="https://tridao.me/blog/2024/mamba2-part1-model/">tridao.me</a></strong></p> <ol> <li>Part I - The Model</li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>Since the release of <a href="https://arxiv.org/abs/2312.00752">Mamba</a> 6 months ago, we‚Äôve been pleasantly surprised by the overwhelming <a href="https://github.com/AvivBick/awesome-ssm-ml">community response</a>. It‚Äôs been incredibly gratifying to see the line of research on efficient sequence models we‚Äôve been pursuing for years really resonate with the machine learning community and take off more than we could have anticipated. We‚Äôve seen an enormous amount of exciting follow-up work, from direct applications (e.g. vision <d-cite key="zhu2024vision"></d-cite><d-cite key="ma2024u"></d-cite><d-cite key="liu2024vmamba"></d-cite>, genomics <d-cite key="schiff2024caduceus"></d-cite>, graphs <d-cite key="wang2024graph"></d-cite><d-cite key="behrouz2024graph"></d-cite>, and more) to understanding (e.g. on recall abilities <d-cite key="jelassi2024repeat"></d-cite>, in-context learning<d-cite key="akyurek2024context"></d-cite> <d-cite key="grazzi2024mamba"></d-cite> <d-cite key="park2024can"></d-cite>, and formal language expressivity <d-cite key="merrill2024illusion"></d-cite><d-cite key="sarrof2024expressive"></d-cite>), and an enormous number of <a href="https://jackcook.com/2024/02/23/mamba.html">online</a> <a href="https://srush.github.io/annotated-mamba/hard.html">blogs</a>, <a href="https://www.youtube.com/watch?v=dVH1dRoMPBc">tutorials</a>, <a href="https://www.youtube.com/watch?v=8Q_tqwpTpVU">and</a> <a href="https://www.youtube.com/watch?v=N6Piou4oYx8">videos</a>. We couldn‚Äôt be more excited about the direction of this research!</p> <p>Yet despite its potential so far, we weren‚Äôt completely satisfied with the first version of Mamba‚Ä¶</p> <h3 id="problem-1-understanding">Problem 1 (Understanding)</h3> <p>From a conceptual standpoint, one of the reasons we found SSMs so fascinating is how they just feel <em>fundamental</em>. One way this is exemplified is how they have rich ties to many major paradigms of sequence models. As developed in our earlier works on structured SSMs <d-cite key="gu2021combining"></d-cite><d-cite key="gu2023thesis"></d-cite>, they seem to capture the essence of continuous, convolutional, and recurrent sequence models ‚Äì all wrapped up in a simple and elegant model.</p> <p>But of course, aside from these, there‚Äôs another major sequence model paradigm: variants of the ubiquitous <strong>attention</strong> mechanism<d-cite key="bahdanau2015neural"></d-cite><d-cite key="vaswani2017attention"></d-cite>. SSMs always felt somewhat disjoint from attention, and we‚Äôve tried for a while to understand their relationship better.</p> <blockquote> <p>Question 1: <strong>What are the conceptual connections between state space models and attention?</strong> Can we combine them?</p> </blockquote> <h3 id="problem-2-efficiency">Problem 2 (Efficiency)</h3> <p>From a computational standpoint, despite the work that went into making Mamba fast (in particular, its hardware-aware selective scan implementation) it‚Äôs still much less hardware-efficient than mechanisms such as attention. The missing piece is that modern accelerators such as GPUs and TPUs are <em>highly</em> specialized for matrix multiplications. While this isn‚Äôt a problem for inference, which is bottlenecked by somewhat different considerations, this can be a big deal during training time.</p> <blockquote> <p>Question 2: <strong>Can we speed up the training of Mamba models by recasting them as matrix multiplications?</strong></p> </blockquote> <p>These are the main questions that Mamba-2 ‚Äì in particular, its new state space model variant ‚Äì tries to address.</p> <h2 id="the-ssd-model">The SSD Model</h2> <p>The main point of the Mamba-2 paper is what we call <strong>structured state space duality</strong> (SSD), which refers to several things:</p> <ol> <li>The <strong>SSD model</strong> refers to a specific standalone layer, like attention or an SSM, that can be incorporated into deep neural networks</li> <li>The <strong>SSD framework</strong> is a general framework for reasoning about this model (and many more theoretical connections)</li> <li>The <strong>SSD algorithm</strong> is an algorithm for computing SSD layers much more efficiently than previous SSMs</li> </ol> <p>The main SSD model or ‚Äústate space dual model‚Äù itself really isn‚Äôt so complicated! In this first part of a series of blog posts, we‚Äôll provide a self-contained description of the SSD layer (and Mamba-2) in isolation and how it compares to related models, particularly Mamba-1.</p> <p>In the next parts of this series, we‚Äôll describe the general framework and theoretical connections, which aren‚Äôt necessary to actually use Mamba-2.</p> <h3 id="the-linear-ssm-mode">The Linear (SSM) Mode</h3> <p>SSD starts from the same set of equations as Mamba:</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} h_t \end{aligned}\] <p>\begin{equation} \label{eq:ssm} (\text{Selective state space model (SSM)}) \end{equation}</p> <p>To recap, a <strong>structured state space model (SSM)</strong> <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> defines a map from $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$. Think of $x_t$ and $y_t$ as being scalars, and the hidden state $h_t$ as an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the <em>state size, state dimension, or state expansion factor</em>.</p> <p>A <em>selective</em> state space model allows the $(A, B, C)$ SSM parameters to vary across time <d-cite key="gu2023mamba"></d-cite>. We‚Äôll think of them as tensors with shapes $A \in \mathbb{R}^\mathtt{(T, N, N)}$, $B \in \mathbb{R}^\mathtt{(T, N)}$, and $C \in \mathbb{R}^\mathtt{(T, N)}$ respectively.<d-footnote>As with Mamba-1, we take everything over the reals $\mathbb{R}$, although complex variants as with other structured SSMs like the S4 lineage <d-cite key="gu2022efficiently"></d-cite> are also possible.</d-footnote></p> <p>Structured SSMs require $A$ to have structure to be efficiently computable, such as the most commonly used diagonal structure <d-cite key="gu2022parameterization"></d-cite><d-cite key="gupta2022diagonal"></d-cite><d-cite key="smith2023s5"></d-cite><d-cite key="gupta2022simplifying"></d-cite>. In this case $A$ has shape $\mathtt{(T, N)}$ where only the diagonal elements of the $\mathtt{N} \times \mathtt{N}$ matrices are stored.</p> <h4 id="ssd-scalar-structured-ssm">SSD: Scalar Structured SSM</h4> <p>The original Mamba (or more precisely its core ‚ÄúS6‚Äù layer) is exactly a selective SSM with diagonal structure.</p> <p><strong>The SSD layer of Mamba-2 makes only one small modification</strong>: it restricts the diagonal $A$ even further to a <em>scalar times identity</em> structure; in other words the diagonal elements of $A$ must all be the same value. In this case $A$ can be represented with shape just $\mathtt{(T)}$ and one can also identify $A_t$ as just a scalar (and so we‚Äôll sometimes denote it $a_t$).</p> <h4 id="multihead-ssms">Multihead SSMs</h4> <p>Equation \eqref{eq:ssm} is defined only for a single dimensional input $x \in \mathbb{R}^\mathtt{T}$. If $X \in \mathbb{R}^\mathtt{(T, P)}$ has $\mathtt{P}$ separate channels, we can use the same dynamics (i.e. the same SSM $(A, B, C)$) independently for each channel. This can be interpreted as a <em>single head</em> of the SSM model.</p> <p>Here, we think of $X$ as a tensor of shape $\mathtt{(T, P)}$ where $\mathtt{T}$ is the sequence (time) dimension and $\mathtt{P}$ is the ‚Äúhead dimension‚Äù.<d-footnote>Normally there's an additional batch dimension $\mathtt{B}$ when implementing these models, which we'll ignore throughout this presentation.</d-footnote></p> <p>Multiple heads can be constructed completely independently; for the remainder of this post, we assume that we‚Äôre working with a single head. Note that these heads are exactly analogous to how heads in multi-head attention models work, and in Mamba-2 we also choose similar dimensions as modern Transformers, e.g. $\mathtt{P} = 64$ or $\mathtt{P}=128$. (To scale to larger model widths $\mathtt{D} = \mathtt{d\_model}$, we keep this fixed and increase the number of independent heads.)</p> <p>We can notate the general (selective) state space model as \begin{equation} \label{eq:ssm-transformation} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T,‚Ä¶)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>Some axes of variation include</p> <ol> <li>The structure on $A$, which affects its parameter shape: <ul> <li><code class="language-plaintext highlighter-rouge">... = (N,N)</code> for general (unstructured) SSMs</li> <li><code class="language-plaintext highlighter-rouge">... = (N)</code> for diagonal SSMs (or other structures, such as diagonal-plus-low-rank <d-cite key="gu2022efficiently"></d-cite>)</li> <li><code class="language-plaintext highlighter-rouge">... = ()</code> for scalar SSMs (i.e. SSD)</li> </ul> </li> <li>The state dimension $\mathtt{N}$ (i.e. <code class="language-plaintext highlighter-rouge">d_state</code>)</li> <li>The head dimension $\mathtt{P}$ (i.e. <code class="language-plaintext highlighter-rouge">d_head</code>)</li> </ol> <p>There are other axes of variation of structured SSMs (e.g. time-invariance vs. selectivity, SISO vs. MIMO<d-cite key="smith2023s5"></d-cite>, real vs. complex, etc.), but we‚Äôre highlighting these so that we can contrast Mamba-2 to Mamba-1 in just a second‚Ä¶</p> <h3 id="the-quadratic-attention-mode">The Quadratic (Attention) Mode</h3> <p>But first, let‚Äôs switch tacks and forget about state space models for a moment. Given the same tensors above with the same shapes $(A^\mathtt{(T)}, B^\mathtt{(T, N)}, C^\mathtt{(T, N)})$, let‚Äôs define a different object.</p> <p>First, we‚Äôll define the following matrix (don‚Äôt worry, we‚Äôll explain more and give it a name in Part II of this series!)</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix} .\] <p>Then, let‚Äôs define the following matrix</p> <p>\begin{equation} \label{eq:ssd-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Finally, $M$ encodes a <em>sequence transformation</em> $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$ mapping a 1D input to a 1D output‚Äîjust as in equation \eqref{eq:ssm}‚Äîthrough basic matrix multiplication $y = Mx$.</p> <p>What‚Äôs special about this? Well, you may notice that it looks very similar to an attention computation. In fact, if all $a_t = 1$, then $L$ is simply the lower-triangular <em>causal mask</em> and \eqref{eq:ssd-attention} is equivalent to <strong>causal linear attention</strong> <d-cite key="katharopoulos2020transformers"></d-cite>:</p> \[Y = (L \circ Q K^\top) V\] <p>This is exactly the same as equation \eqref{eq:ssd-attention} if we rename $(C, B, X) \mapsto (Q, K, V)$!</p> <h2 id="state-space-duality">State Space Duality</h2> <p>The so-called ‚Äúduality‚Äù refers to the fact that the two models defined in equations \eqref{eq:ssm} (for the scalar-identity structured $A_t$ case) and \eqref{eq:ssd-attention} are actually <em>exactly the same model</em>, which we can view as a particular function</p> \[(A^\mathtt{(T)}, B^\mathtt{(T, N)}, C^\mathtt{(T, N)}, X^\mathtt{(T, P)}) \mapsto Y^\mathtt{(T, P)}\] <p>In the general <em>SSD Framework</em> (Part II of this series), we‚Äôll show this equivalence in two completely different ways, both of which are actually much more general and each quite illuminating.</p> <p>If you take our word for it, though, then SSD is relatively simple to contrast in relation to either SSMs or attention.</p> <h3 id="ssd-vs-state-space-models">SSD vs. State Space Models</h3> <p>Compared to previous SSMs, SSD is pretty much the same as the core layer of Mamba but with even more structure on the recurrent $A$ matrices.</p> <ol> <li>Mamba-1 (S6) uses diagonal structure on $A$, while Mamba-2 (SSD) uses scalar-times-identity structure on $A$.</li> <li>Mamba-1 has a head dimension of $\mathtt{P}=1$ (i.e. all channels are completely independently controlled by separate SSMs), while Mamba-2 uses a head dimension of $\mathtt{P}&gt;1$ (something like $\mathtt{P}=64$ by default).</li> </ol> <p>In particular, this can be viewed as weight-tied in two ways:</p> <ul> <li>By restricting the diagonal structure of $A$ to scalar-times-identity, the recurrence dynamics are shared across all $\mathtt{N}$ elements of the state space.</li> <li>These dynamics are also shared across all $\mathtt{P}$ channels of a given head.</li> </ul> <p>In other words, a single SSM head has total state size $\mathtt{P} \times \mathtt{N}$, which are each governed by separate scalar recurrences in Mamba-1 but are controlled by a single shared recurrence in Mamba-2.</p> <p>Why make these restrictions? The main motivation is efficiency: these changes are necessary to be able to view the model in its [<a href="#the-quadratic-attention-mode">dual attention form</a>], which allows matrix multiplications to be used.</p> <blockquote class="block-tip"> <h4 id="the-bottom-line-mamba-1-vs-mamba-2">The Bottom Line: Mamba-1 vs. Mamba-2</h4> <p>Compared to Mamba-1, Mamba-2 allows <strong>much larger state dimensions</strong> (from <code class="language-plaintext highlighter-rouge">N=16</code> in Mamba-1 to <code class="language-plaintext highlighter-rouge">N=64</code> to <code class="language-plaintext highlighter-rouge">N=256</code> or even higher in Mamba-2) while simultaneously being <strong>much faster during training</strong>.</p> </blockquote> <p>But can this hurt us? There‚Äôs some intuition to believe that it shouldn‚Äôt. One of the main reasons for the selectivity (e.g. $A$ that depends on the input $X$) introduced in Mamba is to let the SSM be able to control whether to remember or ignore particular pieces of information; for example, if a filler ‚Äúum‚Äù is encountered in a text transcript. But if such information should be ignored, then the entire state can ignore it together, and so it should be okay if the state‚Äôs dynamics are shared across all features.</p> <p>Empirically, we haven‚Äôt found evidence that the restricted expressivity of Mamba-2 might hurt, but the jury‚Äôs still out! From one perspective, Mamba-2 isn‚Äôt <em>strictly</em> better than Mamba-1: while it‚Äôs a dramatic improvement from a <em>training</em> perspective, Mamba-1 might be better from a pure <em>inference</em> perspective. Since inference speed of SSMs is entirely governed by the state dimension, if one wants to maximize performance for a target inference efficiency (i.e. for a particular state size $\mathtt{N}$), then the increased expressivity of Mamba-1 might be better. We haven‚Äôt fully analyzed the (theoretical or empirical) tradeoffs here, and think this would be a cool direction for the community to dig in more!</p> <h3 id="ssd-vs-attention">SSD vs. Attention</h3> <p>Compared, to standard (self-)attention, SSD also only has two differences:</p> <ol> <li>The softmax normalization is dropped.</li> <li>A separate elementwise mask matrix is applied multiplicatively.</li> </ol> <p>The first difference can be interpreted as what reduces the effective state size of the model from linear to constant, and improves its efficiency from quadratic to linear.</p> <p>The second difference is what distinguishes SSD from standard linear attention. One way to think of the mask is as <strong>input-dependent relative positional encodings</strong>. Because of the mask $L$ in \eqref{eq:ssd-attention}, the standard attention score $\langle Q_i, K_j \rangle$ is attenuated by a weight</p> \[a_{i:j}^\times = a_i \cdots a_{j+1}\] <p>which can be interpreted as a ‚Äúdiscount factor‚Äù based on how far apart the positions $i$ and $j$ are. (This interpretation was concurrently espoused by Tobias Katsch‚Äôs <a href="https://arxiv.org/abs/2311.01927">GateLoop</a> paper<d-cite key="katsch2023gateloop"></d-cite>.) In its attention form, this input-dependent positional mask can be interpreted as the key factor that encodes the ‚Äúselectivity‚Äù of Mamba!</p> <h2 id="best-of-both-worlds">Best of Both Worlds</h2> <p>So why do we care that there are two views of this model? Well, first of all, it‚Äôs extremely mathematically interesting, as we‚Äôll cover in <a href="/blog/2024/mamba2-part2-theory/">Part II</a>, and we hope will inspire future directions. But there are immediate practical benefits too!</p> <h3 id="the-ssm-and-attention-modes">The SSM and Attention Modes</h3> <p>The SSM \eqref{eq:ssm} and attention \eqref{eq:ssd-attention} modes represent two different ways of computing the same function, so let‚Äôs contrast them.</p> <p>First, remember that one main reason why SSMs are interesting to begin with is because computing \eqref{eq:ssm} as a recurrence requires maintaining a <em>constant-size state</em> (size $\mathtt{N}$ per channel) and scales <em>linearly in the sequence length</em> $\mathtt{T}$. The downside is that the raw FLOPs don‚Äôt reflect actual speed in practice because of hardware considerations‚Ä¶</p> <p>On the other hand, computing this sequence transformation $y = Mx$ through equation \eqref{eq:ssd-attention} takes quadratic time in the sequence length, because we‚Äôre materializing this $\mathtt{T} \times \mathtt{T}$ matrix. But it can be fast in practice because it only uses matrix multiplications, which are extremely optimized on GPUs and TPUs.</p> <h3 id="the-ssd-mode">The SSD Mode</h3> <p>So if there are two equivalent ways of computing the same model, when should we use one mode or the other? During inference, there‚Äôs no trade-off: the SSM mode is designed for fast autoregressive inference. But what about training? Here there‚Äôs a tension between FLOPs and hardware efficiency where the attention mode uses more FLOPs, but uses them more efficiently through matrix multiplications.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_algorithm-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_algorithm.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It turns out we can get the best of both worlds by combining the algorithms! There are two equivalent interpretations of this ‚Äústate space dual‚Äù algorithm, either as</p> <ol> <li>A block decomposition of a particular structured matrix that defines the SSD ‚Äútoken-mixing‚Äù sequence transformation.</li> <li>A ‚Äúchunkwise‚Äù algorithm that splits the sequence into segments, computes the quadratic attention form on each segment, and adjusts the result by passing the SSM states between segments.</li> </ol> <p>We‚Äôll leave the details of this algorithm to <a href="/blog/2024/mamba2-part3-algorithm/">Part III</a> (or Section 6 of the <a href="https://arxiv.org/abs/2405.21060">full paper</a>), as it requires a bit of machinery from the theory to derive. But we do emphasize that the implementation of this algorithm isn‚Äôt too complicated ‚Äì a minimal implementation that we provide is only ~30 lines of PyTorch!</p> <p>The benefits of the SSD algorithm is that it preserves the same efficient FLOP counts as SSMs (compared to quadratic attention), and also dramatically speeds up training compared to general state space models by utilizing matmuls.</p> <table> <thead> <tr> <th>¬†</th> <th>Attention</th> <th>SSM</th> <th>SSD</th> </tr> </thead> <tbody> <tr> <td>State size</td> <td>$\mathrm{T}$</td> <td>$\mathbf{N}$</td> <td>$\mathbf{N}$</td> </tr> <tr> <td>Training FLOPs</td> <td>$\mathrm{T}^2\mathrm{N}$</td> <td>$\mathbf{TN^2}$</td> <td>$\mathbf{TN^2}$</td> </tr> <tr> <td>Inference FLOPs</td> <td>$\mathrm{T}\mathrm{N}$</td> <td>$\mathbf{N^2}$</td> <td>$\mathbf{N^2}$</td> </tr> <tr> <td>(Naive) memory</td> <td>$\mathrm{T}^2$</td> <td>$\mathrm{TN}^2$</td> <td>$\mathbf{TN}$</td> </tr> <tr> <td>Matrix multiplications?</td> <td>:heavy_check_mark:</td> <td>:x:</td> <td>:heavy_check_mark:</td> </tr> </tbody> </table> <h2 id="the-mamba-2-architecture">The Mamba-2 Architecture</h2> <p>Although the core contribution of Mamba-2 is the new SSD layer and theory, we also make some small changes to Mamba‚Äôs neural network architecture.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/architecture_2-480.webp 480w,/assets/img/2024-05-31-mamba-2/architecture_2-800.webp 800w,/assets/img/2024-05-31-mamba-2/architecture_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/architecture_2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The main change is producing the $(A, B, C)$ SSM parameters in parallel with the $X$ input, instead of sequentially. This is partly motivated by the connections to attention; but more pragmatically, it‚Äôs simpler and more amenable to scaling techniques such as tensor parallelism, which will be discussed in Part IV of this series!</p> <p>There are some other small differences which are covered in more detail in the paper. However, we do want to emphasize that these architectural changes aren‚Äôt really the main point of the model.</p> <h3 id="language-modeling">Language Modeling</h3> <p>In terms of empirical results, we didn‚Äôt test Mamba-2 as extensively as Mamba-1, but believe it should generally be on par or better across the board. Our full language model results use the same protocol as Mamba, and found slightly better scaling at Chinchilla laws <d-cite key="hoffmann2022empirical"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-480.webp 480w,/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-800.webp 800w,/assets/img/2024-05-31-mamba-2/pile_8k_mamba2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/pile_8k_mamba2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fully trained models on the Pile dataset<d-cite key="pile"></d-cite> and the standard zero-shot downstream evaluations show similar trends. We emphasize that even when the performance is comparable, Mamba-2 is <em>much</em> faster to train than Mamba-1!</p> <h3 id="associative-recall">Associative Recall</h3> <p>More interestingly, we highlight the one synthetic task we tried. Since the original Mamba paper, which investigated synthetics such as Synthetic Copying and Induction Heads, many follow-up works have begun investigating harder associative recall tasks. The <strong>multi-query associative recall (MQAR)</strong> task introduced by the Zoology and Based <d-cite key="arora2024zoology"></d-cite><d-cite key="arora2024simple"></d-cite> line of work has become a de facto standard.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mqar-480.webp 480w,/assets/img/2024-05-31-mamba-2/mqar-800.webp 800w,/assets/img/2024-05-31-mamba-2/mqar-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mqar.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We ran a version of this task that‚Äôs much harder than the one usually reported in the literature, and found that Mamba-2 is substantially better than Mamba-1. One reason for the improved performance is the much larger state size (up to $16\times$ larger than Mamba-1 here), which was one of the primary motivations of Mamba-2 in the first place.</p> <p>Interestingly, Mamba-2 also appears to be noticeably better than Mamba-1 on this particular task even when the state size is controlled. We‚Äôre not quite sure why to be honest, and it would be great to ablate the other aspects of the model to investigate‚Ä¶ for example, could it be possible that the [<a href="#ssd-vs-state-space-models">restricted structure of SSD</a>] is actually <em>helpful</em> here?</p> <h2 id="next-up">Next Up</h2> <p>In <a href="/blog/2024/mamba2-part2-theory/">the next part of this series</a>, we‚Äôll go more into the full SSD framework, including how to prove the claimed ‚Äúduality‚Äù of the SSD layer, and strong generalizations of it.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part II - The Theory</title><link href="https://goombalab.github.io/blog/2024/mamba2-part2-theory/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part II - The Theory"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part2-theory</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part2-theory/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li>Part II - The Theory</li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>In <a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series, we defined the state space dual (SSD) <em>model</em>. In isolation, this model is relatively simple to define, and we claimed that it can be computed either as an SSM recurrence or with an attention-like pattern. If you just want to use the model, feel free to skip this post!</p> <p>In this post, we‚Äôll dive into the theory behind the model. We‚Äôll derive the SSD ‚Äúduality‚Äù in two completely separate ways, one starting from the SSM perspective and one from the attention perspective. Each method is actually much more broad than the SSD model itself, and the union of these two strong generalizations is what we call the SSD <em>framework</em>. This framework provides a rich body of connections between state space models, attention, and structured matrices. While the SSD model can be viewed as a specific instantiation of each prong of the framework, the SSD framework is much more general opens up many directions for future work.</p> <h4 id="the-state-space-duality-framework">The State Space Duality framework</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_venn-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_venn-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_venn-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_venn.png" width="100%" height="auto" title="Structured State Space Duality" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SSD Framework (red, blue): State space models (i.e. semiseparable matrices) and structured masked attention encapsulate large classes of efficient sequence models. Their intersection is the SSD model (purple).</figcaption> </figure> <p>For each of the two parts of this framework, we‚Äôll</p> <ol> <li>Define the general concepts</li> <li>Show how the SSD model is an instantiation, and prove the duality</li> <li>Suggest future directions for how the framework can be used</li> </ol> <p>Note that this theory is <em>not necessary</em> to use the SSD model itself; this part of the series can be safely skipped for the practitioner that just wants to use SSD (Mamba-2).</p> <h2 id="recap-the-ssd-model">Recap: The SSD Model</h2> <p><a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series introduced the SSD layer, which is defined as a selective SSM</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} y_t \end{aligned}\] <p>\begin{equation} \label{eq:ssm} (\text{Selective state space model (SSM)}) \end{equation}</p> <p>with scalar-identity structure on $A$.</p> <p>More formally, we view it as a <em>sequence transformation</em> $X \mapsto Y$</p> <p>\begin{equation} \label{eq:ssm-transformation} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>The dual attention-like form of the SSD layer is</p> <p>\begin{equation} \label{eq:ssd-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Now let‚Äôs see how to prove this!</p> <h2 id="ssd-framework-1-structured-matrix-transformations">SSD Framework 1: Structured Matrix Transformations</h2> <p>The first framing of the duality will be from an SSM-centric perspective, where we‚Äôll prove the duality through the framework of <strong>matrix sequence transformations</strong> or ‚Äúmatrix mixers‚Äù.</p> <h3 id="matrix-transformations">Matrix Transformations</h3> <p>The idea is that many sequence models, i.e. <em>sequence transformations</em> $X \in \mathbb{R}^\mathtt{(T,P)} \mapsto Y \in \mathbb{R}^\mathtt{(T,P)}$, can be written in the form of a single matrix multiplication $Y = M(X) \cdot X$ where $M$ is a matrix which can itself depend on $X$. We call this a <em>matrix sequence transformation</em>, or matrix transformation for short. In the literature sequence transformations have also been referred to as ‚Äúsequence mixers‚Äù or ‚Äútoken mixers‚Äù, and matrix sequence transformations as ‚Äúmatrix mixers‚Äù. There are many examples of these, which are distinguished by the structure of the $M$ matrix. The de facto example is self-attention itself, where $M = \mathsf{softmax}(QK^\top)$ is the attention matrix. Other examples include MLP-Mixer<d-cite key="tolstikhin2021mlp"></d-cite>, FNet<d-cite key="lee2021fnet"></d-cite>, and Monarch Mixer<d-cite key="dao2022monarch"></d-cite><d-cite key="fu2024monarch"></d-cite>.</p> <p>Why do we care about these types of models?</p> <blockquote> <p>Writing a sequence model as a matrix transformation provides a powerful tool to understand the structure and characteristics of the model.</p> </blockquote> <p>And although general non-linear RNNs such as LSTMs <em>cannot</em> be written as matrix mixers, state space models can! In fact, this is pretty easy to see by just unrolling the definition of the SSM recurrence. The upshot is that the SSM \eqref{eq:ssm-transformation} can be written as a matrix transformation</p> \[Y = \mathsf{SSM}(A, B, C)(X) = MX\] <p>where $M_{ij} = 0$ for $i &lt; j$ (i.e. it‚Äôs lower triangular) and otherwise \begin{equation} \label{eq:semiseparable} M_{ij} = C_i^\top A_{i:j}^\times B_j := C_i^\top A_i \dots A_{j+1} B_j \end{equation}</p> <p>Drawing it out, this matrix looks like</p> \[\begin{bmatrix} C_0^\top B_0 &amp; \\ C_1^\top A_1 B_0 &amp; C_1^\top B_1 &amp; \\ C_2^\top A_2A_1 B_0 &amp; C_2^\top A_2 B_1 &amp; C_2^\top B_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_1 B_0 &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_2 B_1 &amp; \dots &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1} B_{\mathtt{T}-2} &amp; C_\mathtt{T}^\top B_{\mathtt{T}-1} \\ \end{bmatrix}\] <p>\begin{equation} \label{eq:ssm-matrix} (\text{Matrix Transformation Representation of State Space Models}) \end{equation}</p> <h3 id="semiseparable-matrices">Semiseparable Matrices</h3> <p>This type of matrix in fact has a name: it‚Äôs called a (triangular) <strong>semiseparable matrix</strong>, and has been studied in other fields of engineering and computational linear algebra<d-cite key="vandebril2005bibliography"></d-cite>. These matrices are (IMO) quite fundamental and beautiful, and the full paper talks about more of their properties. For example, an alternative characterization of semiseparable matrices is their <em>structured rank property</em>, which says that every submatrix contained in the lower-triangular portion is low rank.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/semiseparable-480.webp 480w,/assets/img/2024-05-31-mamba-2/semiseparable-800.webp 800w,/assets/img/2024-05-31-mamba-2/semiseparable-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/semiseparable.png" width="100%" height="auto" title="State Space Models are Semiseparable Matrices" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">All submatrices contained on-and-below the diagonal of a semiseparable matrix are low-rank.</figcaption> </figure> <p>For our purposes, we‚Äôll care about this form mainly for the algorithmic considerations. One of the central messages of this SSD paper is that:</p> <blockquote class="block-tip"> <h4 id="takeaway-computing-ssms-through-matrix-multiplication">Takeaway: Computing SSMs Through Matrix Multiplication</h4> <p>All algorithms for computing state space models can be viewed as structured matrix multiplication algorithms on semiseparable matrices.</p> </blockquote> <p>Let‚Äôs see an easy instantiation of this, focusing on our main objective!</p> <h3 id="deriving-the-duality-ssm-to-attention">Deriving the Duality: SSM to Attention</h3> <p>To show that equation \eqref{eq:ssd-attention} follows from equation \eqref{eq:ssm} (in the case of the SSD model, i.e. scalar SSM), we directly use the matrix form of the state space model \eqref{eq:semiseparable}. Because the $A_t$ are all scalars in this case, they can be factored out of the entries</p> \[C_i^\top A_{i:j}^\times B_j = A_{i:j}^\times \cdot (C_i^\top B_j)\] <p>which directly implies equation \eqref{eq:ssd-attention}.</p> <p>In summary:</p> <blockquote class="block-tip"> <h4 id="duality-representation-1-ssm">Duality Representation 1 (SSM)</h4> <p>The duality for the SSD model can be seen as two <strong>different matrix multiplication algorithms</strong> on the semiseparable matrix.</p> </blockquote> <ul> <li>The linear form is a <em>structured matrix multiplication algorithm</em> that computes the outputs $Y_0, Y_1, \dots$ sequentially, leveraging the structure of the semiseparable matrix.</li> <li>The quadratic form is the <em>naive matrix multiplication algorithm</em> that materializes the full matrix.</li> </ul> <h3 id="going-beyond-the-ssd-layer-1">Going Beyond the SSD Layer 1</h3> <p>The power of the semiseparable matrix representation applies to <em>all</em> state space models, with various downstream implications.</p> <h4 id="algorithms">Algorithms</h4> <p>Algorithmically, the Mamba-2 paper explores several consequences, such as:</p> <ol> <li>The above duality result for the SSD model, i.e. a scalar-identity structured SSM.</li> <li>New asymptotic efficiency results for state space models (<a href="https://arxiv.org/abs/2405.21060">Theorem 3.7</a>), which follow from applying known results from the semiseparable matrix literature <d-cite key="pernet2016computing"></d-cite><d-cite key="pernet2018time"></d-cite><d-cite key="pernet2023exact"></d-cite>.</li> <li>A more general hybrid algorithm that can be viewed as combining both the linear and quadratic forms to get the best of both worlds. This can be derived as a new matrix multiplication algorithm utilizing <em>block decompositions</em> of the semiseparable matrix. This is the subject of Part III of this blog series!</li> </ol> <h4 id="understanding">Understanding</h4> <p>Conceptually, the matrix transformation viewpoint helps provide a unifying view of sequence models. Some example downstream ideas include</p> <ul> <li><strong>New sequence models</strong>: Restricting ourselves to matrix transformations reduces the problem of developing new sequence models to that of finding structured matrix classes with target properties. In ongoing work by my students, we study this point of view, and use it to derive the most natural bidirectional extension of Mamba (coming very soon!).</li> <li><strong>Expressivity</strong>: Looking at the matrix transformation representation can help us understand what different models can represent from a linear algebraic perspective. In another ongoing work, we use this as a tool to study which subquadratic models are the most amenable to being distilled from Transformers.</li> <li><strong>Interpretability</strong>: A concurrent work <d-cite key="ali2024hidden"></d-cite> derived the matrix formulation of SSMs and use it to probe the internal representations of Mamba models.</li> </ul> <p>We‚Äôre excited to see what algorithmic and conceptual ideas from the structured matrix literature can be applied to further improve state space models!</p> <h2 id="ssd-framework-2-structured-attention">SSD Framework 2: Structured Attention</h2> <p>The second framing of the duality is from an attention-centric perspective, where we‚Äôll prove the duality through the framework of <strong>tensor contractions</strong>.</p> <p>Note that this is entirely independent of the previous [<a href="#ssd-framework-1-structured-matrix-transformations">matrix transformation viewpoint</a>].</p> <h3 id="warm-up-kernel-attention">Warm-up: Kernel Attention</h3> <p>For our purposes, we‚Äôll define attention as a function</p> \[(Q^\mathtt{(T,N)}, K^\mathtt{(S,N)} , V^\mathtt{(S,P)} ) \mapsto Y^\mathtt{(T,P)}\] <p>given by the pairwise matrix multiplications</p> \[Y = (QK^\top) \cdot V\] <details><summary>On Dimensions</summary> <p>Think of $\mathtt{P} = \mathtt{N}$ as the head dimension; technically speaking, in attention the $V$ head dimension $\mathtt{P}$ can differ from the $QK$ head dimension $\mathtt{N}$. Think of $\mathtt{T}$ as the <em>target</em> sequence dimension and $\mathtt{S}$ as the <em>source</em> sequence dimension. Giving these two axes different names will make the math more clear and also covers more general forms of attention such as cross-attention, where the source and target are separate sequences with different lengths. However, for our purposes we‚Äôll assume the self-attention setting where $\mathtt{S}=\mathtt{T}$.</p> </details> <details><summary>Why can we assume this form?</summary> <p>The usual form of attention $Y = f(QK^\top) \cdot V$ (e.g. where $f$ is the softmax function) can, for essentially all functions $f$<d-footnote>And up to some additional massaging such as row-wise normalization, which is easy to handle</d-footnote>, be written as $Y = \psi(Q)\psi(K)^\top \cdot V$ for some appropriate feature map $\psi$ (which may be infinite dimensional). In this case, we can simply redefine $Q \leftarrow \psi(Q)$ and define $\mathtt{N}$ to be the <strong>feature dimension</strong> of the attention kernel to begin with. Softmax attention, for example, can be represented with a particular infinite-dimensional feature map ($\mathtt{N}=\infty$) which represents the exponential kernel.</p> </details> <p>We‚Äôll restrict ourselves to the case when $\psi$ is finite, which is sometimes called <strong>kernel attention</strong>. Many, many variants have been proposed before!<d-cite key="katharopoulos2020transformers"></d-cite><d-cite key="peng2021random"></d-cite><d-cite key="choromanski2021rethinking"></d-cite><d-cite key="qin2022cosformer"></d-cite><d-cite key="zheng2022linear"></d-cite><d-cite key="wang2020linformer"></d-cite><d-cite key="xiong2021nystromformer"></d-cite></p> <p>Why do we care about this formulation? When the sequence length $\mathtt{T}$ grows and the feature dimension $\mathtt{N}$ is small‚Äîcommonly, in the regime when $\psi$ is simple such as an elementwise transform and so $\mathtt{N}$ is constant‚Äîthen the cost of attention can be reduced from quadratic in $\mathtt{T}$ to linear. This follows from simply computing the matrix multiplications in a different order</p> \[Y = Q \cdot (K^\top V)\] <p>This is a somewhat ‚Äúfolklore‚Äù interpretation of linear attention.<d-footnote>At least, one lineage of efficient attention; other varieties exist, such as those based on sparsity or hashing. We reserve the term "linear attention" to those related to Katharopoulos et al.<d-cite key="katharopoulos2020transformers"></d-cite>, or more broadly low-rank attention.</d-footnote></p> <blockquote> <p>The most common way of linearizing attention is usually viewed as a consequence of the <strong>associativity of matrix multiplication</strong></p> </blockquote> <h3 id="causal-linear-attention">(Causal) Linear Attention</h3> <p>However, once the basic kernel attention is slightly modified, we can no longer use the associativity of matrix multiplication directly.</p> <p>The seminal <strong>Linear Attention (LA)</strong> framework of Katharopoulos et al. <d-cite key="katharopoulos2020transformers"></d-cite> shows that it can still be extended to the important case of incorporating causality into attention, for autoregressive settings such as language modeling.</p> <p>Let‚Äôs be a lot more explicit about how it works. The quadratic form of <strong>causal linear attention</strong> is \begin{equation} \label{eq:quadratic-kernel-attention} Y = (L \circ QK^\top) \cdot V \end{equation} where</p> \[L = \begin{bmatrix} 1 \\ \vdots &amp; \ddots \\ 1 &amp; \dots &amp; 1 \end{bmatrix}\] <p>is the <strong>causal mask</strong> matrix.</p> <p>The issue is: once the $L$ mask is incorporated into \eqref{eq:quadratic-kernel-attention}, we can no longer directly apply matrix associativity! This is the problem that the original Linear Attention paper addresses. What they show is that \eqref{eq:quadratic-kernel-attention} is equivalent to a different form which avoids materializing the quadratic $QK^\top$ attention matrix and has linear time complexity</p> \[Y = Q \cdot \mathsf{cumsum}(K^\top V)\] <p>As far as we‚Äôre aware this wasn‚Äôt explicitly proved in the paper, although it isn‚Äôt too hard to write out the summation to show it.</p> <p>What we‚Äôll do is prove this equivalence in essentially one line, while revealing <em>exactly</em> where the ‚Äúlinear‚Äù part of Linear Attention comes from, and how to strongly generalize it.</p> <p>Spoiler alert:</p> <blockquote class="block-tip"> <h4 id="where-does-the-cumsum-in-linear-attention-come-from">Where does the cumsum in Linear Attention come from?</h4> <p>The appearance of the <em>cumulative sum</em> in linear attention is exactly equivalent to the fact that the causal mask $L$, as a matrix multiplication, encodes cumulative sums:</p> \[y = L \cdot x \iff y = \mathsf{cumsum}(x)\] </blockquote> <h3 id="a-tensor-contraction-proof-of-linear-attention">A Tensor Contraction Proof of Linear Attention</h3> <p>Let‚Äôs write out the quadratic form of linear attention \eqref{eq:quadratic-kernel-attention} very explicitly in <strong>tensor contraction</strong> or <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a> notation, with shape annotations:</p> \[\begin{aligned} G &amp;= \mathsf{contract}(\mathtt{TN, SN} \to \mathtt{TS})(Q, K) \\ M &amp;= \mathsf{contract}(\mathtt{TS, TS} \to \mathtt{TS})(G, L) \\ Y &amp;= \mathsf{contract}(\mathtt{TS, SP} \to \mathtt{TP})(M, V) \end{aligned}\] <p>\begin{equation} \label{eq:sma-quad} (\text{Structured Masked Attention - Quadratic Form}) \end{equation}</p> <p>With this notation, we can notice that this sequence of contractions can be written as a <em>single four-way contraction</em></p> <p>\begin{equation} \label{eq:sma} y = \mathsf{contract}(\mathtt{TN},\mathtt{SN},\mathtt{SP},\mathtt{TS} \to \mathtt{TP})(Q, K, V, L) . \end{equation}</p> <p>And finally, it can be computed with any other contraction ordering. In particular, we can perform pairwise reductions on the order $V, K, L, Q$ instead of $Q, K, L, V$</p> \[\begin{aligned} Z &amp;= \mathsf{contract}(\mathtt{SP},\mathtt{SN} \to \mathtt{SPN})(V, K) \\ H &amp;= \mathsf{contract}(\mathtt{TS},\mathtt{SPN} \to \mathtt{TPN})(L, Z) \\ Y &amp;= \mathsf{contract}(\mathtt{TN},\mathtt{TPN} \to \mathtt{TP})(Q, H) \end{aligned}\] <p>\begin{equation} \label{eq:sma-lin} (\text{Structured Masked Attention - Linear Form}) \end{equation}</p> <p>Now the key observation is that the second line of \eqref{eq:sma-lin} is simply a matrix multiplication by $L$, which can be computed with a cumulative sum.</p> <p>That‚Äôs the entire proof of linear attention! The beauty of it is that we didn‚Äôt have to write out a single summation, which was abstracted out into a tensor contraction combined with the structure of $L$.</p> <p>This immediately proves our claim about the <a href="#where-does-the-cumsum-in-linear-attention-come-from">cumsum in linear attention</a>. Moreover, this immediately reveals that the efficiency of linear attention can be made much more general‚Ä¶</p> <h3 id="structured-masked-attention">Structured Masked Attention</h3> <p>The critical observation is that in order for \eqref{eq:sma-lin} to be fast, all that is necessary is for $L$ to be <em>any structured matrix</em> ‚Äì in other words any matrix that has subquadratic matrix-vector multiplication.</p> <p>This immediately motivates one of the main prongs of the SSD framework, which can be seen as a strong generation of LA.</p> <blockquote class="block-tip"> <h4 id="definition-structured-masked-attention">Definition: Structured Masked Attention</h4> <p><strong>Structured masked attention (SMA)</strong> is defined as the <em>four-way tensor contraction</em> \eqref{eq:sma} using an attention mask $L$ that is a structured matrix.</p> </blockquote> <blockquote class="block-tip"> <h4 id="duality-representation-2-sma">Duality Representation 2 (SMA)</h4> <p>SMA has <strong>dual quadratic and linear</strong><d-footnote>Assuming that the structured matrix $L$ has linear time matrix-vector multiplication</d-footnote> <strong>modes</strong> which are simply <em>two different pairwise reduction orders</em> \eqref{eq:sma-quad} and \eqref{eq:sma-lin}.</p> </blockquote> <p>Finally, let‚Äôs just connect this back to the commonly held view of linear attention as matrix multiplication associativity.</p> <blockquote> <p>Although it is commonly believed that incorporating attention masks $L$ prevents matrix multiplication reordering, it turns out to still be compatible. In particular, <strong>associativity of matrix multiplication</strong> is a special case of <strong>tensor contraction reduction orders</strong>; although the former no longer applies, the latter can integrate the attention mask $L$.</p> </blockquote> <p>Next, let‚Äôs look at some consequences of the structured attention framework.</p> <h3 id="deriving-the-duality-attention-to-ssm">Deriving the Duality: Attention to SSM</h3> <p>Recall that the SSD model is defined as either a scalar-identity SSM in equation \eqref{eq:ssm}, or through the attention-like form in equation \eqref{eq:ssd-attention}.</p> <p>To show the equivalence of these forms, we simply recognize that \eqref{eq:ssd-attention} is a special case of structured masked attention where the mask matrix is</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix} .\] <p>\begin{equation} \label{eq:1-ss} (\text{1-semiseparable (1-SS) matrix}) \end{equation}</p> <p>We call this a <strong>1-semiseparable (1-SS) matrix</strong>, for reasons that are explained in more detail in the Mamba-2 paper.</p> <p>Thus, we can also say that the SSD model is <strong>1-semiseparable masked attention</strong> or <strong>1-SS SMA</strong>.</p> <p>To prove that this can be written as an SSM, we simply appeal to the SMA framework, which says that the dual form of this model can be computed through matrix multiplication by $L$. So how fast is that? It‚Äôs not too hard to see that multiplication $y = Lx$ can be computed in linear time through a scalar recurrence:</p> \[\begin{aligned} y_0 &amp;= x_0 \\ y_1 &amp;= a_1 x_0 + a_1 \\ y_2 &amp;= a_2a_1 x_0 + a_2 x_1 + x_2 = a_2 y_1 + x_2 \\ \vdots &amp; \qquad \vdots \end{aligned}\] <p>This corresponds exactly to the original SSM recurrence!</p> <p>(In fact, multiplication by 1-SS matrices $L$ can be computed in a <em>lot</em> more ways, which we compile in the full paper! Alternative algorithms can reveal more insights: for example, the associative scan algorithm used by S5 <d-cite key="smith2023s5"></d-cite> and Mamba can also be shown to be a structured matrix multiplication algorithm on 1-SS matrices.)</p> <h3 id="going-beyond-the-ssd-layer-2">Going Beyond the SSD Layer 2</h3> <p>Structured masked attention not only helps define the SSD model and prove its duality, but it is a much broader framework of efficient attention models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/sma-480.webp 480w,/assets/img/2024-05-31-mamba-2/sma-800.webp 800w,/assets/img/2024-05-31-mamba-2/sma-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/sma.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prior examples include the original linear attention as well as the recent Retentive Network (RetNet) model<d-cite key="sun2023retentive"></d-cite>. These can be viewed as direct special cases of SSD. But beyond SSD, we can define classes of efficient attention by replacing the mask $L$ with <em>any structured matrix</em>. As a suggestion, we think that Toeplitz or Fourier structured attention may be interesting to consider because they might encode different forms of positional information.</p> <p>Additionally, other forms of structure can be incorporated into the $L$ mask. For example, another extension my students are developing is viewing SSD (and recurrences in general) as an algorithm operating on <em>directed line graphs</em>, and generalizing it to incorporate arbitrary graph structures.</p> <h2 id="state-space-duality">State Space Duality</h2> <p>We‚Äôll end this post with a brief recap of what we‚Äôve covered.</p> <p>The <strong>SSD framework</strong> consists of the two broad approaches covered in this post, which is summarized by the two areas of the [<a href="#the-state-space-duality-framework">Venn diagram</a>]:</p> <ol> <li>Viewing state space models through [<a href="#ssd-framework-1-structured-matrix-transformations">structured matrix transformations</a>]</li> <li>Generalizing linear attention through [<a href="#ssd-framework-2-structured-attention">tensor contractions</a>]</li> </ol> <p>The [<a href="#recap-the-ssd-model">SSD layer</a>] is a particular model which is the purple intersection in the figure, which can be viewed as an instance of either part of the SSD framework, and in particular has dual quadratic and linear forms that can be derived from either representation.</p> <table> <thead> <tr> <th><em>SSD Framework</em></th> <th>Structured SSMs</th> <th>Structured Attention</th> </tr> </thead> <tbody> <tr> <td>The main representation is‚Ä¶</td> <td>Structured matrix \eqref{eq:ssm-matrix} <br/> sequence transformations</td> <td>The 4-way \eqref{eq:sma} <br/> tensor contraction</td> </tr> <tr> <td>This generalizes‚Ä¶</td> <td>State space models</td> <td>Linear attention</td> </tr> <tr> <td>The SSD model is <br/> an instantiation as‚Ä¶</td> <td>Scalar state space model <br/> ($A_t$ is a scalar-identity matrix)</td> <td>1-semiseparable masked attention <br/> ($L$ mask is a 1-SS matrix)</td> </tr> <tr> <td>The linear-quadratic duality is <br/> revealed through‚Ä¶</td> <td>Structured matrix <br/> multiplication algorithms</td> <td>Tensor contraction <br/> reduction orderings</td> </tr> </tbody> </table> <h2 id="next-up">Next Up</h2> <p>In <a href="/blog/2024/mamba2-part3-algorithm/">the next part of this series</a>, we‚Äôll see how to use some of the SSD framework (in particular, the <a href="#takeaway-computing-ssms">structured matrix algorithm</a> point of view) to derive the more efficient hybrid SSD algorithm that leverages both of the dual forms.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part III - The Algorithm</title><link href="https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part III - The Algorithm"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part3-algorithm</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li>Part III - The Algorithm</li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>The theoretical framework of structured state space duality (see <a href="/blog/2024/mamba2-part1-model/">Part I</a> and <a href="/blog/2024/mamba2-part2-theory/">Part II</a> of this series) connects SSMs and (linear) attention through structured matrices. As mentioned in Part I, this connection allows us to derive new algorithms for selective SSMs that are faster than the parallel associative scan in Mamba-1 by leveraging matrix multiplication as a primitive. Moreover, the connection can bring system optimizations (e.g. tensor parallelism, sequence parallelism, variable sequence length) originally developed for Transformer to SSM-land.</p> <h2 id="the-ssd-algorithm">The SSD Algorithm</h2> <p>Even though we already developed optimized scans implementations for Mamba-1, we were limited to small state expansion (typically $\mathtt{N}=16$) as the algorithm and implementation did not use tensor cores (specialized hardware units that perform matrix multiplication). Typically matrix multiplication (matmul) FLOPs are much faster (up to 16x) than non-matmul FLOPs: the A100 GPU has 312 TFLOPS of BF16 matmul but only 19 TFLOPS of FP32 arithmetics, and the H100 has 989 TFLOPS of BF16 matmul but only 67 TFLOPS of FP32 arithmetics. One of our primary goals with Mamba-2 is to <strong>leverage tensor cores to speed up the SSM</strong>.</p> <p>To recap, after tying parameters and introducing the head structure, the SSM in Mamba-1 turns into SSD, a more restrictive form that has an attention-like formulation. And as SSD connects SSMs and structured matrices, we saw in Part II that efficient algorithms to compute SSMs correspond directly to different decompositions of the ‚Äútoken-mixing‚Äù or ‚Äúsequence-mixing‚Äù matrix $M$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_algorithm-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_algorithm.png" width="100%" height="auto" title="SSD Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can therefore create new algorithms to compute SSMs simply by looking for alternative ways to multiply this matrix, for example by decomposing it in various ways. A simple block decomposition of this matrix, with carefully chosen block sizes, turns out to get all the advantages of both the linear-recurrent and quadratic-attention dual forms of SSD. This leads to the SSD algorithm, which has 4 steps. There are two completely different interpretations of this algorithm!</p> <h3 id="ssd-algorithm-block-matrix-decomposition">SSD Algorithm: Block Matrix Decomposition</h3> <p>We first partition the SSM (semiseparable) matrix into blocks of size $\mathtt{Q} \times \mathtt{Q}$. Then, we use the properties of semiseparable matrices to factorize each off-diagonal block, which is low rank.</p> <ol> <li>(<em>Orange</em>) Each diagonal block is a smaller semiseparable matrix; we can compute this multiplication however we like; in particular, using the quadratic (attention-like) form of SSD.</li> <li>(<em>Green</em>) There are only $\mathtt{T} / \mathtt{Q}$ total different green blocks because many of them are shared. These can be computed with a batched matmul.</li> <li>(<em>Yellow</em>) Notice that the yellow terms themselves form a 1-semiseparable matrix; in other words, this step is equivalently to an SSM scan (on some modified $A$ factors)!</li> <li>(<em>Blue</em>) Similar to green, these can be computed with a batched matmul.</li> </ol> <h3 id="ssd-algorithm-chunking-and-state-passing">SSD Algorithm: Chunking and State Passing</h3> <p>An alternative interpretation of the algorithm involves reasoning about how the SSM operates on the actual sequence. We first split the sequence of input into blocks (or chunks) of size $\mathtt{Q}$. The steps then have the interpretation</p> <ol> <li><strong>Intra-chunk outputs</strong>: compute the local output of each chunk (<em>what is the output per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Chunk states</strong>: compute the final state of each chunk (<em>what is the final state per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Pass states</strong>: compute a recurrence on all of the chunks‚Äô final states ‚Äì using any desired algorithm, e.g. parallel or sequential scan (<em>what is the actual final state per chunk taking into account all previous inputs?</em>)</li> <li><strong>Output states</strong>: for each chunk, given its true initial state (computed in Step 3), compute the contribution to the output just from the initial state</li> </ol> <p>Either way, we see that most of the algorithm (Step 1, 2, and 4) leverages matmuls (and hence tensor cores), and also can be computed completely in parallel! Only Step 3 requires a scan, but it operates on a much shorter sequence and usually only takes a small fraction of the time of the full algorithm.</p> <h3 id="special-cases">Special Cases</h3> <p>We note that special cases of this algorithm have been seen before. In particular RetNet<d-cite key="sun2023retentive"></d-cite>, which we showed in Part II to be a special case of SSD, mention a ‚Äúchunkwise‚Äù algorithm which computes the quadratic form on a chunk of the input one-at-a-time and passes the final state to the next chunk. This turns out to be essentially equivalent to the SSD algorithm specialized to a restricted case (i.e. a decay matrix mask $L$). Our derivation comes from a different direction‚Äîthe block matrix decomposition‚Äîwhich also makes it more obvious how to parallelize this algorithm and make it really fast in practice.</p> <p>Other forms of ‚Äúchunkwise‚Äù recurrences have recently become popular, such as in <a href="https://arxiv.org/abs/2312.06635">Gated Linear Attention (GLA)</a><d-cite key="yang2024gated"></d-cite>.</p> <h2 id="the-code">The Code</h2> <p>In the ‚ÄúMinimal SSD‚Äù code that we provide in the paper and the <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py">code release</a>, we delineate each of these four steps. As promised, this algorithm is not only faster but also much easier to implement than the original selective scan of Mamba, coming in at just around 25 lines of code!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>

<span class="k">def</span> <span class="nf">ssd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">block_len</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">initial_states</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">A</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">C</span><span class="p">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_len</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># Rearrange into blocks/chunks
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (c l) ... -&gt; b c l ...</span><span class="sh">"</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">block_len</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)]</span>

    <span class="n">A</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h -&gt; b h c l</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">A_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 1. Compute the output for each intra-chunk (diagonal blocks)
</span>    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="n">Y_diag</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bcshn,bhcls,bcshp-&gt;bclhp</span><span class="sh">"</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 2. Compute the state for each intra-chunk
</span>    <span class="c1"># (right term of low-rank factorization of off-diagonal blocks; B terms)
</span>    <span class="n">decay_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">((</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">A_cumsum</span><span class="p">))</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bhcl,bclhp-&gt;bchpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">decay_states</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
</span>    <span class="c1"># (middle term of factorization of off-diag blocks; A terms)
</span>    <span class="k">if</span> <span class="n">initial_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">initial_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">states</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">initial_states</span><span class="p">,</span> <span class="n">states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">decay_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))))</span>
    <span class="n">new_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhzc,bchpn-&gt;bzhpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">decay_chunk</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># 4. Compute state -&gt; output conversion per chunk
</span>    <span class="c1"># (left term of low-rank factorization of off-diagonal blocks; C terms)
</span>    <span class="n">state_decay_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">)</span>
    <span class="n">Y_off</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bclhn,bchpn,bhcl-&gt;bclhp</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">state_decay_out</span><span class="p">)</span>

    <span class="c1"># Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">Y_diag</span><span class="o">+</span><span class="n">Y_off</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h p -&gt; b (c l) h p</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">final_state</span>
</code></pre></div></div> <h2 id="the-details">The Details</h2> <p>Let‚Äôs talk about a couple of additional details in the implementation (these don‚Äôt even appear in the full paper, so pay attention!) that unpack some of the choices in this reference code.</p> <h3 id="the-ssm-scan">The SSM Scan</h3> <p>In the above code, we utilized the connection between scalar SSM recurrences</p> \[h_{t+1} = A_t h_t + B_t x_t\] <p>and matrix multiplication by 1-semiseparable matrices</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix}\] <p>which we covered in Part II (and Section 3.2.2 of the paper). In this minimal implementation, we compute Step 3 of the algorithm, which is computing a scalar SSM by <em>any</em> algorithm of our choice, by explicitly materializing a 1-SS matrix and doing dense matrix multiplication.</p> <p>We use this version for several reasons:</p> <ol> <li>Code-wise, it‚Äôs simpler to materialize and multiply by this matrix than to actually implement a parallel associative scan</li> <li>Because of the block decomposition of the SSM matrix, the sequence length $\mathtt{T}$ is reduced by a factor of $\approx 100$ ‚Äì so doing the scan in time $O(\mathtt{T}^2)$ instead of $O(\mathtt{T})$ isn‚Äôt too bad</li> <li>We have to materialize a 1-SS matrix anyways for Step 1 of the algorithm (the diagonal blocks), so might as well reuse the code ¬Ø\_(„ÉÑ)_/¬Ø</li> </ol> <p>While this example code is simpler and reasonably efficient on GPU (and probably TPU as well!), it‚Äôs no longer truly linear at long sequences. Our more optimized Triton implementation does replace the 1-SS multiplication in Step 3 with an actual associative scan.</p> <h3 id="stability">Stability</h3> <h4 id="attempt-1-ratios-of-cumprods">Attempt 1: Ratios of cumprods</h4> <p>The first naive attempt may be to notice that the entries of this matrix are cumulative products</p> \[a_{i:j}^\times = a_i \times \cdots \times a_{j-1} = \frac{a_{i:\mathtt{T}}^\times}{a_{j:\mathtt{T}}^\times}\] <p>However, this runs into severe numerical issues because these products can get really tiny (imagine $a_t \approx 0.9$ and powering it up for a sequence length $\mathtt{T}$ in the thousands!)</p> <h4 id="fix-1-the-segment-sum-segsum-operation">Fix 1: The Segment Sum (<code class="language-plaintext highlighter-rouge">segsum</code>) Operation</h4> <p>The second attempt would be to do all of this in log-space, because all the $a_t$ are positive; so the products become additions, and instead of <code class="language-plaintext highlighter-rouge">cumprod</code>s to deal with we have <code class="language-plaintext highlighter-rouge">cumsum</code>s instead. Then in order to compute the 1-SS matrix, we just have to compute the sums $\log a_i + \dots + \log a_{j-1}$ for every <em>segment</em> $[i:j]$. We call this the <strong>segment sum (segsum)</strong> primitive, analogous to cumulative sum (cumsum).</p> <h4 id="attempt-2-differences-of-cumsums">Attempt 2: Differences of cumsums</h4> <p>The obvious way to do this again is using the same idea as above, but in log space</p> \[a_{i:j}^\times = \exp\left( \log a_i + \cdots + \log a_{j-1} \right) = \left( (\log a)_{i:\mathtt{T}}^+ - (\log a)_{j:\mathtt{T}}^+ \right)\] <p>where we compute a single cumulative sum of $a$ along the time axis, and then compute all pairwise differences. In code, we can do this with</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum_unstable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>
</code></pre></div></div> <p>(and then the 1-semiseparable matrix is just the exponential of this output).</p> <p>Sums/differences are a lot more stable than products/quotients, so this should work ‚Äì right?</p> <h4 id="fix-2-remove-all-subtractions">Fix 2: Remove All Subtractions</h4> <p>Unfortunately, it turns out this still doesn‚Äôt work. The values of this 1-SS matrix roughly represent the SSM dynamics, which are very sensitive to these values of $a_t$, so we have to be very precise. And even in log space, these cumsums can be fairly large, which runs into <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation">catastrophic cancellation</a> when subtracted. So we really have to find a way to compute this matrix with only additions, while still vectorizing everything‚Ä¶</p> <h4 id="attempt-3-stable-segsum">Attempt 3: Stable Segsum</h4> <p>This leads to the helper function in the reference SSD code. Instead of computing a single cumsum and then subtracting, we find a way to use a batch of independent cumsums that immediately produces the right answer without subtraction.</p> <p>These details do matter! Without the right implementation of these primitives, the basic SSD algorithm produces NaNs immediately during training (even with FP32).</p> <h3 id="discretization">Discretization</h3> <p>This lineage of structured state space models developed from <a href="https://arxiv.org/abs/2111.00396">S4</a> and <a href="https://arxiv.org/abs/2110.13985">its</a> <a href="https://arxiv.org/abs/2008.07669">predecessors</a> which were viewed as continuous-time systems.<d-cite key="gu2023thesis"></d-cite><d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2021combining"></d-cite><d-cite key="gu2020hippo"></d-cite></p> <p>In Mamba, however, we don‚Äôt really view the SSM as continuous anymore. In fact, as mentioned in the Discussion (Section 5) of the <a href="https://arxiv.org/abs/2312.00752">original paper</a>, Mamba trades off with S4 on modeling different types of data:</p> <ul> <li>S4 is a continuous-time model that excels at modeling continuous data, e.g. perceptual signals such as audio waveforms and pixel-level vision.</li> <li>Mamba is a discrete-time model that excels at modeling discrete data, e.g. tokenized data such as language.</li> </ul> <p>However, the parameterization of Mamba still used the same discretization step as in prior structured SSMs, where there is another parameter $\Delta$ being modeled. We do this because the discretization step has other side effects such as properly normalizing the activations <d-cite key="gu2023train"></d-cite><d-cite key="orvieto2023resurrecting"></d-cite> which is important for performance.</p> <p>The initializations and parameterizations from the previous <a href="https://arxiv.org/abs/2206.12037">theory on structured SSMs</a> still work out-of-the-box, so why fix what‚Äôs not broken?</p> <p>Despite this, we‚Äôre pretty sure that the discretization step isn‚Äôt really necessary for Mamba. In the Mamba-2 paper, we chose to work directly with the ‚Äúdiscrete parameters‚Äù $A$ and $B$, which in all previous structured SSM papers (including Mamba-1) were denoted $(\bar{A}, \bar{B})$ and defined through an additional transformation</p> \[\begin{align*} \bar{A} &amp;= \exp(e^{\Delta A}) \\ \bar{B} &amp;= (\exp(e^{\Delta A}) - I) A^{-1} B \end{align*}\] <p>This doesn‚Äôt pose any problems: to use the continuous SSM parameterization, simply transform the parameters through the above formulas before plugging into the SSD code above.</p> <p>In the full Mamba-2 code, we also kept the same parameterization and discretization step as in Mamba‚Äîagain, why fix what‚Äôs not broken?‚Äîbut hypothesize that ‚Äúdiscrete-centric‚Äù variants (such as the <em>gamma normalization</em> of <a href="https://arxiv.org/abs/2303.06349">LRU</a><d-cite key="orvieto2023resurrecting"></d-cite> and <a href="https://arxiv.org/abs/2402.19427">Griffin</a><d-cite key="de2024griffin"></d-cite>) should work equally well.</p> <blockquote class="block-tip"> <h4 id="is-discretization-necessary">Is Discretization Necessary?</h4> <p>It‚Äôs useful for other structured SSMs, but perhaps not needed for Mamba. But it‚Äôs just a simple invertible transformation, so use either discrete or continuous parameterizations as you like!</p> </blockquote> <h2 id="whats-next">What‚Äôs Next</h2> <p>In the <a href="/blog/2024/mamba2-part4-systems/">final part of this series</a>, we‚Äôll continue talking about the implementation of Mamba-2, but on a more macroscopic level; about the entire neural network, instead of just details of the core SSD layer.</p> <p>We‚Äôll also talk about the actual speed of the algorithm covered in this post.</p>]]></content><author><name>Tri Dao</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part IV - The Systems</title><link href="https://goombalab.github.io/blog/2024/mamba2-part4-systems/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part IV - The Systems"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://goombalab.github.io/blog/2024/mamba2-part4-systems</id><content type="html" xml:base="https://goombalab.github.io/blog/2024/mamba2-part4-systems/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li>Part IV - The Systems</li> </ol> <p>Transformers have benefited from 7 years of systems optimization from the whole research community and large companies. The SSD framework draws connections between SSMs and attention, and allows us to implement many of these optimizations for models like Mamba-2 as well. We focus on tensor parallel and sequence parallel for large-scale training, as well as variable-length sequences for efficient finetuning and inference.</p> <h2 id="systems-and-scaling-optimizations">Systems and Scaling Optimizations</h2> <h3 id="tensor-parallelism">Tensor Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_tp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_tp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_tp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_tp.png" width="100%" height="auto" title="Mamba-2 Tensor Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>One difficulty with large-scaling training of Mamba-1 using tensor parallelism (TP) is that it requires 2 all-reduces per layer, compared to just 1 all-reduce per attention or MLP layer in Transformer. This is because some of the SSM parameters are functions of the inner activations, not of the input to the layer. In Mamba-2, with the ‚Äúparallel projection‚Äù structure, all SSM parameters are functions of the input to the layer, and we can easily apply TP to the input projection: We split the input projection and output projection matrices into 2, 4, 8 shards, depending on the TP degree. We use a grouped norm with number of groups divisible by the TP degree, so that normalization is done separately per GPU. These changes result in 1 all-reduce per layer, instead of 2.</p> <h3 id="sequence-parallelism">Sequence Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_cp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_cp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_cp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_cp.png" width="100%" height="auto" title="Mamba-2 Sequence Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When training on very long sequence length, we might need to split along the sequence length and assign different parts to different devices. There are two main forms of sequence parallelism (SP): For the residual and normalization operation: this replaces the all-reduce in TP with a reduce-scatter, residual + normalization, then all-gather. Since Mamba-2 uses the same residual and normalization structure as Transformer, this form of SP applies directly with no modification. For the attention or SSM operation, aka context parallelism (CP). For attention, one could use Ring attention to split it up along the sequence dimension. For Mamba-2, the SSD framework comes to our help once again: using the same block decomposition, we can have each GPU computing its local output and its final states, then pass the states between GPUs (using send/receive communication primitives), before updating the final output of each GPU.</p> <h3 id="variable-length">Variable Length</h3> <p>For finetuning and inference, in the same batch we often have sequences of different lengths. For Transformer, one would usually pad so all sequences have the same length (wasting computation), or implement attention specifically for variable length sequences with careful load-balancing. With SSM, we can simply treat the whole batch as a long ‚Äúsequence‚Äù, and avoid passing the states between different sequences in the batch by setting the state transition $A_t$ to 0 for tokens at the end of each sequence.</p> <h2 id="results">Results</h2> <p>How well do these optimizations work? The faster SSD algorithm allows us to increase the state dimension ($\mathtt{N}=64$ or $128$ compared to $\mathtt{N}=16$ in Mamba-1). Even though technically Mamba-2 is more restricted than Mamba-1 for the same $\mathtt{N}$, the larger state dimensions generally improve model quality. Here we show results for models trained on 300B tokens on the Pile, with Mamba-2 outperforming Mamba-1 and Pythia.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_lm_downstream-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/blog_lm_downstream.png" width="100%" height="auto" title="Downstream Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Standard downstream evaluations for open source models trained on the Pile</figcaption> </figure> <p>What about <strong>hybrid models</strong>? We have seen from recent and concurrent work (such as <a href="https://arxiv.org/abs/2403.19887">Jamba</a> and <a href="https://arxiv.org/abs/2405.16712">Zamba</a>) that combining Mamba layers with attention layers can improve over pure Transformer or Mamba. We validate at 2.7B parameters and 300B tokens scale that a hybrid model with just 6 attention blocks (and 58 SSD blocks) outperforms 64 SSD blocks, as well as our standard Transformer++ baseline (32 gated MLP and 32 attention blocks).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_hybrid-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_hybrid-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_hybrid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/blog_hybrid.png" width="100%" height="auto" title="Downstream Evaluations for Hybrid Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Downstream evaluations for hybrid Mamba/attention models</figcaption> </figure> <p>We also validated that the SSD algorithm is significantly faster than the selective scan algorithm from Mamba-1 for the same state dimension, and scales much better computationally to larger state dimensions. Getting those tensor cores to go brrr is the key!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate.png" width="100%" height="auto" title="Mamba-2 Efficiency Benchmarks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Efficiency benchmarks on sequence length 2K</figcaption> </figure> <h2 id="future-directions">Future Directions</h2> <p>With SSD, we have connected (linear) attention and SSMs, allowing us to design faster algorithms and implement systems optimizations for SSMs. There are still tons of exciting directions that we (and hopefully the community) want to tackle:</p> <ul> <li><strong>Understanding</strong>: hybrid models with a few (4-6) attention layers perform very well, even better than pure Mamba(-2) or Transformer++. What are these attention layers doing? Can they be replaced with another mechanism?</li> <li><strong>Training optimizations</strong>: though SSD might be faster than attention, Mamba-2 as a whole might still be slower than Transformers at short (e.g. 2K) sequence length, since the MLP layers in Transformers are very hardware-friendly. Our implementation of SSD does not specifically take advantage of new features on H100 GPUs, and we look forward to future optimizations that could make SSMs faster to train than Transformers for large-scale pretraining at 2-4K sequence length.</li> <li><strong>Inference optimizations</strong>: there‚Äôs a whole suite of optimizations tailored to Transformers, in particular handling the KV cache (quantization, speculative decoding). How would the inference landscape change if model states (e.g. SSM states) no longer scale with context length, and KV cache is no longer the bottleneck?</li> </ul>]]></content><author><name>Tri Dao</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry></feed>