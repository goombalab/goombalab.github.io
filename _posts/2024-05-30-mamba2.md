---
layout: distill
title: State Space Duality (Mamba-2) Part 1 - The Model
description: 
tags:
giscus_comments: true
date: 2024-05-27
featured: true

authors:
  - name: Albert Gu
    url:
    affiliations:
      name: Carnegie Mellon University
  - name: Tri Dao
    url:
    affiliations:
      name: Princeton

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Equations
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: The SSD Model
  - name: Code

---


Since the release of [Mamba](https://arxiv.org/abs/2312.00752) 6 months ago, we've been surprised and overwhelmed by the [community response](https://github.com/AvivBick/awesome-ssm-ml).
It's been incredibly gratifying to see the line of research on efficient sequence models we've been pursuing for years really resonate with the machine learning community and take off more than we could have anticipated.
We've seen an enormous amount of exciting follow-up work from direct applications
(e.g. vision <d-cite key="zhu2024vision"></d-cite><d-cite key="ma2024u"></d-cite><d-cite key="liu2024vmamba"></d-cite>, genomics <d-cite key="schiff2024caduceus"></d-cite>, graphs <d-cite key="wang2024graph"></d-cite><d-cite key="behrouz2024graph"></d-cite>, and more)
to understanding (e.g. on recall abilities <d-cite key="jelassi2024repeat"></d-cite>,
in-context learning<d-cite key="akyurek2024context"></d-cite> <d-cite key="grazzi2024mamba"></d-cite> <d-cite key="park2024can"></d-cite>,
and formal language expressivity <d-cite key="merrill2024illusion"></d-cite><d-cite key="sarrof2024expressive"></d-cite>),
and an enormous number of [online](https://jackcook.com/2024/02/23/mamba.html) [blogs](https://srush.github.io/annotated-mamba/hard.html),
[tutorials](https://www.youtube.com/watch?v=dVH1dRoMPBc),
[and](https://www.youtube.com/watch?v=8Q_tqwpTpVU),
[videos](https://www.youtube.com/watch?v=N6Piou4oYx8).
We couldn't be more excited about the direction of this research!


Yet despite its promise so far, we weren't completely satisfied with the first version of Mamba...

### Problem 1 (Understanding):
From a conceptual standpoint, one of the reasons we found SSMs so fascinating is how they just feel _fundamental_. One way this is exemplified is how they have rich ties to many major paradigms of sequence models.
As developed in our earlier works on structured SSMs <d-cite key="gu2021combining"></d-cite><d-cite key="gu2023thesis"></d-cite>, they seem to capture the essence of continuous, convolutional, and recurrent sequence models -- all wrapped up in a simple and elegant model.

But of course, aside from these, there is another major sequence model paradigm: the variants of the ubiquitous **attention** mechanism<d-cite key="bahdanau2015neural"></d-cite><d-cite key="vaswani2017attention"></d-cite>.
SSMs always felt somewhat disjoint from attention, and we've tried for a while to understand their relationship better.

> Question 1: **What are the conceptual connections between SSMs and attention?**

### Problem 2 (Efficiency):
From a computational standpoint, 
despite the work that went into making Mamba fast (in particular, its hardware-aware selective scan implementation) it is still much less hardware-efficient than mechanisms such as attention.
The missing piece is that modern accelerators such as GPUs and TPUs are *highly* specialized for matrix multiplications.
While this is not a problem for inference, which is bottlenecked by somewhat different considerations, this can be a big deal during training time.
For example, an end-to-end Mamba-1 model is XX times slower than an equivalent Transformer.

> Question 2: **Can we speed up the training of Mamba models by recasting them as matrix multiplications?**

These are the main questions that Mamba-2 -- in particular, its new state space model variant -- tries to address.


{% details Some conventions in this post %}
We'll use green boxes to highlight takeaways

Yellow boxes will be for more mathematical definitions and results
{% enddetails %}

## The SSD (State Space Dual) Model

SSD refers to both a general framework, as well as a specific model.
The **state space dual model** itself really isn't so scary -- in this first part of the post, we'll provide a self-contained description of the SSD model (and Mamba-2) in isolation and how it compares to related models, particularly Mamba-1.

In [the next part](), we'll describe the general framework and theoretical connections, which aren't necessary to actually use Mamba-2.

### The Linear (SSM) Mode

SSD starts from the same set of equations as Mamba:

$$
\begin{aligned}
h_{t} &= A_t h_{t-1} + B_t x_t \\
y_t &= C_t^{\top} y_t
\end{aligned}
$$

\begin{equation}
\label{eq:ssm}
(\text{Selective state space model (SSM)})
\end{equation}

<d-cite key="bahdanau2015neural"></d-cite>



To recap, a structured state space model (SSM) <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> defines a map from $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$.
Think of $x_t$ and $y_t$ as being scalars, and the hidden state $h_t$ as an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the state size, state dimension, or state expansion factor.
A *selective* state space model allows the $A, B, C$ SSM parameters to vary across time <d-cite key="gu2023mamba"></d-cite>.
We'll think of them as tensors with shapes $\mathtt{(T, N, N)}$, $\mathtt{(T, N)}$, and $\mathtt{(T, N)}$ respectively.

Structured SSMs require $A$ to have structure to be efficiently computable, such as the most commonly used diagonal structure <d-cite key="gu2022parameterization"></d-cite><d-cite key="gupta2022diagonal"></d-cite><d-cite key="smith2023s5"></d-cite><d-cite key="gupta2022simplifying"></d-cite>.
In this case $A$ has shape $\mathtt{(T, N)}$ where only the diagonal elements of the $\mathtt{N} \times \mathtt{N}$ matrices are stored.

#### SSD: Scalar Structured SSM
The original Mamba (or more precisely its core "S6" layer) is exactly a selective SSM with diagonal structure. 
**The SSD layer of Mamba-2 makes only one simple modification**: it restricts the diagonal $A$ even further to a *scalar times identity* structure; in other words the diagonal elements of $A$ must all be the same.
In this case $A$ can be represented with shape just $\mathtt{(T)}$ and one can also identify $A_t$ as just a scalar (and thus we will sometimes denote it $a_t$).

#### Multihead SSMs

Equation \eqref{eq:ssm} is defined only for a single dimensional input $x \in \mathbb{R}^\mathtt{T}$.
If $X \in \mathbb{R}^\mathtt{(T, P)}$ has $\mathtt{P}$ separate channels,
we can use the same dynamics (i.e. the same SSM $(A, B, C)$) independently for each channel.
This can be interpreted as a *single head* of the SSM model.

Here, we think of $X$ as a tensor of shape $\mathtt{(T, P)}$ where $\mathtt{T}$ is the sequence (time) dimension and $\mathtt{P}$ is the "head dimension".<d-footnote>Normally there's an additional batch dimension $\mathtt{B}$ when implementing these models, which we'll ignore throughout this presentation.</d-footnote>

Multiple heads can be constructed completely independently;
for the remainder of this post, we assume that we're working with a single head.
Note that these heads are exactly analogous to how heads in multi-head attention models work,
and in Mamba-2 we also choose similar dimensions, e.g. $\mathtt{P} = 64$ or $\mathtt{P}=128$.

We can notate the general (selective) state space model as
\begin{equation}
\label{eq:ssm-transformation}
Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T,...)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)})
\end{equation}

Axes of variation include the structure on $A$, which affects its parameter shape, and the state dimension $\mathtt{N}=\mathtt{d\\_state}$ and head dimension $\mathtt{P}=\mathtt{d\\_head}$.

#### Efficiency

The reason why SSMs are interesting is because computing it as a recurrence requires maintaining a *constant-size state* (size $\mathtt{N}$) and scales *linearly in the sequence length* $\mathtt{T}$.
But as mentioned above, the raw FLOPs don't reflect actual speed in practice...


### The Quadratic (Attention) Mode

Let's switch tacks and forget about state space models for a moment.
Given the same $(A, B, C)$ tensors above with the same shapes $(\mathtt{T})$, \mathtt{(T, N)}$, and \mathtt{(T, N)}$,
let's define a different object.

First, we'll define the following matrix (don't worry, we'll explain more and give it a name in the second part of this post).

$$
  L =
  \begin{bmatrix}
    1 & \\
    a_1 & 1 & \\
    a_2a_1 & a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    a_{T-1}\dots a_1 & a_{T-1}\dots a_2 & \dots & a_{T-1} & 1 \\
  \end{bmatrix}
  .
$$

Then, let's define the following matrix

\begin{equation}
\label{eq:ssd-attention}
M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}}
\end{equation}

Finally, $M$ encodes a sequence transformation
$x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$
just as how we defined above \eqref{eq:ssm}.

What's special about this?
Well, you may notice that it looks very similar to an attention computation.
In fact, if all $a_t = 1$, then $L$ is simply the lower-triangular *causal mask* and \ref{eq:ssd-attention} is exactly **causal linear attention** :warning: :
$$
Y = (L \circ Q K^\top) V
$$

This is exactly the same as equation \eqref{eq:ssd-attention} if we rename $(C, B, X) \mapsto (Q, K, V)$!

#### Efficiency

## State Space Duality

The so-called "duality" refers to the fact that the two models defined in XX and XX are in fact *exactly the same model*, which is a particular function $(A, B, C, X) \mapsto Y$ with tensor shapes specified above.
We'll show this fact in two completely different ways, both of which are actually much more general and each quite illuminating.

If you take our word for it, though, then SSD is relatively simple to understand in contrast to either SSMs or attention.

### SSD vs. State Space Models
Compared to previous SSMs, SSD is pretty much the same as the core layer of Mamba but with even more structure on the recurrent $A$ matrices.
- Mamba (S6) uses diagonal structure on $A$ with a head dimension of $\mathtt{P}=1$.
- Mamba-2 (SSD) uses scalar-times-identity structure on $A$ with a head dimension of $\mathtt{P}>1$ (something like $\mathtt{P}=64$ by default).

In particular, this can be viewed as weight-tied in two ways:
- By restricting the diagonal structure of $A$ to scalar-times-identity, the scalar recurrence dynamics are tied across all $\mathtt{N}$ elements of the state space.
- These dynamics are also shared across all $\mathtt{P}$ channels of a given head.

In other words, a single SSM head has total state size $\mathtt{P} \times \mathtt{N)}$,
which are each governed by separate scalar recurrences in Mamba but are controlled by a single shared recurrence in Mamba-2.

Why make these restrictions? The main motivation is efficiency: these changes are necessary to be able to view the model in its [[dual attention form](#the-quadratic-attention-mode)], which allows matrix multiplications to be used.

> ##### The Bottom Line: Mamba-2 vs. Mamba-1
>
> Compared to Mamba-1, Mamba-2 allows **much larger state dimensions** (from $\mathtt{N}=16$ in Mamba-1 to $\mathtt{N}=64,$ or $\mathtt{N}=256$ or even higher in Mamba-2) while simultaneously being **much faster during training**.
{: .block-tip}

But can this hurt us? There's some intuition to believe that it shouldn't.
One of the main reasons for the selectivity (e.g. $A$ that depends on the input $X$) introduced in Mamba
is to let the SSM be able to control whether to remember or ignore particular pieces of information;
for example, if a filler "um" is encountered in a text transcript.
But if such information should be ignored, then the entire state can ignore it together, and so it should be okay if the state's dynamics are shared across all features.

Empirically, we haven't found evidence that the restricted expressivity of Mamba-2 might hurt, but the jury's still out!
From one perspective, Mamba-2 isn't *strictly* better than Mamba-1: while it's a dramatic improvement from a *training* perspective, Mamba-1 might be better from a pure *inference* perspective.
Since inference speed of SSMs is entirely governed by the state dimension, if one wants to maximize performance for a target inference efficiency (i.e. for a particular state size $\mathtt{N}$), then the increased expressivity of Mamba-1 might be better.
We haven't fully analyzed the (theoretical or empirical) tradeoffs here, and think this would be a cool direction for the community to dig in more.

### SSD vs. Attention

Compared, to standard (self-)attention, SSD also only has two differences:
1. The softmax normalization is dropped.
2. A separate elementwise mask matrix is applied multiplicatively.

The first difference can be interpreted as what reduces the effective state size of the model from infinite to finite, and improves its efficiency from quadratic to linear.

The second difference is what distinguishes SSD from standard linear attention.
One way to think of the mask is as **input-dependent relative positional encodings**.
Because of the mask (definition :warning:), the standard attention score $Q_i K_j$ is attenuated by a score $a_{i:j}^\times = a_i \dots a_{j+1}$ which can be interpreted as a discount factor based on how far apart the positions $i$ and $j$ are. <d-footnote>This interpretation was concurrently espoused by Tobias Katsch's GateLoop paper :warning:</d-footnote>
This is the key factor that encodes the "selectivity" of Mamba.

### Best of Both Worlds: the Hybrid Mode
Computationally, one can use either formulation to compute the model. Loosely speaking, the attention form is faster during training because it's dominated by matrix multiplications, while the SSM form is preferred during autoregressive inference.

In the next two sections [LINK], we'll present two broad frameworks with which to understand the state space dual model.
Each of them will both prove the equivalence of these two formulations, but each is much more general, and we'll discuss other consequences of the frameworks.

If you just want to use the model, stop here!
In the rest of this post, we'll give an overview of the theoretical aspects of the SSD framework.

## The Mamba-2 Architecture

Although the core contribution of Mamba-2 is the new SSD layer,
we also make some small changes to the architecture.

{% include figure.liquid loading="eager" path="assets/img/2024-06-01-mamba2/architecture_2.png" %}

The main change is producing the $(A, B, C)$ SSM parameters in parallel with the $X$ input, instead of sequentially.
This is partly motivated by the connections to attention;
but more pragmatically, it's simpler and more amenable to scaling techniques such as tensor parallelism, which will be covered in the next part of this series!

There are some other small differences which are covered in more detail in the paper.
However, we emphasize that these aren't the main point of the model.

### Language Modeling

In terms of empirical results, we didn't test Mamba-2 as extensively as Mamba-1, but believe it should generally be on par or better.
Our full language model results use the same protocol as Mamba, and found slightly better scaling both at Chinchilla laws :warning: (figure).

{% include figure.liquid loading="eager" path="assets/img/2024-06-01-mamba2/pile_8k_mamba2.png" %}

Fully trained models on the Pile dataset :warning: and the standard zero-shot downstream evaluations show similar trends.
We emphasize that even when the performance is comparable, Mamba-2 is *much* faster to train than Mamba-1!

### Synthetic Language Modeling: MQAR

More interestingly, we highlight the one synthetic task we tried.
Since the original Mamba paper, which investigated synthetics such as Synthetic Copying and Induction Heads,
many follow-up works have begun investigating harder associative recall tasks.
The multi-query associative recall (MQAR) task introduced by the Zoology and Based :warning: line of work
has become a de facto standard.

{% include figure.liquid loading="eager" path="assets/img/2024-06-01-mamba2/mqar.png" %}

We ran a version of this task that's much harder than the one usually reported in the literature,
and found that Mamba-2 is substantially better than Mamba-1.
One reason for the improved performance is the much larger state size (up to $16\times$ larger than Mamba-1 here),
which was one of the primary motivations of Mamba-2 in the first place.

Interestingly, Mamba-2 also appears to be noticeably better than Mamba-1 on this particular task even when the state size is controlled.
We're not quite sure why to be honest, and it would be great to ablate the other aspects of the architecture to investigate...

## What's Next?

[AG: perhaps include a section to shout out related work and directions?]

---
---

# State Space Duality (Mamba-2) Part 1.5 - The Theory

Header here.... don't need to read

## SSD Framework 1 (Structured Matrix Transformations)

The first framing of the duality will be from an SSM-centric perspective, where we'll prove the duality through the framework of **matrix sequence transformations** or "matrix mixers".

### Matrix Transformations
The idea is that many sequence models, i.e. sequence transformations $X \in \mathbb{R}^\mathtt{(T,P)} \mapsto Y \in \mathbb{R}^\mathtt{(T,P)}$,
can be written in the form of a single matrix multiplication $Y = M(X) \cdot X$ where $M$ is a matrix which can itself depend on $X$.
We call this a matrix sequence transformation, or matrix transformation for short.
In the literature sequence transformations have also been referred to as "sequence mixers", and matrix sequence transformations as matrix mixers.
There are many examples of these, which are distinguished by the structure of the $M$ matrix:
- In self-attention, $M = \mathsf{softmax}(QK^\top)$ is the attention matrix
- MLP-Mixer :warning: : $M$ is an unstructured dense matrix
- Monarch Mixer: 

Why do we care about these types of models?
> Writing a sequence model as a matrix transformation provides a powerful tool to understand the structure and characteristics of the model.

And although general non-linear RNNs such as LSTMs *cannot* be written as matrix mixers,
state space models can!
In fact, this is pretty easy to see by just unrolling the definition of the SSM recurrence.
The upshot is that the SSM \eqref{eq:ssm-transformation} can be written as a matrix transformation

$$
Y = \mathsf{SSM}(A, B, C)(X) = MX
$$

where $M_{ij} = 0$ for $i < j$ (i.e. it's lower triangular)
and otherwise
\begin{equation}
\label{eq:semiseparable}
M_{ij} = C_i^\top A_{i:j}^\times B_j := C_i^\top A_i \dots A_{j+1} B_j
\end{equation}

[//]: # $$
[//]: # M_{ij} =
[//]: # \begin{cases}
[//]: #   C_i^\top A_{i:j}^\times B_j := C_i^\top A_i \dots A_{j+1} B_j & i \ge j \\
[//]: #   0 & i < j
[//]: # \end{cases}
[//]: # $$

Drawing it out, this matrix looks like
$$
  \begin{bmatrix}
    C_0^\top B_0 & \\
    C_1^\top A_1 B_0 & C_1^\top B_1 & \\
    C_2^\top A_2A_1 B_0 & C_2^\top A_2 B_1 & C_2^\top B_2 \\
    \vdots & \vdots & \ddots & \ddots \\
    C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_1 B_0 & C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_2 B_1 & \dots & C_\mathtt{T}^\top A_{\mathtt{T}-1} B_{\mathtt{T}-2} & C_\mathtt{T}^\top B_{\mathtt{T}-1} \\
  \end{bmatrix}
$$

### Semiseparable Matrices

This type of matrix in fact has a name: it's called a (triangular) **semiseparable** matrix,
and has been studied in other fields of engineering and computational linear algebra :warning:.
These matrices are (IMO) quite fundamental and beautiful,
and the full paper talks about more of their properties.

[//]: # The power of writing state space models as matrix transformations is that 

For our purposes, we'll care about this form mainly for the algorithmic considerations.
One of the central messages of this SSD paper is that:

> All algorithms for computing state space models can be viewed as structured matrix multiplication algorithms on semiseparable matrices
{: .block-tip}

Let's see an easy instantiation of this, focusing on our main objective!

### Deriving the SSD Model's Duality (SSM →  Attention)

To show that equation \eqref{eq:ssd-attention} follows from equation \eqref{eq:ssm} (in the case of the SSD model, i.e. scalar SSM), we directly use the matrix form of the state space model \eqref{eq:semiseparable}.
Because the $A_t$ are all scalars in this case, they can be factored out of the entries

$$
C_i^\top A_{i:j}^\times B_j = A_{i:j}^\times \cdot (C_i^\top B_j)
$$

which directly implies equation \eqref{eq:semiseparable}.

In summary:

> The duality for the SSD model can be seen as two **different matrix multiplication algorithms** on the semiseparable matrix.
{: .block-tip}

- The linear form is a *structured matrix multiplication algorithm* that computes the outputs $Y_0, Y_1, \dots$ sequentially, leveraging the structure of the semiseparable matrix.
- The quadratic form is the *naive matrix multiplication algorithm* that materializes the quadratic matrix.

### Going Beyond SSD

The power of the semiseparable matrix representation applies to *all* state space models,
with various downstream implications.

#### Algorithms

Algorithmically, the Mamba-2 paper explores several consequences, such as:
1. The above duality result for the SSD model, i.e. scalar-identity SSM.
2. New asymptotic efficiency results for state space models (Theorem XXX :warning:), which follow directly from applying known results from the semiseparable matrix literature :warning:.
3. A more general hybrid algorithm that can be viewed as combining both the linear and quadratic forms to get the best of both worlds. This can be derived as a new matrix multiplication algorithm utilizing *block decompositions* of the semiseparable matrix. This is the subject of Part 2 of this blog post!

#### Understanding
Conceptually, the matrix transformation viewpoint helps provide a unifying view of sequence models.
Some example downstream ideas include:
- New sequence models: Restricting ourselves to matrix transformations reduces the problem of generating new sequence models to that of finding structured matrix classes with target properties. In ongoing work by my students, we study this point of view, and use it to derive the most natural bidirectional extension of Mamba (coming very soon!).
- Expressivity: Looking at the matrix transformation representation can help us understand what different models can represent from a linear algebraic perspective. In another ongoing work, we use this as a tool to study which subquadratic models are the most amenable to being distilled from Transformers.
- Interpretability: A concurrent work :warning: derived the matrix formulation of SSMs and use it to probe the internal representations of Mamba models.

We're excited to see what algorithmic and conceptual ideas from the structured matrix literature can be applied to further improve state space models!



## SSD Framework 2 (Structured Attention)

The second framing of the duality is from an attention-centric perspective, where we'll prove the duality through the framework of **tensor contractions**.

Note that this is entirely independent of the previous [[matrix transformation viewpoint](#ssd-framework-1-structured-matrix-transformations)].

### Warm-up: Kernel Attention

For our purposes, we'll define attention as a function

$$
(Q^\mathtt{(T,N)}, K^\mathtt{(S,N)} , V^\mathtt{(S,P)} ) \mapsto Y^\mathtt{(T,P)}
$$

given by the pairwise matrix multiplications

$$
Y = (QK^\top) \cdot V
$$

{% details On Dimensions %}
Think of $\mathtt{P} = \mathtt{N}$ as the head dimension; technically speaking, in attention the $V$ head dimension $\mathtt{P}$ can differ from the $QK$ head dimension $\mathtt{N}$.
Think of $\mathtt{T}$ as the *target* sequence dimension and $\mathtt{S}$ as the *source* sequence dimension.
Giving these two axes different names will make the math more clear and also covers more general forms of attention such as cross-attention, where the source and target are separate sequences with different lengths.
However, for our purposes we'll assume $\mathtt{S}=\mathtt{T}$ as in the self-attention setting.
{% enddetails %}


{% details Why can we assume this form? %}
The usual form of attention $Y = f(QK^\top) \cdot V$ (e.g. where $f$ is the softmax function)
can, for essentially all functions $f$<d-footnote>And up to some additional massaging such as row-wise normalization, which is easy to handle</d-footnote>,
be written as $Y = \psi(Q)\psi(K)^\top \cdot V$ for some appropriate feature map $\psi$ (which may be infinite dimensional).
In this case, we can simply redefine $Q \leftarrow \psi(Q)$ and define $\mathtt{N}$ to be the **feature dimension** of the attention kernel to begin with.
Softmax attention, for example, can be represented with a particularly infinite-dimensional feature map ($\mathtt{N}=\infty$) which represents the exponential kernel.
{% enddetails %}
We'll restrict ourselves to the case when $\psi$ is finite, which is sometimes called **kernel attention** and for which many many variants exist :warning:.

Why do we care about this?
When the feature dimension $\mathtt{N}$ is small---commonly, in the regime when $\psi$ is simple such as an elementwise transform, $\mathtt{N}$ is constant---and the sequence length $\mathtt{T}$ grows, then the cost of attention can be reduced from quadratic in $\mathtt{T}$ to linear.
This follows from simply computing the matrix multiplications in a different order

$$
Y = Q \cdot (K^\top V)
$$

> The most common way of linearizing attention is usually viewed as a consequence of the *associativity of matrix multiplication*

### (Causal) Linear Attention

However, once the basic kernel attention is slightly modified, we can no longer use the associativity of matrix multiplication directly.

The seminal **Linear Attention (LA)** paper by Katharopoulos et al. <d-cite key="katharopoulos2020transformers"></d-cite> shows that it can still be extended to the important case of incorporating causality into attention, for autoregressive settings such as language modeling.

Let's be a lot more explicit about how it works.
The quadratic form of **causal linear attention** is
\begin{equation}
\label{eq:quadratic-kernel-attention}
Y = (L \circ QK^\top) \cdot V
\end{equation}
where

$$
L =
\begin{bmatrix} 1 \\ \vdots & \ddots \\ 1 & \dots & 1 \end{bmatrix}
$$

is the **causal mask** matrix.

The issue is: Once the $L$ mask is incorporated into \eqref{eq:quadratic-kernel-attention}, we can no longer directly apply matrix associativity!
This is the problem that the original Linear Attention paper addresses.
What they show is that \eqref{eq:quadratic-kernel-attention} is equivalent to

$$
Y = Q \cdot \mathsf{cumsum}(K^\top V)
$$

As far as we're aware this wasn't explicitly proved in the paper, although it isn't hard to write out the summation to show it.

What we'll do is prove this in essentially one line, while revealing *exactly* where the "linear" part of Linear Attention comes from and how to strongly generalize it.

Spoiler:
> The appearance of the *cumulative sum* in linear attention is exactly equivalent to the fact that the causal mask $L$, as a matrix multiplication, encodes cumulative sums:
>
> $$y = L \cdot x \iff y = \mathsf{cumsum}(x)$$
{: .block-tip }

### A Tensor Contraction Proof of Linear Attention

Let's write out the quadratic form of linear attention \eqref{eq:quadratic-kernel-attention}
very explicitly in **tensor contraction** or [einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) notation, with shape annotations:


[//]: #    Q &= \mathsf{input} && \qquad \mathtt{(T,N)} \\
[//]: #    K &= \mathsf{input} && \qquad \mathtt{(S,N)} \\
[//]: #    V &= \mathsf{input} && \qquad \mathtt{(S,P)} \\

$$
  \begin{aligned}
    G &= \mathsf{contract}(\mathtt{TN, SN} \to \mathtt{TS})(Q, K) \\
    M &= \mathsf{contract}(\mathtt{TS, TS} \to \mathtt{TS})(G, L) \\
    Y &= \mathsf{contract}(\mathtt{TS, SP} \to \mathtt{TP})(M, V) 
  \end{aligned}
$$

\begin{equation}
\label{eq:sma-quad}
(\text{Structured Masked Attention - Quadratic Form})
\end{equation}


With this notation, we can notice that this sequence of contractions can be written as a *single four-way contraction*

\begin{equation}
  \label{eq:sma}
  y = \mathsf{contract}(\mathtt{TN},\mathtt{SN},\mathtt{SP},\mathtt{TS} \to \mathtt{TP})(Q, K, V, L)
  .
\end{equation}

And finally, it can be computed with any other contraction ordering. In particular,
we can perform pairwise reductions on the order $V, K, L, Q$ instead of $Q, K, L, V$

$$
  \begin{aligned}
    Z &= \mathsf{contract}(\mathtt{SP},\mathtt{SN} \to \mathtt{SPN})(V, K)  \\
    H &= \mathsf{contract}(\mathtt{TS},\mathtt{SPN} \to \mathtt{TPN})(L, Z) \\
    Y &= \mathsf{contract}(\mathtt{TN},\mathtt{TPN} \to \mathtt{TP})(Q, H)
  \end{aligned}
$$

\begin{equation}
\label{eq:sma-lin}
(\text{Structured Masked Attention - Linear Form})
\end{equation}

Now the key observation is that the second line of \eqref{eq:sma-lin} is simply a matrix multiplication by $L$,
which can be computed with a cumulative sum.

That's the entire proof of linear attention! The beauty of it is that we didn't have to write out a single summation, which was abstracted out into a particular contraction. [This proves Proposition :warning: (connect to cumsum remark)]
Moreover, this immediately reveals that the efficiency of linear attention can be made *much more general*...

:warning: [Restate the previous result about causal mask = cumsum]

### Structured Masked Attention

The critical observation is that in order for 
\eqref{eq:sma-lin} to be fast,
all that is necessary is for $L$ to be any *structured matrix* --
in other words any matrix that has subquadratic matrix-vector multiplication.

> **Structured masked attention (SMA)** is defined as the *four-way tensor contraction* \eqref{eq:sma} using an attention mask $L$ that is a structured matrix.
>
> SMA has **dual quadratic and linear**<d-footnote>Assuming that the structured matrix $L$ has linear time matrix-vector multiplication</d-footnote> **modes** which are simply *two different pairwise reduction orders* \eqref{eq:sma-quad} and \eqref{eq:sma-lin}.
{: .block-warning }


Finally, let's just connect this back to the commonly held view of linear attention as matrix multiplication associativity.

[LINK TO PLACE WHERE PREVIOUS TAKEAWAY BOX WAS DEFINED]
> Although it is commonly believed that incorporating attention masks $L$ prevents matrix multiplication reordering, it turns out to still be compatible.
>
> In particular, **associativity of matrix multiplication** is a special case of **tensor contraction reduction orders**;
> although the former no longer applies, the latter can integrate the attention mask $L$.
{: .block-info}


Next, let's look at some consequences of the structured attention framework.

### Deriving the SSD Model's Duality (Attention →  SSM)

Recall that the SSD model is defined as either a scalar-identity SSM in equation \eqref{eq:ssm},
or through the attention-like form in equation \eqref{eq:ssd-attention}.

To show the equivalence of these forms, we simply recognize that \eqref{eq:ssd-attention} is a special case of structured masked attention where the mask matrix is

$$
  L =
  \begin{bmatrix}
    1 & \\
    a_1 & 1 & \\
    a_2a_1 & a_2 & 1 \\
    \vdots & \vdots & \ddots & \ddots \\
    a_{T-1}\dots a_1 & a_{T-1}\dots a_2 & \dots & a_{T-1} & 1 \\
  \end{bmatrix}
  .
$$

\begin{equation}
\label{eq:1-ss}
(\text{1-semiseparable (1-SS) matrix})
\end{equation}

We call this a **1-semiseparable (1-SS) matrix**, for reasons that are explained in more detail in the Mamba-2 paper.

Thus, we can also say that the SSD model is **1-semiseparable masked attention** or **1-SS SMA**.

To prove that this can be written as an SSM, we simply appeal to the SMA framework, which says that the dual form of this model can be computed through matrix multiplication by $L$.
So how fast is that?
It's not too hard to see that multiplication by $L$ can be computed in linear time through a scalar recurrence:

$$
\begin{aligned}
y_0 &= x_0 \\
y_1 &= a_1 x_0 + a_1 \\
y_2 &= a_2a_1 x_0 + a_2 x_1 + x_2 = a_2 y_1 + x_2 \\
\vdots & \qquad \vdots
\end{aligned}
$$

This corresponds exactly to the original SSM recurrence!

(In fact, multiplication by 1-SS matrices $L$ can be computed in a *lot* more ways, which we compile in the full paper! Alternative algorithms can reveal more insights: for example, the associative scan algorithm used by S5 :warning: and Mamba can also be shown to be a structured matrix multiplication algorithm on 1-SS matrices.)


### Going Beyond SSD

Give a figure with many examples of structured matrices
- causal mask
- RetNet
- semiseparable
- Toeplitz
- Fourier

- test

Also mention the Graph idea

## State Space Duality (Redux)

We'll end this post with a brief recap of what we've covered.

{% include figure.liquid loading="eager" path="assets/img/2024-06-01-mamba2/ssd_venn.png" %}

The **SSD framework** consists of the two broad approaches covered in this post, which is roughly the union of the figure:
1. Viewing state space models through [[structured matrix transformations](#ssd-framework-1-structured-matrix-transformations)]
2. Viewing linear attention through [[structured masked attention](#ssd-framework-2-structured-attention)]

The [[SSD model](#the-state-space-dual-model)] is a particular model which is the purple intersection in the figure, which can be viewed as an instance of either part of the SSD framework, and in particular has dual quadratic and linear forms that can be derived from either representation.


| SSD Framework                                       | Structured Matrix Transformations                                  | Structured Attention                                              |
| -------------                                       | -----------                                                        | ----                                                              |
| Which sequence models does this focus on?           | State space models (SSM)                                           | Attention                                                         |
| The SSD model is an instantiation as...             | Scalar state space models <br> ($A_t$ is a scalar-identity matrix) | 1-semiseparable masked attention <br> ($L$ mask is a 1-SS matrix) |
| The linear-quadratic duality is revealed through... | Matrix multiplication algorithms                                   | Tensor contraction reduction orderings                            |

## Code

{% highlight python linenos %}

def test():
  return None

{% endhighlight %}

{% highlight python %}

def segsum(x):
    """Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM."""
    T = x.size(-1)
    x_cumsum = torch.cumsum(x, dim=-1)
    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
    return x_segsum

def ssd(X, A, B, C, block_len=64, initial_states=None):
    """
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    """
    assert X.dtype == A.dtype == B.dtype == C.dtype
    assert X.shape[1] % block_len == 0

    # Rearrange into blocks/chunks
    X, A, B, C = [rearrange(x, "b (c l) ... -> b c l ...", l=block_len) for x in (X, A, B, C)]

    A = rearrange(A, "b c l h -> b h c l")
    A_cumsum = torch.cumsum(A, dim=-1)

    # 1. Compute the output for each intra-chunk (diagonal blocks)
    L = torch.exp(segsum(A))
    Y_diag  = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", C, B, L, X)

    # 2. Compute the state for each intra-chunk
    # (right term of low-rank factorization of off-diagonal blocks; B terms)
    decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
    states = torch.einsum("bclhn,bhcl,bclhp->bchpn", B, decay_states, X)

    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
    # (middle term of factorization of off-diag blocks; A terms)
    if initial_states is None:
        initial_states = torch.zeros_like(states[:, :1])
    states = torch.cat([initial_states, states], dim=1)
    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
    new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
    states, final_state = new_states[:, :-1], new_states[:, -1]

    # 4. Compute state -> output conversion per chunk
    # (left term of low-rank factorization of off-diagonal blocks; C terms)
    state_decay_out = torch.exp(A_cumsum)
    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)

    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
    Y = rearrange(Y_diag+Y_off, "b c l h p -> b (c l) h p")
    return Y, final_state

{% endhighlight %}

Backticks:

```javascript
def segsum(x):
    """Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM."""
    T = x.size(-1)
    x_cumsum = torch.cumsum(x, dim=-1)
    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
    return x_segsum

def ssd(X, A, B, C, block_len=64, initial_states=None):
    """
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    """
    assert X.dtype == A.dtype == B.dtype == C.dtype
    assert X.shape[1] % block_len == 0

    # Rearrange into blocks/chunks
    X, A, B, C = [rearrange(x, "b (c l) ... -> b c l ...", l=block_len) for x in (X, A, B, C)]

    A = rearrange(A, "b c l h -> b h c l")
    A_cumsum = torch.cumsum(A, dim=-1)

    # 1. Compute the output for each intra-chunk (diagonal blocks)
    L = torch.exp(segsum(A))
    Y_diag  = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", C, B, L, X)

    # 2. Compute the state for each intra-chunk
    # (right term of low-rank factorization of off-diagonal blocks; B terms)
    decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
    states = torch.einsum("bclhn,bhcl,bclhp->bchpn", B, decay_states, X)

    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
    # (middle term of factorization of off-diag blocks; A terms)
    if initial_states is None:
        initial_states = torch.zeros_like(states[:, :1])
    states = torch.cat([initial_states, states], dim=1)
    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
    new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
    states, final_state = new_states[:, :-1], new_states[:, -1]

    # 4. Compute state -> output conversion per chunk
    # (left term of low-rank factorization of off-diagonal blocks; C terms)
    state_decay_out = torch.exp(A_cumsum)
    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)

    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
    Y = rearrange(Y_diag+Y_off, "b c l h p -> b (c l) h p")
    return Y, final_state
```

`<d-code>`:

<d-code block language="javascript">
def segsum(x):
    """Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM."""
    T = x.size(-1)
    x_cumsum = torch.cumsum(x, dim=-1)
    x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]
    mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0)
    x_segsum = x_segsum.masked_fill(~mask, -torch.inf)
    return x_segsum

def ssd(X, A, B, C, block_len=64, initial_states=None):
    """
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    """
    assert X.dtype == A.dtype == B.dtype == C.dtype
    assert X.shape[1] % block_len == 0

    # Rearrange into blocks/chunks
    X, A, B, C = [rearrange(x, "b (c l) ... -> b c l ...", l=block_len) for x in (X, A, B, C)]

    A = rearrange(A, "b c l h -> b h c l")
    A_cumsum = torch.cumsum(A, dim=-1)

    # 1. Compute the output for each intra-chunk (diagonal blocks)
    L = torch.exp(segsum(A))
    Y_diag  = torch.einsum("bclhn,bcshn,bhcls,bcshp->bclhp", C, B, L, X)

    # 2. Compute the state for each intra-chunk
    # (right term of low-rank factorization of off-diagonal blocks; B terms)
    decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))
    states = torch.einsum("bclhn,bhcl,bclhp->bchpn", B, decay_states, X)

    # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
    # (middle term of factorization of off-diag blocks; A terms)
    if initial_states is None:
        initial_states = torch.zeros_like(states[:, :1])
    states = torch.cat([initial_states, states], dim=1)
    decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))
    new_states = torch.einsum("bhzc,bchpn->bzhpn", decay_chunk, states)
    states, final_state = new_states[:, :-1], new_states[:, -1]

    # 4. Compute state -> output conversion per chunk
    # (left term of low-rank factorization of off-diagonal blocks; C terms)
    state_decay_out = torch.exp(A_cumsum)
    Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)

    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
    Y = rearrange(Y_diag+Y_off, "b c l h p -> b (c l) h p")
    return Y, final_state
</d-code>
