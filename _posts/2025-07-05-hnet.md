---
layout: distill
title: H-Net - The Past
description:
tags:
giscus_comments: false
date: 2025-07-05
featured: false
thumbnail: assets/img/2024-05-31-mamba-2/mamba-2-V3-transparent.png

authors:
  - name: Albert Gu
    url:
    affiliations:
      name: CMU, Cartesia AI

bibliography: albert.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Hierarchy is Everywhere
  - name: Hierarchical RNNs
  - name: S4 and SaShiMi
  - name: A Differentiable Chunking Module?
  - name: Information-based Chunking
    subsections:
      - name: 
      - name: 
      - name: 
  - name: Differentiable Selection with <br> Modern Techniques

---

The [H-Net] model has been a dream of mine for years.
I've been fortunate enough to be able to work with my student [June](https://sukjunhwang.github.io/) who made all the technical breakthroughs behind this model, solving (or at least making serious progress towards) what I consider to be a very difficult but foundational problem for deep learning.

In this post, I provide an informal personal recounting of the motivation and history of this project, providing various context and discussion that wouldn't make sense in a paper.
This post is really just for fun -- I've always found it interesting to read about the windy history of research instead of just the streamlined final result, and I felt like telling this story.

In the next post [Part II (The Future)](#hnet-the-future), I'll explain why I think this model is so important and a variety of potential downstream implications.


## Hierarchy is Everywhere

The idea of hierarchy has always resonated with me.
Perhaps its comes from my fondness for discrete math and roots in competitive programming, where I spent all day working with algorithms on trees, the canonical hierarchical structure.
Perhaps it reflects the way I tend to think; I structure almost everything hierarchically, for example using bulleted outlines for everything to a perhaps excessive degree (I'd write papers in outline format if I could!).

Or maybe it relates to when I came across the concept of [chunking](https://en.wikipedia.org/wiki/Chunking_(psychology)) in human cognition<d-footnote>I think I might have heard about this from an undergrad psychology class, and then later again when I was struggling in the PhD and browsed a class on [Learning How to Learn](https://www.coursera.org/learn/learning-how-to-learn) to feel productive üòÖ</d-footnote>, which seems [crucial] for intelligence.
For example, human cognitive processing is strongly hierarchical: language is parsed into words, phrases, clauses, sentences, and beyond;
perception is organized into objects and events;
tasks are decomposed into subtasks and goals.
Raw streams of thought are chunked into *abstractions* and *ideas*.
The *chunking* concept seems really important to me, and later became adopted for this project's terminology.

At any rate, I've always felt like hierarchy is a fundamental property of how the world works.

## Hierarchical RNNs

From the very beginning of my work on sequence models, I was drawn to hierarchical models.

My first exposure to sequence models came during an internship in 2019 at DeepMind under the amazing [Caglar Gulcehre] and [Nando de Freitas].
I started thinking about RNNs with help from Caglar, who was an encyclopedia of classical sequence models (e.g. on the original [GRU](https://arxiv.org/abs/1412.3555) paper <d-cite key="chung2014empirical"></d-cite>) and pointed me to a lot of interesting ideas.

I was particularly interested in the notion of improving long-range dependencies through hierarchies.
This had been attempted in many flavors of RNNs, such as the Clockwork RNN <d-cite key="koutnik2014clockwork"></d-cite>,
Hierarchical Multiscale RNN <d-cite key="chung2017hierarchical"></d-cite>,
and Dilated RNN <d-cite key="chang2017dilated"></d-cite>.
I played around with tons of ideas on incorporating hierarchies into RNNs;
I particularly liked the idea of the [Ordered Neuron LSTM](https://arxiv.org/abs/1810.09536) (ICLR 2019 best paper!) <d-cite key="shen2019ordered"></d-cite>,
which introduced a clever mechanism to incorporate an implicit hierarchy in the forget/input gates of an LSTM.
My [first ever paper on sequence models](https://arxiv.org/abs/1910.09890) <d-cite key="gu2020improving"></d-cite> was heavily inspired by this paper (as well as by [Chrono LSTM](https://arxiv.org/abs/1804.11188) <d-cite key="tallec2018can"></d-cite>,
which inspired me to think of RNNs as continuous systems and led to a lot of my work on state space models -- but that's another story).


## S4 and SaShiMi

After DeepMind, my research switched completely to sequence models, and I started developing state space models from [HIPPO]() to [LSSL]() to [S4]() CITE, which was the culmination of my PhD.
A little known fact (I'm guessing) about the S4 paper is that I actually introduced a hierarchical architecture in one of the experiments!

{% include figure.liquid loading="eager" path="assets/img/2025-07-11-hnet/s4_cifar.png" %}

### Autoregressive units for image modeling
In this paper, we tested S4's auto-aggressive modeling ability on a variety of distributions:
on the left is the CIFAR-10 density estimation problem,
and on the right is language modeling on WikiText-103 (at the time a popular data set, but far too small scale now).

To improve the pixel modeling problem performance, I actually introduced an *autoregressive U-Net* structure that had two stages of down-sampling.
The overall structure of this model, such as the fixed-width pooling and how to preserve causality for the autoregressive sampling, is very similar to a bunch of models that have since been used for language (EXAMPLES: Hourglass, Megabyte, block)

{% include figure.liquid loading="eager" path="assets/img/2025-07-11-hnet/s4_cifar_appendix.png" caption="I just realized that I really never hinted at the U-Net structure anywhere in the main body, which seems like a big oversight. Oops." %}

As far as I'm aware, this paper (concurrently with [Hourglass Transformer](https://arxiv.org/abs/2110.13711) <d-cite key="nawrot2022hierarchical"></d-cite>, submitted to arXiv just 5 days apart and with nearly identical architectures!) might have been the first to actually use an autoregressive U-Net?<d-footnote>On 1D sequences specifically; [PixelCNN++](https://arxiv.org/abs/1701.05517) <d-cite key="salimans2017pixelcnn"></d-cite> did this on 2D structure, which was the motivation for my architecture.</d-footnote>
Although it's not particularly complicated, so I wouldn't be surprised if there were some earlier ones I missed.

### U-Nets don't work for language

Something not reported in the paper, though, was that I tried the same backbone on the WikiText-103 language modeling experiments as well.
I really wanted it to work, since I felt like hierarchy should be important!
But no matter what I did, the hierarchical version was always *noticeably worse* than the isotropic backbone for language modeling.

I eventually gave up on this, and I think it formed the basis for some of my intuition around language modeling.
In particular, I realized that fixed-width pooling just didn't make sense for language.
(And moreover, I realized that this is essentially the same as convolutions, or at least has the same inductive bias, and this helped form the basis of my hypothesis about how convolutions don't work for language modeling or more general "discrete data" that has variable spacing. See the Aside of my [LINK TO PREVIOUS BLOG] for a bit more discussion.)

To be honest, I don't really understand how so many papers on this U-Net-like strategy have been published for language modeling.
I'm pretty skeptical that they work at all, as has also been shown in ablations by follow-up works on byte-level language modeling like [SpaceByte]() and [BLT]() CITE.
I guess one major difference is that all of these papers were using Transformers, which is much better for language than S4, but I still don't think it really makes sense ü§∑.

### But they work great for audio!

{% include figure.liquid loading="eager" path="assets/img/2025-07-11-hnet/sashimi.png" %}

Instead, I realized that S4 and U-Nets (and convolutions in general) have a great inductive bias for data that has an underlying notion of uniform sampling rate, namely **perceptual modalities** like *audio, images, and video*.
So [Karan](https://www.linkedin.com/in/krandiash/) took this autoregressive U-Net backbone with S4 layers and applied it to modeling/generating raw audio waveforms,
where [it worked really well](https://arxiv.org/abs/2202.09729) <d-cite key="goel2022raw"></d-cite>.<d-footnote> I'm still proud of the method name and title, my most important contribution to this paper üòÅ</d-footnote>

## A Differentiable Chunking Module?

Around 2022-2023, I was idly thinking about hierarchies in the back of my mind, and the way I formulated the goal was something like this.

### Attention as a primitive
To me self-attention could be phrased as: A simple, differentiable module that captures the "primitive" of forming *dense connections* and building *functions on sets*.
What makes it powerful is that it feels simple and foundational; capturing some [fundamental transformation] in a simple differentiable way that allows it to be scaled.

My research is always after these **primitives**: lower-level building blocks that capture fundamental properties and transformations, and can be flexibly applied and scaled.
My work on state space models, for example, is really about fleshing out the *primitive* of **recurrence**, which I also do view as being a fundamental property of the world.
(It's probably not a very well-known fact that my academic job talk was not about SSMs *per se*; it was called *New Structured Primitives for Machine Learning*!)

### Chunking as a primitive
Back then, I thought that *chunking* was such another fundamental primitive.
I thought: would it be possible to invent a simple differentiable building block that encapsulated the idea of chunking?
Based on my experience with language modeling, and my overall priors about the world, I felt like this would be an incredibly powerful module that would allow models to organize raw data into more meaningful units.
Surely this was a more appropriate way of modeling language and much more!

But how can one create such a differentiable chunking primitive?
Grouping discrete units of data together is, of course, a discrete selection problem, which is generally very hard for differentiable optimization.
Unfortunately, **I had absolutely no idea how to do this**.
So this was always just an idle thought in the back of my head, and I never actively worked on it.

## Information-based Chunking

In 2024, I started idly thinking about this again; maybe partly motivated by [Cartesia](http://cartesia.ai/)'s focus on audio, where it seemed like differentiable chunking could be very useful.

### Tokenization as a test case

[to me, solving tokenization was actually never the end goal in itself. I had always cared about it because I felt like the proper way to solve it would be through creating a differentiable chunking module which would be so much more powerful.
this is still the way that I like to pitch the problem now: overcoming tokenization is not about tokenizers *per se*, but about learning how to build abstractions [, and composing this which would lead to more intelligent models].]

### Total information content as a chunking strategy
I finally had an idea for how to do this: I still didn't know how to make this process differentiable, but at the very least, perhaps we could come up with some smart general principles for what chunks should look like.
It seemed like the key for a chunk to be "meaningful" is that it should have a certain amount of "information" in it.
And there's an easy proxy to create such constant-information chunks:
Simply train a neural network to model the information content of the data (which can be read off of the conditional distributions of a standard autoregressive model, of course) and use that to segment the data,
by grouping tokens together until the total negative log-likelihood of that chunk overflowed some threshold.
this intuitively had a number of desirable properties, such as
- if there were a lot of low-information tokens that are very easy to predict, then there probably isn't valuable meaning to them, so they should all be grouped together;
- if the model is surprised by the next token (such as the beginning of a word, which generally has higher entropy), then the model would be forced to draw a boundary and start the next chunk, which ideally represented a new unit of meaning.

(Fast forwarding a bit: in H-Net, we don't *completely* understand where and why it decides to draw chunk boundaries, but we did notice a connection to this uncertainty principle; it tends to draw boundaries when surprised. But that's the subject for a future post.)

### The project begins
About a year ago, in summer 2024, I gave a research talk (an early version of [my last blog post]) to my academic group and highlighted some key problems and approaches that I thought could be important.
Solving tokenization and finding a way to create hierarchical models that fit the natural information of the data was one such problem,
and I pitched the above approach as a concrete direction to get started.

Some students got interested and started thinking about this.
I was still very, very interested in the above approach about information-based chunking;
[Ricardo](https://www.linkedin.com/in/ricardobuitrago/) explored it a little bit but didn't try too hard as he started working on another project on [length generalization in recurrent models](https://goombalab.github.io/blog/2025/improving-length-generalization/) <d-cite key="buitrago2025understanding"></d-cite> (I'm also very fond of this result!).
I kept trying to get June to try this approach as well, which I was convinced was a good idea...


## Differentiable Selection with <br> Modern Techniques

Fortunately, June refused to listen to me because he had a much better idea! (A recurring theme in this project...)

He wanted to try something loosely inspired by mixture-of-experts (MoE), the idea being that MoE (where each token gets "routed" to a fixed set of $k$ "experts) is essentially a discrete selection problem like the one we want to solve.

The main advantage of this approach is that it provides some hope for building a *differentiable* chunking strategy in an end-to-end model, unlike my information-based heuristic which would require training a separate proxy model first.
At the time, I actually basically didn't know at all how MoE worked, so I left June to his own devices for a long time,
and he got the main ideas working relatively quickly.
But there were a lot of really confusing training behaviors and instabilities, so he spent a long time building intuition and slowly improving the model.

[TODO: add a whiteboard picture from June or an early PowerPoint diagram he drew]

Incidentally, it's kind of interesting to me how obvious in hindsight, but not in foresight, the main idea is.
As this project developed, and I told a bunch of people about it, their reaction usually went from "this differentiable selection problem seems impossible" to "oh yeah that might work" as soon as I mentioned the connection to MoE being another variant of a discrete-selection problem.

## Discovering Prior and Concurrent Works

When we started this project, we hadn't done a very serious literature search attempt because there are astoundingly few papers on this topic -- I guess it's a pretty hard problem after all.
Over the course of the project, we discovered two related works that were most directly related,
and I expect we'll get plenty of questions about how they compare.

### Hourglass and Dynamic Pooling Transformer

Around late October 2024, we discovered the [Dynamic Pooling Transformer (DPT)](https://arxiv.org/abs/2211.09761) <d-cite key="nawrot2023efficient"></d-cite> (and its prior work, the Hourglass Transformer, which introduced an auto-aggressive U-Net structure concurrently with S4/SaShiMi). CITE
To the best of our knowledge, the DPT is the first, and I think essentially only, prior attempt at building an **end-to-end hierarchical autoregressive model with dynamic chunking**.
The overall network structure was a similar autoregressive U-Net as Hourglass,
the main difference being the incorporation of a binary-valued *boundary predictor* that tries to learn where to chunk the data.

Their motivation for how to overcome the discrete-choice problem was [delegating to] stochastic re-parametrization techniques, namely the Gumbel-softmax trick CITE.
In their paper, this fully end-to-end version didn't seem to work too well; it underperformed their alternative variants that relied on more heuristics and supervision and had stability issues.
Ultimately, I guess that DPT attempted to tackle this problem somewhat prematurely and important ideas weren't available yet (for example, the most popular [Switch Transformer](https://arxiv.org/abs/2101.03961) MoE method <d-cite key="fedus2022switch"></d-cite>  was pretty new and perhaps not as established yet).
But I think this paper deserves more credit for at least making a real attempt at this problem!

Actually, before we found this paper we had tried many variations of Gumbel noise already;
I'm rather fond of the Gumbel-softmax trick myself and kept trying to get June to incorporate it.
But it never seemed to help empirically ü•≤.
I'm still not convinced there isn't something useful here...

<span style="color:blue">Is the tone here appropriate? I want to point out limitations and differentiate us in an honest way without looking disparaging of prior work or sounding defensive.</span>

{% details A missed naming opportunity %}
Just for fun, I'll mention an earlier working name for the model I considered.

I think Hourglass is an aesthetically pleasing name, and also elegantly describes the characteristic shape of the architecture; it's essentially equivalent to a U-Net, but refers to the actual tensor dimensions (gradually shrinking and then expanding the sequence, like an hourglass figure) instead of the abstract downsampling/upsampling process.

{% include figure.liquid loading="eager" path="assets/img/2025-07-11-hnet/hourglass.png" caption="The Hourglass Transformer" %}

I thought about calling our model the **Hourglass Network** (shortened to H-Net), and the inner dynamic chunking module the **TickTok** layer.
I really like this name because there are several layers of meaning to it:
1. Hourglass, of course, still refers to the shape of the hierarchical network. (Calling it "Network" also differentiated from the Hourglass Transformer, since one of our important details was using SSMs in the outer layers.)
2. The network structure could be viewed as *multi-rate sequence modeling* involving modalities that **tick** at different rates, such as *characters vs. words*, or *audio vs. phonemes*. The **Tick**Tok layer would be the interface between these modalities.
3. The network could also be viewed as performing a form of dynamic **tok**enization that compresses contiguous inputs just as tokenizers do. The Tick**Tok** layer would again be the mechanism that performs this compression. (*Dynamic tokenization*, instead of dynamic chunking, was also the working name for this project for a long time.)
4. Finally, there's a **time**-related theme to all of it: *Hourglass* Networks would be defined as models that have *temporally-dynamic* downsampling rates, where the core mechanism is the *"tick tock"* layer.

Unfortunately, I was a bit [uneasy/worried] about the name "Hourglass" being confused for unrelated lineages of work;
it's also been used by plenty of other [unrelated](https://arxiv.org/abs/1603.06937) [models](https://arxiv.org/abs/2401.11605).
I also felt that "TickTok" might be a little too cheesy;
and the "tick tock"/TikTok name is also of course a well-recognized brand that might be confusing, and the idea has also already been used by a [very popular tokenizer](https://github.com/openai/tiktoken).
So we didn't go with it. Alas ü•≤.

(I still liked the H-Net name, so we kept it and later "backronymed" it to Hierarchical Network.)
{% enddetails %}

### BLT

By December, we had made a lot of progress and pretty much had the main architecture done (in particular, June had come up with the *smoothing module* which at the time we were calling the "EMA detokenizer"; as we show in the paper's ablations, this was one of the most important new techniques introduced.)

At this point, the [Byte Latent Transformer (BLT)] paper came out, which made a big splash for claiming to solve tokenization end-to-end, through a similar hierarchical architecture using an entropy-based segmentation rule.
I think the terminology is a little confusing because they indeed built a model operating "end-to-end" on bytes, but to me, the crux of the problem is *learning the dynamic boundaries* jointly with the model itself.
This is not possible (at least not obviously) with the entropy heuristic and is the main reason [why we didn't explore this approach](#information-based-chunking).
In the H-Net paper, we specifically use "end-to-end" to refer to this stronger definition.
Still, it was gratifying to see that my idea to use information-based chunking heuristics does (somewhat) work!

{% details A deeper dive into this approach %}
There is one critical difference between the [original idea I had](#total-information-content-as-a-chunking-strategy) and what BLT did.

{% enddetails %}

After BLT came out, we had the thought to just clean up our (pretty reasonable already at the time) results and put out a pre-print quickly so that our work would be seen as concurrent.
But we decided to follow my principles: put our heads down and keep working in the direction we believe in, and wait to put something out when it was done in the ideal way we wanted, instead of worrying about community perception.



[One thing that we did borrow from these related works was some naming conventions. Internally we had always been calling the outer (e.g. byte-level) models the "pre-stage" and "post-stage", and the inner model the "main stage".
It seemed more popular to call these networks an "encoder" and "decoder" respectively, and we decided to change to this naming convention to be more consistent with the existing literature.]

## A World of Tuning

I think the architecture has been relatively stable since early 2025, but we spent a long time trying to understand and simplify every part of it.
For example,
- **Architecture components**: our final architecture included post-norm layers at the end of every sub-network, as well as linear projections on the residual connection (Section REF). We spent a long time trying to remove these since I felt like they were non-standard techniques (most U-Nets don't have these) that complicated the model; and I'm a bit worried about how they affect stability at larger scales. But in the end, they seem very useful and we kept them.
- **Sparsity auxiliary loss**: Our auxiliary loss targets a specific down-sampling ratio (Section REF). This seems a little artificial to me and introduces an annoying new hyperparameter (thankfully, the only one!) And I felt like it would be cleaner to simply impose a "sparsity loss" that encouraged higher compression rates (it would be counter-balanced by the main loss function which encourages more compute, hence lower compression rates). We found some things that sort of worked but it didn't seem as consistent, so for this version of the H-Net we kept the targeted loss.
- **Chunking mechanisms**: We tried so many different variations of the routing module, upsampling step, downsampling step, smoothing module, and every other component. June's PowerPoint deck of ideas and variations has hundreds of slides, 

{% include figure.liquid loading="eager" path="assets/img/2025-07-11-hnet/tuning.png" caption="A random slide from the deck of ideas involving multiple residual connections and extra stages/sub-networks, which remained as a flag in the internal code for a long time." %}

Although our final (current) design has a lot of moving parts,
they all had concrete motivations, are as simple as we could make it without sacrificing stability or performance.
I have to say that this was one of the most difficult empirical projects I've witnessed (and I'm somewhat decent at tuning architectures üëÄ).
Normally when tuning models, one hopes that different axes of variation act somewhat independently so they can be ablated in isolation and we can perform a "coordinate descent in hyperparameter space".
But somehow for this architecture, it felt like there were 10 different axes that were all critical and that *all had to interact together in exactly the right way*.
There were so many times when we thought that option "A" was better than "B",
before changing some other design decisions and flip-flopping to B,
before changing even more things and going back to A.
Even after we had all the main ingredients, it was really difficult to get them to interact together seamlessly.
(If it isn't clear, I didn't do any of this; June is an amazing empiricist and has by far the best intuition for everything related to H-Nets.)

We also spent a lot of effort re-running baselines and trying to make sure our recipe was solid and the comparisons were fair.
For example, at the last second we realized that our "space-delimiter baseline" didn't incorporate all the new techniques we developed for H-Net, such as the learning rate multipliers.
It was tricky because we wanted to call this baseline "SpaceByte++" but didn't know how many of our improvements to give it.
We eventually gave it all our architecture improvements which made it incredibly strong and renamed it to H-Net (spaces), so that we could directly compare the chunking mechanism in isolation, the most important factor of variation.

<span style="color:blue">AG: Is this section interesting at all, or is it too much? I guess I'm maybe trying to convey a few things: (i) There were so many more things behind the scenes which is why it took a long time, but the main model was done a lot earlier and independently of related work (ii) Conveying the impression of how difficult it is. And why it looks complicated, but everything is necessary.</span>

## The Future

So that's an abridged story 
