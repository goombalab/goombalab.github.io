<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws | Goomba Lab </title> <meta name="author" content="Goomba AI Lab"> <meta name="description" content="Homepage of the Goomba AI Lab @ CMU MLD. # A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/goomba_light.png?68e59a389531e710f3507b5f12827027"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goombalab.github.io/blog/2024/distillation-part2-phi-mamba/"> <script src="/assets/js/theme.js?daf0da4e15ae2df6b4045ab97d680f8d"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws",
            "description": "",
            "published": "August 20, 2024",
            "authors": [
              
              {
                "author": "Aviv Bick*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Kevin Y. Li*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Eric P. Xing",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, MBZUAI",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "J. Zico Kolter",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Gu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goomba Lab <img src="/assets/img/goomba_transparent.png" width="30" height="30"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Cross-Architecture Distillation Part II - Phi-Mamba-1.5B Model and Training Laws</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#final-results">Final Results</a> </div> <div> <a href="#importance-of-each-mohawk-stage">Importance of Each MOHAWK Stage</a> </div> <div> <a href="#training-laws-for-mohawk">Training Laws for MOHAWK</a> </div> <ul> <li> <a href="#training-the-final-phi-mamba-model">Training the Final Phi-Mamba Model</a> </li> </ul> <div> <a href="#hybrid-phi-mamba-model">Hybrid Phi-Mamba Model</a> </div> </nav> </d-contents> <div> <p>[<a href="https://arxiv.org/abs/2408.10189" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba" rel="external nofollow noopener" target="_blank">Code</a>]</p> <ol> <li><a href="/blog/2024/distillation-part1-mohawk/">Part I - MOHAWK</a></li> <li>Part II - Phi-Mamba</li> </ol> <p>In <a href="/blog/2024/distillation-part1-mohawk/">Part I</a> of this series, we covered important terminology, the Mamba-2 architecture, and the MOHAWK architecture. We also demonstrated Mamba-2’s ability to match the self-attention matrix of Transformers, which influenced our choice to use it as the student model for validating our MOHAWK method.</p> <p>In this section, we will explore the training laws regarding each of the three stages of MOHAWK and empirically validate the importance of all stages. We use the cumulative insights gained to then distill a <strong>fully subquadratic Mamba model using only 3B tokens</strong> - less than 1% of many of the other models’ token budget - while being <strong>competitive with many of the current state-of-the-art open-source subquadratic models</strong>! We also distill a strong Mamba-Attention hybrid.</p> <h2 id="final-results">Final Results</h2> <p>We empirically validate the MOHAWK framework by distilling the pretrained Phi-1.5 model into a 1.5B Mamba variant, dubbed Phi-Mamba. Our final model was distilled with <strong>only 3B tokens</strong>, with a 80M/160M/2.76B token split among Stage 1/2/3, from the C4 dataset with a context length of 2048. The choices for these token splits were influenced by our verification of the importance of all three stages and training laws that determined, given a fixed token budget, how to allocate resources, which we detail in the following sections.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/table:phi-mamba-performance.png" width="100%" height="auto" title="Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Performance of Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <h2 id="importance-of-each-mohawk-stage">Importance of Each MOHAWK Stage</h2> <p>A brief recap of the three stages of MOHAWK are</p> <p>1) <strong>Matrix Orientation</strong>: matches the matrix mixer of each respective block.</p> <p>2) <strong>Hidden-State Alignment</strong>: independently compares the block output given the same input across all layers of the student model.</p> <p>3) <strong>Weight-Transfer and Knowledge Distillation</strong>: performs knowledge distillation of logits from teacher to student and copies over crucial weights from the teacher model.</p> <p><strong>Each stage plays a crucial role</strong> as shown in our ablations below. All the runs were performed with a fixed total token count.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/table:mohawk-stage-abl.png" width="100%" height="auto" title="MOHAWK Ablations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Effects of various MOHAWK stage ablations on downstream performance</figcaption> </figure> <p>As expected, Stage 3’s end-to-end alignment is important as the <strong>previous stages only match the block outputs</strong>, leaving the blocks disjoint if the hidden state cannot be completely matched, as shown with both the Phi-Mamba and Hybrid-Phi-Mamba trained on Stage 3 outperform their counterparts trained with Stage 2. Of course, student models that have more mixing layers similar to the teacher may see a diminished impact of Stage 3 as the layers may be aligned more with only Stage 2.</p> <p>The addition of a Stage 2 initialization provides additional synergy, <strong>boosting performance significantly compared to Stage 3 only</strong>. We also note that the effects of adding Stage 2 is more pronounced in cases where the student architecture is less similar to the teacher architecture, e.g., the improvement for Phi-Mamba which has zero attention layers is larger than Hybrid-Phi-Mamba which has four.</p> <p>Stage 1 also provides a good in downstream performance. For example, only with the addition of Stage 1 on top of Stage 2 and 3 can a Phi-to-Phi distillation <strong>recover the original teacher Phi’s overall performance</strong>. And, we see in the two other architectures that performance gains can also be observed.</p> <h2 id="training-laws-for-mohawk">Training Laws for MOHAWK</h2> <p>We aimed to evaluate the impact the preceding stage had on the current stage’s performance.</p> <p>For the Stage 2 + 3 pair, we trained Phi-Mamba instances from scratch using Stage 2 to various checkpoints. These checkpoints were then used to initialize Phi-Mamba instances that were trained using Stage 3 to different total budgets. The figure below shows that given an adequate training budget, <strong>student models initialized from Stage 2 outperform students trained only with Stage 3</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage23-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage23-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage23.png" width="100%" height="auto" title="Stage 2 + 3 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Training Laws for Stage 2 and 3 of MOHAWK</figcaption> </figure> <p>Given the previous finding, we then analyze how matrix mixer matching (Stage 1) can impact the student’s ability to match the overall mixer block with the teacher (Stage 2). Similar to before, we train numerous Phi-Mamba models using Stage 1 and use them as initializations for Stage 2 and compare them against each other and also a Stage 2 only model. Here, we find that <strong>even a small budget allocated to Stage 1 can help the subsequent stage</strong> perform better than random initialization.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:training-law-stage12-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:training-law-stage12-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/fig:training-law-stage12.png" width="100%" height="auto" title="Stage 1 + 2 Training Laws" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Training Laws for Stage 1 and 2 of MOHAWK</figcaption> </figure> <h3 id="training-the-final-phi-mamba-model">Training the Final Phi-Mamba Model</h3> <p>Using the insights gained in the training laws above, we finalized our training regime given a fixed budget of 3B tokens. Stage 1 was allocated 80M due to the strong performance on matrix distance and hidden state distance. Stage 2 was trained for 160M tokens given the seeming saturation of both hidden state distance and perplexity when compared to the other initialization states, e.g., 10M, 20M, 40M, etc. We train Stage 3 to reach 3B tokens in total, but reduced the learning rate of the last stage to alleviate training instabilities. We hypothesize that this is due to the Stage 1 + 2 initialization’s Mamba component being quite similar to that of the teacher model, so a large learning rate coupled with disconnect between blocks, which are addressed in Stage 3, can cause training instabilities.</p> <h2 id="hybrid-phi-mamba-model">Hybrid Phi-Mamba Model</h2> <p>There has been a growing body of work that combines both Attention and SSM mechanisms, leading to improved performance over either one used by itself <d-cite key="Samba"></d-cite><d-cite key="jamba2024"></d-cite><d-cite key="MambaVision"></d-cite>. Although incorporating Attention layers does make the model quadratic, limiting their number allows us to mitigate the efficiency drawbacks while increasing expressivity and performance!</p> <p>Thus, we distill the Phi-1.5 model into a Mamba-Attention hybrid model that maintains only four quadratic Attention layers. The remaining layers use the Mamba-2 layer variant also used in our Phi-Mamba model. Trained with 5B tokens using the MOHAWK method, our model achieves an average score of $66.0$ on downstream metrics, <strong>outperforming Phi-Mamba</strong>’s $65.1$ and <strong>approaching Phi-1.5</strong>’s $67.2$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-480.webp 480w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-800.webp 800w,/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/table:hybrid-phi-mamba-performance.png" width="100%" height="auto" title="Hybrid-Phi-Mamba Performance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Performance of Hybrid-Phi-Mamba 1.5B on downstream evaluations</figcaption> </figure> <p>Our Hybrid-Phi-Mamba model is <strong>performs comparably</strong> to strong Attention-Mamba hybrids at the 1.5B range <strong>while using less Attention layers</strong> than Samba (12) and Mamba-SWA-MLP (18). We find that interleaving the Attention layers with the Mamba layers resulted in the strongest model, an observation that was also seen in <d-cite key="Samba"></d-cite>. We also find that increasing the number of Attention layers improved performance.</p> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/mohawk.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Goomba AI Lab. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>