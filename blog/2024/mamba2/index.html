<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> State Space Duality (Mamba-2) Part 1 - The Model | Goomba AI Lab </title> <meta name="author" content="Goomba AI Lab"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gu-group.github.io/blog/2024/mamba2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "State Space Duality (Mamba-2) Part 1 - The Model",
            "description": "",
            "published": "May 27, 2024",
            "authors": [
              
              {
                "author": "Albert Gu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Carnegie Mellon University",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tri Dao",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Princeton",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goomba AI Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>State Space Duality (Mamba-2) Part 1 - The Model</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#equations">Equations</a> </div> <div> <a href="#the-ssd-model">The SSD Model</a> </div> <div> <a href="#code">Code</a> </div> </nav> </d-contents> <p>Since the release of [Mamba], we’ve been overwhelmed by the community response.</p> <p>(give list of examples of applications and understanding papers)</p> <p>(link to Aviv’s compilation)</p> <p>Despite its … we weren’t satisfied with</p> <h3 id="problem-1-understanding">Problem 1 (Understanding):</h3> <p>From a conceptual standpoint, one of the reasons we found SSMs so fascinating is how they just feel <em>fundamental</em>. One way this is exemplified is how they have rich ties to many major paradigms of sequence models. As developed in our earlier works on structured SSMs [cite LSSL and thesis], they seem to capture the essence of continuous, convolutional, and recurrent sequence models – all wrapped up in a simple and elegant model.</p> <p>But of course, aside from these, there is another major sequence model paradigm: the ubiquitous <strong>attention</strong> mechanism (and variants).</p> <blockquote> <p>Question 1: <strong>What are the conceptual connections between SSMs and attention?</strong></p> </blockquote> <p><strong>Problem 2 (Efficiency):</strong> From a computational standpoint, despite the work that went into making Mamba fast – in particular, its hardware-aware selective scan implementation – it is still much less hardware-efficient than mechanisms such as attention. The missing piece is that modern accelerators such as GPUs and TPUs are highly specialized for matrix multiplications (matmuls), While this is not a problem for inference, which is bottlenecked by different types of considerations, this can be a big deal during training time. For example, an end-to-end Mamba-1 model is XX times slower than an equivalent Transformer.</p> <p>Question 2: <strong>Can we speed up the training of Mamba models by recasting them as matrix multiplications?</strong></p> <p>These are the main questions that SSD (a.k.a. Mamba-2) tries to address.</p> <h2 id="outline">Outline</h2> <p>Other Topics:</p> <ul> <li>How does it relate to SSMs</li> <li>How does it relate to attention</li> <li>future work from each viewpoint</li> </ul> <h2 id="the-state-space-dual-model">The State Space Dual Model</h2> <p>SSD refers to both a general framework, as well as a specific model. The <strong>state space dual model</strong> or SSD model itself really isn’t so scary - we’ll first provide a self-contained description of the SSD layer in isolation here before elaborating on some of the theoretical connections!</p> <h3 id="the-linear-ssm-mode">The Linear (SSM) Mode</h3> <p>SSD starts from the same selective state space model as Mamba:</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} y_t \end{aligned}\] <p>To recap, a structured state space model (SSM) defines a map from $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$ <img class="emoji" title=":warning:" alt=":warning:" src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png" height="20" width="20">. Think of $x_t$ and $y_t$ as being scalars, and the hidden state $h_t$ as an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the state size, state dimension, or state expansion factor. A <em>selective</em> state space model allows the $A, B, C$ SSM parameters to vary across time. We’ll think of them as tensors with shapes $\mathtt{(T, N, N)}$, $\mathtt{(T, N)}$, and $\mathtt{(T, N)}$ respectively.</p> <p>Structured SSMs require $A$ to have structure to be efficiently computable, such as the most commonly used diagonal structure <img class="emoji" title=":warning:" alt=":warning:" src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png" height="20" width="20">. In this case $A$ has shape $\mathtt{(T, N)}$ where only the diagonal elements of the $\mathtt{N} \times \mathtt{N}$ matrices are stored.</p> <h4 id="ssd-scalar-structured-ssm">SSD: Scalar Structured SSM</h4> <p>The original Mamba (or more precisely its core “S6” layer) is exactly a selective SSM with diagonal structure. <strong>The SSD layer of Mamba-2 makes only one simple modification</strong>: it restricts the diagonal $A$ even further to a <em>scalar times identity</em> structure; in other words the diagonal elements of $A$ must all be the same. In this case $A$ can be represented with shape just $\mathtt{(T)}$ and one can also identify $A_t$ as just a scalar (and thus we will sometimes denote it $a_t$).</p> <h4 id="multihead-ssms">Multihead SSMs</h4> <p>Here, we think of $X$ as a tensor of shape $\mathtt{(T, P)}$ where $\mathtt{T}$ is the sequence (time) dimension and $\mathtt{P}$ is the “head dimension”.<d-footnote>We will ignore the batch dimension throughout this presentation.</d-footnote></p> <p>We can notate the general (selective) state space model as \begin{equation} \label{eq:ssm} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T,…)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>Axes of variation include the structure on $A$, which affects its parameter shape, and the state dimension $\mathtt{N}=\mathtt{d_state}$ and state dimension $\mathtt{P}=\mathtt{d_head}$.</p> <h4 id="efficiency">Efficiency</h4> <p>The reason why SSMs are interesting is because computing it as a recurrence requires maintaining a <em>constant-size state</em> (size $\mathtt{N}$) and scales <em>linearly in the sequence length</em> $\mathtt{T}$. But as mentioned above, the raw FLOPs don’t reflect actual speed in practice…</p> <h3 id="the-quadratic-attention-mode">The Quadratic (Attention) Mode</h3> <p>Let’s switch tacks and forget about state space models for a moment. Given the same $(A, B, C)$ tensors above with the same shapes $(\mathtt{T})$, \mathtt{(T, N)}$, and \mathtt{(T, N)}$, let’s define a different object.</p> <p>First, we’ll define</p> \[L = \mathsf{1SS}(a_{0:T}) = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{T-1}\dots a_1 &amp; a_{T-1}\dots a_2 &amp; \dots &amp; a_{T-1} &amp; 1 \\ \end{bmatrix} .\] <p>Then, let’s define the following matrix</p> <p>\begin{equation} \label{eq:linear-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Finally, $M$ encodes a sequence transformation $x \in \mathbb{R}^\mathtt{T} \to y \in \mathbb{R}^\mathtt{T}$ just as how we defined <a href="#the-linear-ssm-mode">SSMs</a> above.</p> <p>What’s special about this? Well, you may notice that it looks very similar to an attention computation. In fact, if all $a_t = 1$, then $L$ is simply the lower-triangular <em>causal mask</em> and \ref{eq:linear-attention} is exactly <strong>causal linear attention</strong> <img class="emoji" title=":warning:" alt=":warning:" src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png" height="20" width="20"> : \(Y = (L \circ Q K^\top) V\)</p> <p>This is exactly the same as equation \eqref{eq:linear-attention} if we rename $(C, B, X) \mapsto (Q, K, V)$!</p> <h4 id="efficiency-1">Efficiency</h4> <h3 id="best-of-both-worlds-the-hybrid-mode">Best of Both Worlds: the Hybrid Mode</h3> <p>Computationally, one can use either formulation to compute the model. Loosely speaking, the attention form is faster during training because it’s dominated by matrix multiplications, while the SSM form is preferred during autoregressive inference.</p> <p>In the next two sections [LINK], we’ll present two broad frameworks with which to understand the state space dual model. Each of them will both prove the equivalence of these two formulations, but each is much more general, and we’ll discuss other consequences of the frameworks.</p> <p>If you just want to use the model, stop here! In the rest of this post, we’ll give an overview of the theoretical aspects of the SSD framework.</p> <h3 id="state-space-duality">State Space Duality</h3> <p>The so-called “duality” refers to the fact that the two models defined in XX and XX are in fact <em>exactly the same model</em>, which is a particular function $(A, B, C, X) \mapsto Y$ with tensor shapes specified above. We’ll show this fact in two completely different ways, both of which are actually much more general and each quite illuminating.</p> <p>If you take our word for it, though, then SSD is relatively simple to understand in contrast to either SSMs or attention.</p> <h4 id="ssd-vs-state-space-models">SSD vs. State Space Models</h4> <p>Compared to previous SSMs, SSD is pretty much the same as the core layer of Mamba but with even more structure on the recurrent $A$ matrices.</p> <ul> <li>Mamba (S6) uses diagonal structure on $A$ with a head dimension of $\mathtt{P}=1$.</li> <li>Mamba-2 (SSD) uses scalar-times-identity structure on $A$ with a head dimension of $\mathtt{P}&gt;1$ (something like $\mathtt{P}=64$ by default).</li> </ul> <p>In particular, this can be viewed as weight-tied in two ways:</p> <ul> <li>By restricting the diagonal structure of $A$ to scalar-times-identity, the scalar recurrence dynamics are tied across all $\mathtt{N}$ elements of the state space.</li> <li>These dynamics are also shared across all $\mathtt{P}$ channels of a given head.</li> </ul> <p>In other words, a single SSM head has total state size $\mathtt{P} \times \mathtt{N)}$, which are each governed by separate scalar recurrences in Mamba but are controlled by a single shared recurrence in Mamba-2.</p> <p>Why make these restrictions? The main motivation is efficiency: these changes are necessary to be able to view the model in its [<a href="#the-quadratic-attention-mode">dual attention form</a>], which allows matrix multiplications to be used.</p> <blockquote class="block-tip"> <h5 id="the-bottom-line-mamba-2-vs-mamba-1">The Bottom Line: Mamba-2 vs. Mamba-1</h5> <p>Compared to Mamba-1, Mamba-2 allows <strong>much larger state dimensions</strong> (from $\mathtt{N}=16$ in Mamba-1 to $\mathtt{N}=64,$ or $\mathtt{N}=256$ or even higher in Mamba-2) while simultaneously being <strong>much faster during training</strong>.</p> </blockquote> <p>But can this hurt us? There’s some intuition to believe that it shouldn’t. One of the main reasons for the selectivity (e.g. $A$ that depends on the input $X$) introduced in Mamba is to let the SSM be able to control whether to remember or ignore particular pieces of information; for example, if a filler “um” is encountered in a text transcript. But if such information should be ignored, then the entire state can ignore it together, and so it should be okay if the state’s dynamics are shared across all features.</p> <p>Empirically, we haven’t found evidence that the restricted expressivity of Mamba-2 might hurt, but the jury’s still out! From one perspective, Mamba-2 isn’t <em>strictly</em> better than Mamba-1: while it’s a dramatic improvement from a <em>training</em> perspective, Mamba-1 might be better from a pure <em>inference</em> perspective. Since inference speed of SSMs is entirely governed by the state dimension, if one wants to maximize performance for a target inference efficiency (i.e. for a particular state size $\mathtt{N}$), then the increased expressivity of Mamba-1 might be better. We haven’t fully analyzed the (theoretical or empirical) tradeoffs here, and think this would be a cool direction for the community to dig in more.</p> <h4 id="ssd-vs-attention">SSD vs. Attention</h4> <p>Compared, to standard (self-)attention, SSD also only has two differences:</p> <ol> <li>The softmax normalization is dropped.</li> <li>A separate elementwise mask matrix is applied multiplicatively.</li> </ol> <p>The first difference can be interpreted as what reduces the effective state size of the model from infinite to finite, and improves its efficiency from quadratic to linear.</p> <p>The second difference is what distinguishes SSD from standard linear attention. One way to think of the mask is as <strong>input-dependent relative positional encodings</strong>. Because of the mask (definition <img class="emoji" title=":warning:" alt=":warning:" src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png" height="20" width="20">), the standard attention score $Q_i K_j$ is attenuated by a score $a_{i:j}^\times = a_i \dots a_{j+1}$ which can be interpreted as a discount factor based on how far apart the positions $i$ and $j$ are. <d-footnote>This interpretation was concurrently espoused by Tobias Katsch's GateLoop paper <img class="emoji" title=":warning:" alt=":warning:" src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png" height="20" width="20"></d-footnote> This is the key factor that encodes the “selectivity” of Mamba.</p> <h2 id="ssd-viewpoint-1-structured-matrix-transformations">SSD Viewpoint 1 (structured matrix transformations)</h2> <p>What is known in the literature as a (triangular) <strong>semiseparable matrix</strong>.</p> <h2 id="ssd-viewpoint-2-structured-attention">SSD Viewpoint 2 (structured attention)</h2> <h2 id="code">Code</h2> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
</pre></td> <td class="code"><pre><span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
  <span class="k">return</span> <span class="bp">None</span>
</pre></td> </tr></tbody></table></code></pre></figure> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">segsum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>

<span class="k">def</span> <span class="nf">ssd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">block_len</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">initial_states</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">A</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">C</span><span class="p">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_len</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># Rearrange into blocks/chunks
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (c l) ... -&gt; b c l ...</span><span class="sh">"</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">block_len</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)]</span>

    <span class="n">A</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h -&gt; b h c l</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">A_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 1. Compute the output for each intra-chunk (diagonal blocks)
</span>    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="n">Y_diag</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bcshn,bhcls,bcshp-&gt;bclhp</span><span class="sh">"</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 2. Compute the state for each intra-chunk
</span>    <span class="c1"># (right term of low-rank factorization of off-diagonal blocks; B terms)
</span>    <span class="n">decay_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">((</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">A_cumsum</span><span class="p">))</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bhcl,bclhp-&gt;bchpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">decay_states</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
</span>    <span class="c1"># (middle term of factorization of off-diag blocks; A terms)
</span>    <span class="k">if</span> <span class="n">initial_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">initial_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">states</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">initial_states</span><span class="p">,</span> <span class="n">states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">decay_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))))</span>
    <span class="n">new_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhzc,bchpn-&gt;bzhpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">decay_chunk</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># 4. Compute state -&gt; output conversion per chunk
</span>    <span class="c1"># (left term of low-rank factorization of off-diagonal blocks; C terms)
</span>    <span class="n">state_decay_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">)</span>
    <span class="n">Y_off</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bclhn,bchpn,bhcl-&gt;bclhp</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">state_decay_out</span><span class="p">)</span>

    <span class="c1"># Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">Y_diag</span><span class="o">+</span><span class="n">Y_off</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h p -&gt; b (c l) h p</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">final_state</span></code></pre></figure> <p>Backticks:</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">def</span> <span class="nf">segsum</span><span class="p">(</span><span class="nx">x</span><span class="p">):</span>
    <span class="dl">"""</span><span class="s2">Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM.</span><span class="dl">"""</span>
    <span class="nx">T</span> <span class="o">=</span> <span class="nx">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="nx">x_cumsum</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span> <span class="nx">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="nx">x_segsum</span> <span class="o">=</span> <span class="nx">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="nx">None</span><span class="p">]</span> <span class="o">-</span> <span class="nx">x_cumsum</span><span class="p">[...,</span> <span class="nx">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="nx">mask</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="nx">T</span><span class="p">,</span> <span class="nx">T</span><span class="p">,</span> <span class="nx">device</span><span class="o">=</span><span class="nx">x</span><span class="p">.</span><span class="nx">device</span><span class="p">,</span> <span class="nx">dtype</span><span class="o">=</span><span class="nx">bool</span><span class="p">),</span> <span class="nx">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nx">x_segsum</span> <span class="o">=</span> <span class="nx">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="nx">mask</span><span class="p">,</span> <span class="o">-</span><span class="nx">torch</span><span class="p">.</span><span class="nx">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="nx">x_segsum</span>

<span class="nx">def</span> <span class="nf">ssd</span><span class="p">(</span><span class="nx">X</span><span class="p">,</span> <span class="nx">A</span><span class="p">,</span> <span class="nx">B</span><span class="p">,</span> <span class="nx">C</span><span class="p">,</span> <span class="nx">block_len</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="nx">initial_states</span><span class="o">=</span><span class="nx">None</span><span class="p">):</span>
    <span class="dl">"""</span><span class="s2">
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    </span><span class="dl">"""</span>
    <span class="nx">assert</span> <span class="nx">X</span><span class="p">.</span><span class="nx">dtype</span> <span class="o">==</span> <span class="nx">A</span><span class="p">.</span><span class="nx">dtype</span> <span class="o">==</span> <span class="nx">B</span><span class="p">.</span><span class="nx">dtype</span> <span class="o">==</span> <span class="nx">C</span><span class="p">.</span><span class="nx">dtype</span>
    <span class="nx">assert</span> <span class="nx">X</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="nx">block_len</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="err">#</span> <span class="nx">Rearrange</span> <span class="nx">into</span> <span class="nx">blocks</span><span class="o">/</span><span class="nx">chunks</span>
    <span class="nx">X</span><span class="p">,</span> <span class="nx">A</span><span class="p">,</span> <span class="nx">B</span><span class="p">,</span> <span class="nx">C</span> <span class="o">=</span> <span class="p">[</span><span class="nf">rearrange</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span> <span class="dl">"</span><span class="s2">b (c l) ... -&gt; b c l ...</span><span class="dl">"</span><span class="p">,</span> <span class="nx">l</span><span class="o">=</span><span class="nx">block_len</span><span class="p">)</span> <span class="k">for</span> <span class="nx">x</span> <span class="k">in </span><span class="p">(</span><span class="nx">X</span><span class="p">,</span> <span class="nx">A</span><span class="p">,</span> <span class="nx">B</span><span class="p">,</span> <span class="nx">C</span><span class="p">)]</span>

    <span class="nx">A</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="nx">A</span><span class="p">,</span> <span class="dl">"</span><span class="s2">b c l h -&gt; b h c l</span><span class="dl">"</span><span class="p">)</span>
    <span class="nx">A_cumsum</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="nx">A</span><span class="p">,</span> <span class="nx">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="err">#</span> <span class="mi">1</span><span class="p">.</span> <span class="nx">Compute</span> <span class="nx">the</span> <span class="nx">output</span> <span class="k">for</span> <span class="nx">each</span> <span class="nx">intra</span><span class="o">-</span><span class="nf">chunk </span><span class="p">(</span><span class="nx">diagonal</span> <span class="nx">blocks</span><span class="p">)</span>
    <span class="nx">L</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="nx">A</span><span class="p">))</span>
    <span class="nx">Y_diag</span>  <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="dl">"</span><span class="s2">bclhn,bcshn,bhcls,bcshp-&gt;bclhp</span><span class="dl">"</span><span class="p">,</span> <span class="nx">C</span><span class="p">,</span> <span class="nx">B</span><span class="p">,</span> <span class="nx">L</span><span class="p">,</span> <span class="nx">X</span><span class="p">)</span>

    <span class="err">#</span> <span class="mi">2</span><span class="p">.</span> <span class="nx">Compute</span> <span class="nx">the</span> <span class="nx">state</span> <span class="k">for</span> <span class="nx">each</span> <span class="nx">intra</span><span class="o">-</span><span class="nx">chunk</span>
    <span class="err">#</span> <span class="p">(</span><span class="nx">right</span> <span class="nx">term</span> <span class="k">of</span> <span class="nx">low</span><span class="o">-</span><span class="nx">rank</span> <span class="nx">factorization</span> <span class="k">of</span> <span class="nx">off</span><span class="o">-</span><span class="nx">diagonal</span> <span class="nx">blocks</span><span class="p">;</span> <span class="nx">B</span> <span class="nx">terms</span><span class="p">)</span>
    <span class="nx">decay_states</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">((</span><span class="nx">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="nx">A_cumsum</span><span class="p">))</span>
    <span class="nx">states</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="dl">"</span><span class="s2">bclhn,bhcl,bclhp-&gt;bchpn</span><span class="dl">"</span><span class="p">,</span> <span class="nx">B</span><span class="p">,</span> <span class="nx">decay_states</span><span class="p">,</span> <span class="nx">X</span><span class="p">)</span>

    <span class="err">#</span> <span class="mi">3</span><span class="p">.</span> <span class="nx">Compute</span> <span class="nx">the</span> <span class="nx">inter</span><span class="o">-</span><span class="nx">chunk</span> <span class="nx">SSM</span> <span class="nx">recurrence</span><span class="p">;</span> <span class="nx">produces</span> <span class="nx">correct</span> <span class="nx">SSM</span> <span class="nx">states</span> <span class="nx">at</span> <span class="nx">chunk</span> <span class="nx">boundaries</span>
    <span class="err">#</span> <span class="p">(</span><span class="nx">middle</span> <span class="nx">term</span> <span class="k">of</span> <span class="nx">factorization</span> <span class="k">of</span> <span class="nx">off</span><span class="o">-</span><span class="nx">diag</span> <span class="nx">blocks</span><span class="p">;</span> <span class="nx">A</span> <span class="nx">terms</span><span class="p">)</span>
    <span class="k">if</span> <span class="nx">initial_states</span> <span class="nx">is</span> <span class="nx">None</span><span class="p">:</span>
        <span class="nx">initial_states</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="nx">states</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span>
    <span class="nx">states</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nx">initial_states</span><span class="p">,</span> <span class="nx">states</span><span class="p">],</span> <span class="nx">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nx">decay_chunk</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="nx">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="nx">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))))</span>
    <span class="nx">new_states</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="dl">"</span><span class="s2">bhzc,bchpn-&gt;bzhpn</span><span class="dl">"</span><span class="p">,</span> <span class="nx">decay_chunk</span><span class="p">,</span> <span class="nx">states</span><span class="p">)</span>
    <span class="nx">states</span><span class="p">,</span> <span class="nx">final_state</span> <span class="o">=</span> <span class="nx">new_states</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="nx">new_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="err">#</span> <span class="mi">4</span><span class="p">.</span> <span class="nx">Compute</span> <span class="nx">state</span> <span class="o">-&gt;</span> <span class="nx">output</span> <span class="nx">conversion</span> <span class="nx">per</span> <span class="nx">chunk</span>
    <span class="err">#</span> <span class="p">(</span><span class="nx">left</span> <span class="nx">term</span> <span class="k">of</span> <span class="nx">low</span><span class="o">-</span><span class="nx">rank</span> <span class="nx">factorization</span> <span class="k">of</span> <span class="nx">off</span><span class="o">-</span><span class="nx">diagonal</span> <span class="nx">blocks</span><span class="p">;</span> <span class="nx">C</span> <span class="nx">terms</span><span class="p">)</span>
    <span class="nx">state_decay_out</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nx">A_cumsum</span><span class="p">)</span>
    <span class="nx">Y_off</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="dl">'</span><span class="s1">bclhn,bchpn,bhcl-&gt;bclhp</span><span class="dl">'</span><span class="p">,</span> <span class="nx">C</span><span class="p">,</span> <span class="nx">states</span><span class="p">,</span> <span class="nx">state_decay_out</span><span class="p">)</span>

    <span class="err">#</span> <span class="nx">Add</span> <span class="nx">output</span> <span class="k">of</span> <span class="nx">intra</span><span class="o">-</span><span class="nx">chunk</span> <span class="nx">and</span> <span class="nx">inter</span><span class="o">-</span><span class="nx">chunk</span> <span class="nf">terms </span><span class="p">(</span><span class="nx">diagonal</span> <span class="nx">and</span> <span class="nx">off</span><span class="o">-</span><span class="nx">diagonal</span> <span class="nx">blocks</span><span class="p">)</span>
    <span class="nx">Y</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="nx">Y_diag</span><span class="o">+</span><span class="nx">Y_off</span><span class="p">,</span> <span class="dl">"</span><span class="s2">b c l h p -&gt; b (c l) h p</span><span class="dl">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="nx">Y</span><span class="p">,</span> <span class="nx">final_state</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code>:</p> <d-code block="" language="javascript"> def segsum(x): """Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix, which is equivalent to a scalar SSM.""" T = x.size(-1) x_cumsum = torch.cumsum(x, dim=-1) x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :] mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool), diagonal=0) x_segsum = x_segsum.masked_fill(~mask, -torch.inf) return x_segsum def ssd(X, A, B, C, block_len=64, initial_states=None): """ Arguments: X: (batch, length, n_heads, d_head) A: (batch, length, n_heads) B: (batch, length, n_heads, d_state) C: (batch, length, n_heads, d_state) Return: Y: (batch, length, n_heads, d_head) """ assert X.dtype == A.dtype == B.dtype == C.dtype assert X.shape[1] % block_len == 0 # Rearrange into blocks/chunks X, A, B, C = [rearrange(x, "b (c l) ... -&gt; b c l ...", l=block_len) for x in (X, A, B, C)] A = rearrange(A, "b c l h -&gt; b h c l") A_cumsum = torch.cumsum(A, dim=-1) # 1. Compute the output for each intra-chunk (diagonal blocks) L = torch.exp(segsum(A)) Y_diag = torch.einsum("bclhn,bcshn,bhcls,bcshp-&gt;bclhp", C, B, L, X) # 2. Compute the state for each intra-chunk # (right term of low-rank factorization of off-diagonal blocks; B terms) decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum)) states = torch.einsum("bclhn,bhcl,bclhp-&gt;bchpn", B, decay_states, X) # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries # (middle term of factorization of off-diag blocks; A terms) if initial_states is None: initial_states = torch.zeros_like(states[:, :1]) states = torch.cat([initial_states, states], dim=1) decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0)))) new_states = torch.einsum("bhzc,bchpn-&gt;bzhpn", decay_chunk, states) states, final_state = new_states[:, :-1], new_states[:, -1] # 4. Compute state -&gt; output conversion per chunk # (left term of low-rank factorization of off-diagonal blocks; C terms) state_decay_out = torch.exp(A_cumsum) Y_off = torch.einsum('bclhn,bchpn,bhcl-&gt;bclhp', C, states, state_decay_out) # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks) Y = rearrange(Y_diag+Y_off, "b c l h p -&gt; b (c l) h p") return Y, final_state </d-code> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"gu-group/gu-group.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Goomba AI Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>