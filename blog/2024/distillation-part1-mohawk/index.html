<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cross-Architecture Distillation Part I - The MOHAWK Framework | Goomba Lab </title> <meta name="author" content="Goomba AI Lab"> <meta name="description" content="Homepage of the Goomba AI Lab @ CMU MLD. # A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/goomba_light.png?68e59a389531e710f3507b5f12827027"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goombalab.github.io/blog/2024/distillation-part1-mohawk/"> <script src="/assets/js/theme.js?daf0da4e15ae2df6b4045ab97d680f8d"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Cross-Architecture Distillation Part I - The MOHAWK Framework",
            "description": "",
            "published": "August 20, 2024",
            "authors": [
              
              {
                "author": "Aviv Bick*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Kevin Y. Li*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Eric P. Xing",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, MBZUAI",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "J. Zico Kolter",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Gu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goomba Lab <img src="/assets/img/goomba_transparent.png" width="30" height="30"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Cross-Architecture Distillation Part I - The MOHAWK Framework</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#preliminaries">Preliminaries</a> </div> <ul> <li> <a href="#mamba-2">Mamba-2</a> </li> </ul> <div> <a href="#mohawk-method">MOHAWK Method</a> </div> <ul> <li> <a href="#stage-1-matrix-orientation">Stage 1: Matrix Orientation</a> </li> <li> <a href="#stage-2-hidden-state-alignment">Stage 2: Hidden-State Alignment</a> </li> <li> <a href="#stage-3-weight-transfer-and-knowledge-distillation">Stage 3: Weight-Transfer and Knowledge Distillation</a> </li> </ul> <div> <a href="#approximating-self-attention">Approximating Self-Attention</a> </div> <ul> <li> <a href="#linear-attention-and-ssd">Linear Attention and SSD</a> </li> <li> <a href="#general-semi-separable-and-toeplitz">General Semi-separable and Toeplitz</a> </li> <li> <a href="#empirical-approximation">Empirical Approximation</a> </li> </ul> </nav> </d-contents> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-480.webp 480w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-800.webp 800w,/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/fig:phi-mamba-arch.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2408.10189" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/goombalab/phi-mamba" rel="external nofollow noopener" target="_blank">Code</a>]</p> <ol> <li>Part I - MOHAWK</li> <li><a href="/blog/2024/distillation-part2-phi-mamba/">Part II - Phi-Mamba</a></li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p>We start off by summarizing some important aspects from <d-cite key="ssd"></d-cite>, specifically the sequence transformation/mixer viewpoint and the Mamba-2 SSM variant.</p> <p><strong>Definition:</strong> A <em>sequence transformation/mixer</em> refers to a parameterized map on sequences $Y = f_{\theta}(X)$ where $X, Y \in \mathbb{R}^{(T, P)}$ and $\theta$ is an arbitrary collection of parameters. $T$ represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_t, Y_t \in \mathbb{R}^P$.</p> <p>In layman’s terms, <em>sequence mixers</em> aggregate tokens across various time steps. This ability to learn temporal interactions and information forms the foundation of modern deep sequence models, like Transformers.</p> <p><strong>Definition:</strong> <em>Matrix mixers</em> are a specific type of sequence mixers that can be represented as $Y = MX$ for matrix $M \in \mathbb{R}^{(T,T)}$.</p> <p>Examples of <em>matrix mixers</em> which fall under this definition include vanilla self-attention, where $M = \text{Softmax}(\mathbf{Q}\mathbf{K}^\top)$ <d-cite key="vaswani2023attention"></d-cite>, linear attention <d-cite key="katharopoulos2020transformers"></d-cite>, and Toeplitz matrices <d-cite key="qin2023toeplitz"></d-cite>.</p> <h3 id="mamba-2">Mamba-2</h3> <p>Mamba-2 <d-cite key="ssd"></d-cite> is a recent variant of Structured State Space Models (SSMs) <d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2023thesis"></d-cite> which can be viewed as a matrix mixer that can be applied onto an input sequence in subquadratic time due to structured matrix multiplication. Mamba-2 is a time-varying SSM, defined as</p> \[\begin{aligned} h_{t+1} &amp;= A_t h_t + B_t x_t \\ y_t &amp;= C_t h_t \end{aligned}\] <p>where $B_t$ and $C_t$, like in Mamba-1 <d-cite key="gu2023mamba"></d-cite>, are input-dependent projections, but $A_t$ is the identity matrix $I$ multiplied by a scalar $\alpha_t$. Importantly, Mamba-2 identified the <em>Structured State Space Duality (SSD)</em> connection which found that specific variants of SSMs can be viewed as a form of causal linear attention <d-cite key="katharopoulos2020transformers"></d-cite>.</p> <p>Formally, the Mamba-2 SSD matrix mixer can be represented as</p> \[\begin{equation} \label{eq:ssd-matrix-mixer} \begin{aligned} \begin{bmatrix} \alpha_{1} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{2:1} &amp; \alpha_{2} &amp; 0 &amp; \cdots &amp; 0 \\ \alpha_{3:1} &amp; \alpha_{3:2} &amp; \alpha_{3} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \alpha_{n:1} &amp; \alpha_{n:2} &amp; \alpha_{n:3} &amp; \cdots &amp; \alpha_{n} \end{bmatrix} \circ (C \cdot B^\top) \cdot X \end{aligned} \end{equation}\] <p>where $\alpha_{t:i} = \alpha_{t-1} \cdot \alpha_{t-2} \cdots \alpha_{i}$.</p> <p>From this representation, one can see that Mamba-2 can be viewed as causal linear attention with a learnable causal mask!</p> <h2 id="mohawk-method">MOHAWK Method</h2> <p>Inspired by the <em>matrix mixer</em> viewpoint which provides a common lense for viewing the key components of various architectures, we introduce the <strong>MOHAWK</strong> framework for cross-architectural distillation, which is composed of three stages:</p> <ol> <li> <strong>M</strong>atrix <strong>O</strong>rientation</li> <li> <strong>H</strong>idden-State <strong>A</strong>lignment</li> <li> <strong>W</strong>eight-Transfer and <strong>K</strong>nowledge Distillation</li> </ol> <p>These three sequential stages distill the student model from the bottom up, steadily increasing the number of components distilled into at each stage until the end student model has been distilled end-to-end. We find that this multi-stage process is much more effective than traditional knowledge distillation.</p> <p>Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba –based on the Phi-1.5 model <d-cite key="gunasekar2023textbooks"></d-cite>– along with the specifics of its distillation process.</p> <p>For clarity, we refer to the term <em>block</em> as a repeating component that forms the backbone of the end-to-end model. <em>Blocks</em> are composed of layers, for instance the Llama block is composed of a self-attention layer followed by a MLP layer. <em>Layers</em> can be composed of numerous subcomponents, like the self-attention layer, which encompasses the projections and the self-attention mechanism, and the Mamba layer, which includes the projections, convolution, and SSM mixer, etc.</p> <h3 id="stage-1-matrix-orientation">Stage 1: Matrix Orientation</h3> <p>We begin the first stage of MOHAWK by matching the matrix mixer of both the student and teacher. Prior to directly aligning the matrix mixers themselves, we first adjust the <em>matrix mixer layer</em> to be analogous to that of the teacher’s, i.e., structurally both layers are the same except the matrix mixer component. We then minimize the distance between the matrix mixer of the teacher and student layers, which can be expressed as the following equation:</p> <p>\(\begin{equation} \label{eq:matrix-orientation-minimization} \min_{\mathbf{\phi}} \|\mathrm{TeacherMixer}(\mathbf{u}) - \mathrm{StudentMixer}_{\boldsymbol{\phi}}(\mathbf{u})\|_F \end{equation}\) where $\phi$ represents the parameters in the layer and $\mathbf{u}$ is the shared input derived from the teacher model. The stage ensures that the student can closely approximate the teacher’s matrix mixer layer which sets a strong foundation for teacher matching in subsequent stages of the MOHAWK process.</p> <p>For Phi-Mamba: Because the student model uses the Mamba-2 mixer, we initialize the convolution to identity and discarded the nonlinear activation after the convolution to ensure the components upstream of the matrix mixers roughly equivalent to the self-attention layer. The loss calculate was between the self-attention matrix of the teacher and the “unraveled” SSM matrix as shown in Equation \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="stage-2-hidden-state-alignment">Stage 2: Hidden-State Alignment</h3> <p>After optimizing Equation \eqref{eq:matrix-orientation-minimization} in Stage 1, Stage 2 proceeds to match the outputs of the student and teacher blocks.</p> <p>\(\begin{equation} \label{eq:hidden-state-minimization} \min_{\mathbf{\phi}} \|\mathrm{AttnBlock}(\mathbf{u}) - \mathrm{StudentMixerBlock}_{\boldsymbol{\phi}}(\mathbf{u})\|_2 \end{equation}\) where once again the inputs are the same. Like Stage 1, Stage 2 can be run in parallel. We find that the distance between the layer outputs is strongly correlated with the student model’s ability to recover the teacher model’s knowledge.</p> <p>For Phi-Mamba: To keep the block architectures as similar as possible, we initialized the Mamba-2 gate to be a value of 1 to simulate Phi’s lack of gating and removed the norm prior to the output projection.</p> <h3 id="stage-3-weight-transfer-and-knowledge-distillation">Stage 3: Weight-Transfer and Knowledge Distillation</h3> <p>The final stage aims to fine-tune the entire student model to match the performance of the teacher. This stage is critical for mending the potential discrepancies between post-Stage 2 blocks. We also initialize information dense components of the student model, in particular the MLPs, embedding, and LM head, before fine-tuning the student end-to-end. Given the weight transfer of critical architectural components, the overall block structure of the student mirror that of the teacher model, e.g., our student model has the MLPs and matrix mixer layers in parallel. Finally, we use knowledge distillation loss <d-cite key="hinton2015distilling"></d-cite> to encourage the student to imitate the teacher’s distribution:</p> \[\begin{equation} \min_{\mathbf{\phi}} \mathbf{\mathcal{L}}_{\mathrm{CE}}\big(\mathrm{Teacher}(\mathbf{x}), \mathrm{Student}_{\boldsymbol{\phi}} (\mathbf{x})\big) \end{equation}\] <p>For Phi-Mamba: We create a new Phi-Mamba block that has the same parallel MLP-matrix mixer layer structure as the original Phi-1.5 block. We copy over the MLP and norm weights, token embeddings, and language model head and pre-head norm as it has been hypothesized that much of a model’s information is stored in these components. We also find that the MLPs can be frozen after the transfer with only a slight decrease in performance but reduce the number of trainable parameters by more than half!</p> <h2 id="approximating-self-attention">Approximating Self-Attention</h2> <p>With the MOHAWK method we can now distill from any quadratic self-attention model to any model that utilizes a <em>matrix mixer</em> for sequential modeling. But, a caveat is that the performance of the student model is inherently constrained by the expressivity of its matrix mixer. So why did we decide to use the Mamba-2 mixer instead of an alternative like linear attention or gated convolution? In this next section, we will empirically explore Mamba-2’s ability to approximate the self-attention matrix $\text{Softmax}(QK^\top)$ and compare it to some other popular sub-quadratic matrix mixer families. We describe a couple of them below.</p> <h3 id="linear-attention-and-ssd">Linear Attention and SSD</h3> <p>When describing linear attention matrices, we can utilize the fact that both $Q$ and $K$ are token-dependent projections of some input $x \in \mathbb{R}^{d_{in}}$ onto $\mathbb{R}^{d_{out}}$, and therefore the rank of $Q$ and $K$ are bounded by $\min{ { d_{in}, d_{out} } }$ For multi-head linear attention, $d_{out}$, which corresponds to the head dimension, is typically a small value (e.g., $64$ and $128$ for Phi-1.5 and Llama2-7b-Chat respectively). Thus, we approximate linear attention matrix mixers using causal low-rank matrices $\mathbf{L \circ Q K}^\top$, where $\mathbf{L}$ is a lower-triangular causal mask of 1s, and $\mathbf{Q}$, $\mathbf{K}$ are in $\mathbb{R}^{n \times d}$ with $d \ll n$.</p> <p>For the multi-head Mamba-2 matrix family, we utilize the state space dual (SSD) layer in a manner similar to the previous linear attention class, but imbuing the causal matrix $\mathbf{L}$ with an $n$-degree rolling multiplicative structure for $\mathrm{SSD}$. This can be seen as a more expressive mask that generalizes the lower-triangular, ones-only causal mask \eqref{eq:ssd-matrix-mixer}.</p> <h3 id="general-semi-separable-and-toeplitz">General Semi-separable and Toeplitz</h3> <p>To approximate the general class of semi-separable matrices (abbreviated as “SSM” in the following table), we utilize <em>balanced truncation</em>. This method is used in the field of time-invariant Dynamical System model reduction <d-cite key="BTSurvery"></d-cite> and has been modified for use in time-varying systems <d-cite key="TVBTSurvery"></d-cite>. Similarly, for the family of Toeplitz matrices, which represent a convolution operation, we apply a causal mask, the same one used for causal low-rank matrices, on top a Toeplitz matrix.</p> <h3 id="empirical-approximation">Empirical Approximation</h3> <p>To empirically validate the expressiveness of the four aforementioned families, we sample 1,000 attention matrices, each consisting of 512 tokens, from the Llama2-7B-Chat <d-cite key="touvron2023llama"></d-cite> model on four different datasets. One attention head, and its respective attention matrix, from each layer was chosen at random. Both (causal) low-rank (LR) and SSD matrix families were approximated with 10,000 steps of gradient descent per sample. SSM and Toeplitz were both calculated without using gradient descent using balanced truncation and a simple heuristic respectively. We calculate the Frobenius distance between each “ground truth” self-attention matrix and the approximated matrix of each family.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-480.webp 480w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-800.webp 800w,/assets/img/2024-08-20-mohawk/table:attn-matrix-approx-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/table:attn-matrix-approx.png" width="100%" height="auto" title="Matrix Approximation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Self Attention matrix approximation by structured matrix mixers</figcaption> </figure> <p>Given the previous table’s experiment was conducted in a very controlled setting, we further explore the ability of the various families’ abilities to approximate the self-attention matrix within a language model. We replace the self-attention matrix mixers of a Phi-1.5 model with either input-dependent Toeplitz, causal low-rank, or SSD (our Mamba-2 variant) matrix mixers, and ran the second and third stages of our MOHAWK procedure for 1B tokens each.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-480.webp 480w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-800.webp 800w,/assets/img/2024-08-20-mohawk/table:mixer-structure-abl-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-20-mohawk/table:mixer-structure-abl.png" width="100%" height="auto" title="Matrix Structure Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Evaluations of various structured matrices on downstream tasks</figcaption> </figure> <p>We find that there is a constant correlation between the self attention approximation abilities (measured via projection distances) of a matrix family and the downstream performance metrics (accuracy) of the matrix mixer integrated into an end-to-end language model. This finding that more expressive matrix mixers lead to more effective models is echoed in <d-cite key="hwang2024hydrabidirectionalstatespace"></d-cite>.</p> <h2 id="next-up">Next Up</h2> <p>The <a href="/blog/2024/distillation-part2-phi-mamba/">following section</a> will cover MOHAWK in action, distilling our final Phi-Mamba and Hybrid-Phi-Mamba models, and explore the training laws regarding each stage of MOHAWK.</p> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/mohawk.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Goomba AI Lab. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>