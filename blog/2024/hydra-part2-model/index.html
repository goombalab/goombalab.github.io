<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hydra Part II - The Model | Goomba Lab </title> <meta name="author" content="Goomba AI Lab"> <meta name="description" content="Homepage of the Goomba AI Lab @ CMU MLD. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/goomba_light.png?68e59a389531e710f3507b5f12827027"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goombalab.github.io/blog/2024/hydra-part2-model/"> <script src="/assets/js/theme.js?daf0da4e15ae2df6b4045ab97d680f8d"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Hydra Part II - The Model",
            "description": "",
            "published": "July 16, 2024",
            "authors": [
              
              {
                "author": "Sukjun Hwang*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Aakash Lahoti*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tri Dao",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Princeton",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Gu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia AI",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goomba Lab <img src="/assets/img/goomba_transparent.png" width="30" height="30"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Hydra Part II - The Model</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#ssms-are-semiseparable-matrix-mixers">SSMs Are Semiseparable Matrix Mixers</a> </div> <div> <a href="#quasiseparable-matrices">Quasiseparable Matrices</a> </div> <ul> <li> <a href="#quasiseparable-matrices-supset-semiseparable-and-low-rank-matrices">Quasiseparable Matrices $\supset$ Semiseparable and Low-Rank Matrices</a> </li> <li> <a href="#quasiseparable-matrices-supset-two-separate-ssms">Quasiseparable Matrices $\supset$ Two Separate SSMs</a> </li> </ul> <div> <a href="#hydra">Hydra</a> </div> <ul> <li> <a href="#implementation">Implementation</a> </li> <li> <a href="#performance">Performance</a> </li> </ul> <div> <a href="#epilogue">Epilogue</a> </div> </nav> </d-contents> <div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/logo_trans-480.webp 480w,/assets/img/2024-07-16-hydra/logo_trans-800.webp 800w,/assets/img/2024-07-16-hydra/logo_trans-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-16-hydra/logo_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2407.09941" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/goombalab/hydra" rel="external nofollow noopener" target="_blank">Code</a>]</p> <ol> <li><a href="/blog/2024/hydra-part1-matrix-mixer/">Part I - Matrix Mixer Framework</a></li> <li>Part II - Hydra: The Model</li> </ol> <p>In our previous post, we systematically compared various sequence models with different mixer matrices, and the quasiseparable SAM mixer emerged as the top performer. So, what exactly is it?</p> <h2 id="recap-ssms-are-semiseparable-matrix-mixers">Recap: SSMs Are Semiseparable Matrix Mixers</h2> <p>Before diving into the details of quasiseparable SAM mixers, let’s briefly revisit some key findings from <a href="https://arxiv.org/abs/2405.21060" rel="external nofollow noopener" target="_blank">Mamba-2</a><d-cite key="ssd"></d-cite>. Recently, Mamba-2 has shown that the mixer matrices of SSMs are inherently parametrized to one of the fundamental structured matrix classes – semiseparable matrices.</p> <blockquote> <p><strong>Defintion</strong> of Semiseparable Matrices <br> A lower triangular matrix $\textbf{M}$ is $N$-semiseparable iff any submatrix from the lower triangle (on or below the diagonal) has a rank of at most $N$. See (a) in the figure below.</p> </blockquote> <p>So why are SSMs semiseparable matrix mixers? Using our previously defined matrix mixer framework, we can represent SSMs as follows:</p> \[\begin{align} \textbf{y}_t &amp;= \sum^{t}_{s=0} \textbf{C}^T_t \left(\prod_{k=s+1}^{i} \textbf{A}_{k}\right) \textbf{B}_s \textbf{x}_s \\ \\ \textbf{Y} &amp;= \text{SSM}(\textbf{A}, \textbf{B}, \textbf{C})(\textbf{X}) = \textbf{M} \textbf{X} \space ,\\ \\ m_{ij} &amp; = \textbf{c}^T_i \textbf{A}_i \cdots \textbf{A}_{j+1} \textbf{b}_j \end{align}\] <p>where each matrix $\textbf{A}_i \in \mathbb{R}^{N \times N}$ and vector $\textbf{c}_i, \textbf{b}_i \in \mathbb{R}^{N \times 1}$. This decomposition shows that SSMs are indeed semiseparable mixers. [If you are not familiar with this concept, we recommend checking out this <a href="/blog/2024/mamba2-part2-theory/">blog post</a> for a great explanation.]</p> <p>Semiseparable matrices are an excellent choice for mixer matrices – they are sub-quadratic, performant, and can be extended to handle sequences of various lengths. However, there’s one significant limitation: due to their definition, the upper right triangle of semiseparable matrices is filled with zeros, making them inevitably causal. This limitation makes SSMs incapable of <strong>bidirectional sequence processing</strong>.</p> <p>Why is bidirectionality important? Bidirectional processing is crucial for several reasons. One major reason is its importance in handling multiple modalities, such as processing 2D images. Without bidirectionality, models can’t fully leverage information from both past and future contexts within a sequence, which is essential for comprehensive data analysis across various applications.</p> <p>A straightforward way to make SSMs bidirectional is to use two separate SSMs: one for forward sequence processing and one for reverse sequence processing. There are several approaches to combine their outputs, such as adding, multiplying, or concatenating them <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite><d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite>. While these heuristics can work, they lack a principled design philosophy, leading to different heuristics being used for different tasks without a systematic approach.</p> <p>But what if we could use the matrix mixer framework to systematically derive the optimal $\textbf{M}$? Absolutely, we can! In addition to the three desiderata we discussed previously – sub-quadratic complexity, extendability, and high-performance – let’s add one more requirement: <strong>bidirectionality</strong>. For the mixer matrix to achieve bidirectionality, it must feature upper triangular components. So, how should we fill them?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/semiquasi_trans-480.webp 480w,/assets/img/2024-07-16-hydra/semiquasi_trans-800.webp 800w,/assets/img/2024-07-16-hydra/semiquasi_trans-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-16-hydra/semiquasi_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="structured-matrix-of-our-choice-quasiseparable-matrices">Structured Matrix of Our Choice: Quasiseparable Matrices</h2> <p>For our bidirectional sequence mixer, we choose quasiseparable matrices. So, what makes quasiseparable matrices stand out? Let’s start by looking at their definition.</p> <blockquote> <p><strong>Defintion</strong> of Quasiseparable Matrices by the Rank Characterization. <br> A matrix $\textbf{M}$ is $N$-quasiseparable iff any submatrix from either the strictly upper or lower triangle (off from the diagonal) has a rank of at most $N$. See (b) in the figure above.</p> </blockquote> <p>At first glance, this definition might seem similar to that of semiseparable matrices. To clarify, let’s highlight the key differences between quasiseparable and semiseparable matrices:</p> <table> <thead> <tr> <th> </th> <th><strong>Semiseparable</strong></th> <th><strong>Quasiseparable</strong></th> </tr> </thead> <tbody> <tr> <td>(I)</td> <td>any submatrix from <em>the lower triangle</em> </td> <td>any submatrix from either the strictly <em>upper or lower triangle</em> </td> </tr> <tr> <td>(II)</td> <td> <em>on or below</em> the diagonal</td> <td> <em>off</em> from the diagonal</td> </tr> </tbody> </table> <h3 id="quasiseparable-matrices-supset-semiseparable-and-low-rank-matrices">Quasiseparable Matrices $\supset$ Semiseparable and Low-Rank Matrices</h3> <p>Although the differences between quasiseparable and semiseparable matrices might seem subtle, they lead to significant improvements in expressivity. According to difference <strong>(I)</strong>, semiseparable matrices zero out the upper triangular elements, while quasiseparable matrices extend to include these elements, enabling bidirectionality. Consequently, semiseparable matrices can only generalize mixers that use causal low-rank matrices, such as Linear Attention, whereas quasiseparable matrices generalize typical low-rank matrices. Moreover, both differences <strong>(I)</strong> and <strong>(II)</strong> mean that quasiseparable matrices not only generalize but also extend semiseparable matrices.</p> <ul> <li><strong><em>Quasiseparable matrices generalize low-rank matrices.</em></strong></li> <li><strong><em>Quasiseparable matrices generalize and extend semiseparable matrices.</em></strong></li> </ul> <h3 id="quasiseparable-matrices-supset-two-separate-ssms">Quasiseparable Matrices $\supset$ Two Separate SSMs</h3> <p>We now understand that for bidirectional processing scenarios, quasiseparable mixers are indeed better than semiseparable matrices. But what makes quasiseparable mixers superior to the bidirectional extensions using two separate SSMs?</p> <p>Heuristic variants that use the Hadamard product and concatenation <d-cite key="bigs"></d-cite><d-cite key="mssm"></d-cite> are difficult to analyze systematically within the matrix mixer framework. Moreover, concatenation variants double the number of output channels, necessitating additional parameters for reducing the number of channels.</p> <p>In contrast, addition-based variants <d-cite key="sashimi"></d-cite><d-cite key="vision_mamba"></d-cite><d-cite key="caduceus"></d-cite> can be formulated using the matrix mixer framework, as shown in (c) of the figure above, which resembles quasiseparable matrices in (d). However, difference <strong>(II)</strong> highlights that the diagonals of semiseparable matrices are also constrained by the rank characterization, and consequently, so are the diagonals of addition-based extensions. Quasiseparable matrices, on the other hand, do not have this constraint on the diagonals, allowing them to be complete free parameters. This flexibility makes quasiseparable matrices more mathematically expressive than addition-based bidirectional extensions.</p> <ul> <li><strong><em>Quasiseparable matrices are strictly more expressive than mixer matrices of addition-based bidirectional SSMs.</em></strong></li> </ul> <p>This property of complete freedom in the diagonals of quasiseparable matrices is more evident in another definition of quasiseparable matrices:</p> <blockquote> <p>A matrix $\textbf{M}$ is $N$-quasiseparable if each element $m_{ij}$ satisfies:</p> \[\begin{equation} m_{ij} = \begin{cases} \overrightarrow{\textbf{c}^{T}_{i}} \overrightarrow{\textbf{A}_i} \cdots \overrightarrow{\textbf{A}_{j+1}} \overrightarrow{\textbf{b}_{j}}, &amp; \text{if } i &gt; j \\ \delta_{i}, &amp; \text{if } i = j \\ \overleftarrow{\textbf{c}^{T}_{i}} \overleftarrow{\textbf{A}_{i}} \cdots \overleftarrow{\textbf{A}_{j-1}} \overleftarrow{\textbf{b}_{j}}, &amp; \text{if } i &lt; j\\ \end{cases},\\ \end{equation}\] <p>where each $\delta_i$ is a scalar, $\textbf{b}_i, \textbf{c}_i \in \mathbb{R}^{N \times 1}$, and $\textbf{A}_i \in \mathbb{R}^{N \times N}$.</p> </blockquote> <p>These are the actual results we obtained for the C4 and GLUE benchmark, along with the validation loss curve. Supported by these theoretical claims, our Hydra model, which uses a quasiseparable mixer matrix, indeed has shown superior performance to previous heuristic bidirectional extensions!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/bidirectionality_trans-480.webp 480w,/assets/img/2024-07-16-hydra/bidirectionality_trans-800.webp 800w,/assets/img/2024-07-16-hydra/bidirectionality_trans-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-16-hydra/bidirectionality_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="hydra-our-main-bidirectional-sequence-mixer">Hydra: Our Main Bidirectional Sequence Mixer</h2> <h3 id="implementation">Implementation</h3> <p>Now that we’ve confirmed quasiseparable matrices as the go-to mixer matrices, we fully leverage them to propose the two-headed Mamba – <strong><em>Hydra</em></strong>. Take a look at part (d) in the figure above, which illustrates the mixer matrix of Hydra, and also notice it’s also our previosly defined SAM! Utilizing an SSM, which is a semiseparable mixer, we can implement Hydra with the following formula: \(QS(\textbf{X}) = \texttt{shift}(SS(\textbf{X})) + \texttt{flip}(\texttt{shift}(SS(\texttt{flip}(\textbf{X})))) + \textbf{DX},\) where $\textbf{X}$ is the input sequence, $\texttt{flip}(\cdot)$ denotes a function that reverses the input, $\texttt{shift}(\cdot)$ denotes a right-shift function, and $\textbf{D} = \text{diag}(\delta_1, \cdots, \delta_L)$ represents the diagonal elements of $QS$. Here, $QS(\cdot)$ and $SS(\cdot)$ are the mixer matrix of Hydra and an SSM, respectively.</p> <p>Among the various iterations of SSMs, we adopt the latest one – SSD from Mamba-2. Since SSMs are sub-quadratic, this simple implementation maintains the sub-quadratic cost. Compared to heuristic extensions that use two separate SSMs for bidirectionality, Hydra shares the input processing function $f_X$ for forward and reverse sequence processing, which nearly halves the number of parameters.</p> <p>You can check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/hydra.py" rel="external nofollow noopener" target="_blank">the actual code</a>. To sum up:</p> <ul> <li>Hydra’s matrix mixer is meticulously parameterized to be a quasiseparable matrix with enhanced expressivity through shift operations.</li> <li>Hydra is sub-quadratic and super easy to implement using existing SSM implementations like Mamba.</li> <li>Hydra greatly reduces parameter counts compared to bidirectional extensions using two SSMs.</li> </ul> <h3 id="performance">Performance</h3> <p>We have seen that Hydra outperforms heuristic bidirectional extensions of SSMs, but how does it compare to state-of-the-art methods? Surprisingly, Hydra surpasses all previous models, including Transformer-based models such as BERT and ViT. When matched for the number of parameters, Hydra consistently shows the best performance across both NLP and Vision domains, highlighting its versatility.</p> <table> <tr> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #4dabf7">NLP</td> <td colspan="3" style="font-weight:bold; text-align:center; background-color: #69db7c">Vision</td> </tr> <tr> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">GLUE Avg</td> <td style="font-weight:bold;">Method</td> <td style="font-weight:bold;"># Params</td> <td style="font-weight:bold;">Top-1 (%)</td> </tr> <tr> <td style="font-weight:bold;">BERT<d-cite key="bert"></d-cite> </td> <td>110M</td> <td>83.5</td> <td style="font-weight:bold;">ViT-B<d-cite key="vit"></d-cite> </td> <td>87M</td> <td>78.8</td> </tr> <tr> <td style="font-weight:bold;">MLP-Mixer<d-cite key="mlpmixer"></d-cite> </td> <td>112M</td> <td>77.5</td> <td style="font-weight:bold;">S4-ViT-B<d-cite key="s4"></d-cite><d-cite key="s4d"></d-cite> </td> <td>89M</td> <td>79.4</td> </tr> <tr> <td style="font-weight:bold;">FNet<d-cite key="fnet"></d-cite> </td> <td>112M</td> <td>75.8</td> <td style="font-weight:bold;">Hyena-ViT-B<d-cite key="hyena"></d-cite> </td> <td>88M</td> <td>78.4</td> </tr> <tr> <td style="font-weight:bold;">M2<d-cite key="m2"></d-cite> </td> <td>116M</td> <td>80.9</td> <td style="font-weight:bold;">Mamba-ViT-B<d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite> </td> <td>89M</td> <td>79.1</td> </tr> <tr> <td style="background-color: #f783ac; font-weight:bold;">Hydra</td> <td>112M</td> <td>84.3</td> <td style="background-color: #f783ac; font-weight:bold;">Hydra-ViT-B</td> <td>91M</td> <td>81.0</td> </tr> </table> <p>On the GLUE benchmark, Hydra outperforms BERT by 0.8 points. On ImageNet-1K, Hydra improves by 2.2 points over ViT. These results underscore Hydra’s capability to set new standards in both natural language processing and image classification tasks!</p> <h2 id="epilogue">Epilogue</h2> <p>Lately, the demand for large-scale computation has never been higher. Since the emergence of Mamba, interests in structured matrices has surged, and now is their time to shine. Structured matrices offer an exciting approach to efficient and powerful input processing, similar to how M2 improved over MLP-Mixer.</p> <p>In recent years, we’ve seen numerous groundbreaking works showcasing promising results using structured matrices like Mamba. If the community strives together, just as we have spent about seven years investigating and improving Transformers, we believe there is enormous potential for further advancements through systematic exploration of different structured matrices, along with better optimized training settings (which have been fine-tuned for Transformers).</p> <p>A big shout-out to the recent <a href="https://arxiv.org/abs/2406.06248" rel="external nofollow noopener" target="_blank">BTT</a><d-cite key="btt"></d-cite> work, which systematically explores structured matrices for effective channel mixers. We were very excited to see this kind of systematic investigation, which is crucial for the continued advancement of better architectures.</p> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/june.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Goomba AI Lab. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cloud.umami.is/script.js" data-website-id="340bca3c-b84e-462f-98dd-f4f4629b9751"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>