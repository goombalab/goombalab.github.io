<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hydra Part I - Matrix Mixer Framework | Goomba Lab </title> <meta name="author" content="Goomba AI Lab"> <meta name="description" content="Homepage of the Goomba AI Lab @ CMU MLD. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/goomba_light.png?68e59a389531e710f3507b5f12827027"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goombalab.github.io/blog/2024/hydra-part1-matrix-mixer/"> <script src="/assets/js/theme.js?daf0da4e15ae2df6b4045ab97d680f8d"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Hydra Part I - Matrix Mixer Framework",
            "description": "",
            "published": "July 16, 2024",
            "authors": [
              
              {
                "author": "Sukjun Hwang*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Aakash Lahoti*",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Tri Dao",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Princeton",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Albert Gu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia AI",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goomba Lab <img src="/assets/img/goomba_transparent.png" width="30" height="30"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Hydra Part I - Matrix Mixer Framework</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#formalization-of-the-matrix-mixer-framework">Formalization of the Matrix Mixer Framework</a> </div> <div> <a href="#solution-for-sub-quadratic-complexity-structured-matrices">Solution for Sub-quadratic Complexity - Structured Matrices</a> </div> <div> <a href="#solution-for-all-desiderata-sequence-aligned-matrices-sam">Solution for All Desiderata - Sequence Aligned Matrices (SAM)</a> </div> <ul> <li> <a href="#variations">Variations</a> </li> </ul> <div> <a href="#impact-of-sam-parameterization">Impact of SAM Parameterization</a> </div> </nav> </d-contents> <div> <p>[<a href="https://arxiv.org/abs/2407.09941" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/goombalab/hydra" rel="external nofollow noopener" target="_blank">Code</a>]</p> <ol> <li>Part I - Matrix Mixer Framework</li> <li><a href="/blog/2024/hydra-part2-model/">Part II - Hydra: The Model</a></li> </ol> <p>Attention mechanisms<d-footnote>In this work, Attention<d-cite key="attention"></d-cite> exclusively refers to Self-Attention<d-cite key="transformer"></d-cite></d-footnote> have taken center stage in the world of sequence mixing, celebrated for their significant flexibility and performance. However, this power comes with a price: high computational and memory demands. Despite these challenges, attention has become the go-to solution for many applications.</p> <p>In modern state-of-the-art models, architectural designs typically split into two main components: the sequence mixer and the channel mixer. To illustrate, let‚Äôs look at the Transformer encoder architecture. It consists of two key elements: Multi-Head Attention and a Feed-Forward Network (FFN). The Multi-Head Attention serves as the sequence mixer, efficiently managing interactions across the input sequence. Meanwhile, the FFN acts as the channel mixer, processing information within each sequence element.</p> <p>Take a glance at the figure below to see this architecture in action. You‚Äôll notice how these components work together to create the robust models we rely on today.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_mixer_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_mixer_trans-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-16-hydra/matrix_mixer_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In our work, we study the large and important class of sequence mixers that can be represented as basic matrix multiplications: $\textbf{Y} = \textbf{M}\textbf{X}$. We call this approach <strong><em>the matrix mixer framework</em></strong>. This framework includes diverse and important classes of sequence models such as Attention, convolutions<d-cite key="ckconv"></d-cite><d-cite key="tnn"></d-cite>, and state-space models<d-cite key="s4"></d-cite><d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite>. For example, the typical self-attention mechanism, $\textbf{Y} = \text{softmax}(\textbf{Q}\textbf{K}^T)\textbf{V}$, can be seen as a special case where the matrix $\textbf{M}$ is defined as $\text{softmax}(\textbf{Q}\textbf{K}^T)$.</p> <p>Viewing sequence mixers through this lens has a significant advantage: designing new sequence mixers becomes a matter of finding the optimal matrix $\textbf{M}$. This perspective opens up a systematic way to explore and innovate in the field of sequence modeling.</p> <p>So, now the question is, what is a good $\textbf{M}$? Key desiderata for such a matrix would include:</p> <ul> <li>Efficiency: We want sub-quadratic matrix multiplication and parameterization to ensure our models run swiftly and handle long sequences with ease.</li> <li>Performance: The matrix mixer should match the high standards of Attention mechanisms in modeling diverse sequence data across various modalities.</li> <li>Flexibility: The solution should work well with sequences of different lengths (+ capable of both causal and bidirectional sequence modeling, which we will tackle in <a href="/blog/2024/hydra-part2-model/">Part II</a>)</li> </ul> <p>Check out the table below to see how various sequence mixers measure up. While several models like MLP-Mixer<d-cite key="mlpmixer"></d-cite>, FNet<d-cite key="fnet"></d-cite>, TNN<d-cite key="tnn"></d-cite>, LA<d-cite key="la"></d-cite>, and M2<d-cite key="m2"></d-cite> have been introduced, none of them fully meet all our criteria.</p> <table> <thead> <tr> <th>¬†</th> <th>Sub-quadratic</th> <th>Performance</th> <th>Flexibility</th> </tr> </thead> <tbody> <tr> <td>MLP-Mixer</td> <td>üò≠</td> <td>üò≠</td> <td>üò≠</td> </tr> <tr> <td>FNet</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>TNN</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>LA</td> <td>ü§ó</td> <td>üò≠</td> <td>ü§ó</td> </tr> <tr> <td>M2</td> <td>ü§ó</td> <td>üò≠</td> <td>üò≠</td> </tr> <tr> <td>Transformer</td> <td>üò≠</td> <td>ü§ó</td> <td>ü§ó</td> </tr> </tbody> </table> <p>As you can see, each of these models has its strengths and weaknesses, but none perfectly hit all the marks. This gap highlights the need for another approach in developing sequence mixers.</p> <blockquote> <p><strong>So, is it even possible to meet all three key criteria?</strong></p> </blockquote> <p>We believe the answer lies in examining <strong><em>the structures</em></strong> of the mixer matrix $\textbf{M}$. Our work begins with an in-depth theoretical and empirical analysis of various sequence mixers using the matrix mixer framework. We then extend this idea, offering a systematic approach to designing new sequence mixers. By fully leveraging this framework, we have developed <strong>multiple</strong> novel architectures, including a new bidirectional mixer named <strong><em>Hydra</em></strong>.</p> <p>Let‚Äôs dive into more details, which is outlined as follows:</p> <ul> <li>We study and formalize the matrix mixer framework, introducing new theoretical concepts about structures of $\textbf{M}$ that can capture such desiderata.</li> <li>Guided by the properties of different matrix classes, we introduce a series of sequence models with strong and predictable performance.</li> <li>We provide careful systematic studies on these matrix classes, comparing empirical performances by varying only the matrix mixer</li> </ul> <h2 id="formalization-of-the-matrix-mixer-framework">Formalization of the Matrix Mixer Framework</h2> <p>We begin by further formalizing our matrix mixer framework. While this framework can be applied to multi-head architectures, we will focus on the single-headed scenario here for simplicity.</p> <p>In essence, a sequence mixer transforms an input $\textbf{X} \in \mathbb{R}^{L \times C}$ into an output $\textbf{Y} \in \mathbb{R}^{L \times C}$, where $L$ is the sequence length and $C$ is the number of channels.</p> <ol> <li>Input preprocessing function: Denoted as $f_X \colon \mathbb{R}^{L \times C} \rightarrow \mathbb{R}^{L \times D}$, this function handles common data transformations before the mixing process.</li> <li>Matrix construction function: Denoted as $f_{\mathcal{M}} \colon \mathbb{R}^{L \times C} \times \Theta \rightarrow \mathcal{M} \subseteq \mathbb{R}^{L \times L}$, this function maps input data to mixer matrices. Here, $\Theta$ represents the space of learnable parameters, and $\mathcal{M}$ represents the class of mixer matrices.</li> </ol> <p>Given these functions, we denote the mixer matrix as $\textbf{M} = f_{\mathcal{M}}(\textbf{X}, \theta)$. The matrix mixer framework is then defined by the equation: \(\textbf{Y} = \textbf{M} (f_X(\textbf{X})).\)</p> <p>Using this framework, we are now playing a game of finding the optimal $\textbf{M}$ that satisfies all three requirements: efficiency, performance, and flexibility! This systematic approach allows us to analyze the characteristics of different sequence mixers and formalize the properties needed to meet our criteria.</p> <p>Let‚Äôs break down these objectives step-by-step and explore which matrices work best in achieving them.</p> <h2 id="solution-for-sub-quadratic-complexity-structured-matrices">Solution for Sub-quadratic Complexity: Structured Matrices</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-16-hydra/matrix_classes_trans-480.webp 480w,/assets/img/2024-07-16-hydra/matrix_classes_trans-800.webp 800w,/assets/img/2024-07-16-hydra/matrix_classes_trans-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-16-hydra/matrix_classes_trans.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>To meet our first key requirement ‚Äì sub-quadratic matrix multiplication ‚Äì we can focus on a special type of matrices known as <strong>structured matrices</strong>. For a general matrix $\textbf{M}$, matrix multiplication typically incurs a computational cost of $O(L^2)$. However, structured matrices, with their compressed representation, allow us to perform these operations much more efficiently, achieving sub-quadratic complexity. We refer to sequence mixers using these matrices as <strong><em>structured matrix mixers</em></strong>.</p> <p>Structured matrices provide a broad array of options for our matrix mixer $\mathcal{M}$, as illustrated in the figure above. By leveraging these matrices, we can significantly reduce computational overhead while maintaining an efficient parameter count.</p> <p>All previous versions of sub-quadratic sequence mixers fit within the matrix mixer framework. This categorization by the class of mixer matrices helps us systematically analyze and understand the strengths and weaknesses of different approaches.</p> <details><summary>Notations</summary> <p>Think of bold capital letters like $\textbf{X}$ as matrices, bold small letters like $\textbf{x}$ as vectors, and regular small letters like $x$ as scalars. When we talk about elements in a matrix, we‚Äôll use subscripts. So, if we have a matrix $\textbf{X} \in \mathbb{R}^{M \times N}$, the element in the $i$-th row and $j$-th column is $x_{ij}$. If we‚Äôre looking at the whole $i$-th row, it‚Äôs $\textbf{x}_i$.</p> </details> <table> <thead> <tr> <th>Matrix Structure $\mathcal{M}$</th> <th>Formulation (\(ùëö_{ij}\))</th> <th>Complexity</th> <th>Method Instantiations</th> </tr> </thead> <tbody> <tr> <td>Dense</td> <td>$m_{ij}$</td> <td>$O(L^2)$</td> <td>MLP-Mixer<d-cite key="mlpmixer"></d-cite> </td> </tr> <tr> <td>Dense (Softmax Attention)</td> <td>$\text{softmax}_j(q^T_i k_j)$</td> <td>$O(L^2)$</td> <td>Transformer<d-cite key="transformer"></d-cite> </td> </tr> <tr> <td>Low-rank (Linear Attention)</td> <td>$q^T_i k_j$</td> <td>$O(L)$</td> <td>Linear Attention<d-cite key="la"></d-cite>, Linformer<d-cite key="linformer"></d-cite> </td> </tr> <tr> <td>Butterfly</td> <td>Refer to <d-cite key="kaleidoscope"></d-cite><d-cite key="monarch"></d-cite> </td> <td>$O(L \log L)$</td> <td>Kaleidoscope<d-cite key="kaleidoscope"></d-cite>, Monarch<d-cite key="monarch"></d-cite> </td> </tr> <tr> <td>Toeplitz (Convolution)</td> <td>$m_{j-i}$</td> <td>$O(L \log L)$</td> <td>S4<d-cite key="s4"></d-cite>, H3<d-cite key="h3"></d-cite>, TNN<d-cite key="tnn"></d-cite>, CKConv<d-cite key="ckconv"></d-cite> </td> </tr> <tr> <td>Discrete Fourier Transform</td> <td>$w^{ij}$</td> <td>$O(L \log^2 L)$</td> <td>FNet<d-cite key="fnet"></d-cite> </td> </tr> <tr> <td>Semiseparable</td> <td>\(\textbf{c}^T_i \textbf{A}^{\times}_{i:j} \textbf{b}_j \mathbb{1}_{\{i \geq j\}}\)</td> <td>$O(L)$</td> <td>Mamba (S6, SSD) <d-cite key="mamba"></d-cite><d-cite key="ssd"></d-cite> </td> </tr> </tbody> </table> <p>As shown in the table above, using structured matrices (all but the dense variants) as the mixer matrix directly leads to sub-quadratic computational complexity.</p> <h2 id="solution-for-all-desiderata-sequence-aligned-matrices">Solution for All Desiderata: Sequence Aligned Matrices</h2> <p>So, can we simply choose any structured matrix as our sequence mixer matrix and expect it to meet all our requirements for efficiency, performance, and flexibility? Unfortunately, not all structured matrix mixers are up to the task. This begs the question: Is there a class of mixer matrices that can satisfy all three requirements? Fortunately, the answer is yes!</p> <p>We introduce a special subset of structured matrices called <strong><em>Sequence Aligned Matrices (SAM)</em></strong>. SAMs are designed to achieve efficiency, high performance, and flexibility all at once.</p> <h4 id="what-are-sequence-aligned-matrices-sam">What are Sequence Aligned Matrices (SAM)?</h4> <p>In simple terms, SAMs ensure that the parameters for every submatrix $\textbf{M}[: i+1, : i+1]$ are only functions of the tokens up to index $i$. Here is a formal definition of SAM.</p> <details><summary>Formal definition of Sequence Alignment</summary> <p><strong>Definition</strong> <em>(Sequence Aligned Matrices)</em> Let $L$ be the sequence length and let $\textbf{M} \in \mathbb{R}^{L \times L}$ denote a matrix with a parameter set $\mathcal{P}$. Then, we say that $\textbf{M}$ is a Sequence Aligned Matrix if there exists a partition $\Pi$ of $\hat{\mathcal{P}} \subseteq \mathcal{P}$, and $\hat{\mathcal{P}} \neq \phi$, such that for all sets $\mathcal{E} \in \Pi$, there exists a bijective map $f_{\mathcal{E}} : [L] \rightarrow \mathcal{E}$, and, for each $i \in [L]$, the sub-matrix $\textbf{M}[:i+1,:i+1]$ is composed solely from the parameters in the subset $\cup_{\mathcal{E}, k \le i} f_{\mathcal{E}}(k) \subseteq \mathcal{P}$.</p> </details> <h4 id="properties-of-sam">Properties of SAM</h4> <p>SAM matrices come with two crucial properties that make them stand out:</p> <ul> <li> <strong>Data Dependency</strong>: SAM matrices are dynamically generated from the input data. This means they adapt in real-time based on the information they process.</li> <li> <strong>Extendability</strong>: SAM matrices can handle inputs of arbitrary lengths, making them versatile for various applications.</li> </ul> <p>Take, for instance, the Attention mechanism in Transformers. It‚Äôs a perfect example of a SAM matrix: the Query-Key-Value components are all dynamically projected from the input data, and the mechanism itself adapts seamlessly to different sequence lengths.</p> <p>These two properties are not just nice-to-haves; they are essential for the flexibility and performance of modern models. Our experimental results strongly highlight the necessity of SAM, showing that SAM-based mixer matrices significantly enhance the performance of models.</p> <h3 id="sam-variations">SAM Variations</h3> <p>Let‚Äôs dive into a series of new SAM-based models we developed: <em>Toeplitz, Cauchy, Vandermonde, and quasiseparable</em> sequence mixers. By making these mixer matrices SAM, we achieved significant improvements. To make this explanation easier, we‚Äôll assume that Query-Key-Value are projected from an input sequence.</p> <h4 id="cauchy-code">Cauchy <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/cauchy.py" rel="external nofollow noopener" target="_blank">(Code)</a> </h4> <p>We begin with our Cauchy variant, as it shares a significant similarity with the Attention mechanism: the norm of $m_{ij}$ represents the magnitude of correlations between the $i$-th and $j$-th tokens. Following the definition of Cauchy matrices, our SAM Cauchy mixer works as follows:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d} \frac{1}{(q_{id} - k_{jd} + c)} \space, \end{equation}\] <p>where $\textbf{Q}, \textbf{K} \in \mathbb{R}^{L \times D}$, and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected matrices from $\textbf{X}$, and $c$ is a trainable constant that stabilizes training by preventing divide-by-zero errors.</p> <h4 id="vandermonde-code">Vandermonde <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/vandermonde.py" rel="external nofollow noopener" target="_blank">(Code)</a> </h4> <p>Recall the definition of Vandermonde matrices: $m_{rs} = (m_r)^s$. Due to the exponential values, this can lead to instability during training. Therefore, we use the formulation $q_{rs} = \mathfrak{R}(e^{i \cdot r \cdot q_s})$ and $k_{rs} = \mathfrak{R}(e^{i \cdot s \cdot k_r})$ for $\textbf{Q}$ and $\textbf{K}$. This technique, taking the real part of complex numbers, is commonly used in SSMs. Under the same setting as our SAM Cauchy mixer, our SAM Vandermonde mixer $\textbf{M}$ is parameterized as:</p> \[\begin{equation} \textbf{Y} = \textbf{M}\textbf{V}, \qquad \qquad m_{ij} = \sum_{d}(\cos(2 \pi q_{id}^j) - \cos(2 \pi k_{jd}^i)) \space, \end{equation}\] <p>where the cosine function comes from <a href="https://en.wikipedia.org/wiki/Euler's_formula" rel="external nofollow noopener" target="_blank">Euler‚Äôs formula</a>.</p> <h4 id="toeplitz-code">Toeplitz <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/toeplitz.py" rel="external nofollow noopener" target="_blank">(Code)</a> </h4> <p>A Toeplitz matrix mixer is inherently a convolution between weights $\textbf{w} \in \mathbb{R}^{2L-1}$ and an input sequence $\textbf{V} \in \mathbb{R}^{L \times C}$. Usually, a general convolution adopts input-independent $\textbf{w}$, which does not satisfy the definition of SAM. Therefore, we extend our Toeplitz matrix mixer to be SAM as follows:</p> \[\begin{equation} \textbf{Y} = \mathcal{F}^{-1}(\mathcal{F}_\textbf{w} \odot \mathcal{F}_\textbf{V}), \qquad \qquad \textbf{w}_{i} = \begin{cases} q_{i-L+1} &amp; \text{if } i \geq L \\ k_{L-i+1} &amp; \text{if } i \lt L \\ \end{cases} \space , \end{equation}\] <p>where the convolution is implemented using FFT $\mathcal{F}$, and $\textbf{q}, \textbf{k} \in \mathbb{R}^{L}$ and $\textbf{V} \in \mathbb{R}^{L \times C}$ are projected from $\textbf{X}$.</p> <h4 id="quasiseparable-code">Quasiseparable <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixers/quasiseparable.py" rel="external nofollow noopener" target="_blank">(Code)</a> </h4> <blockquote class="block-tip"> <p><strong>This variant has a separate name, Hydra. Stay tuned for <a href="/blog/2024/hydra-part2-model/">Part II</a> ü§≠</strong></p> </blockquote> <h2 id="impact-of-sam-parameterization">Impact of SAM Parameterization</h2> <p>Now, we validate that the SAM matrix mixers are better than non-SAM mixers. To prove this claim, we conducted strictly controlled systematic albations where the only variable was the mixer matrix. Check out <a href="https://github.com/goombalab/hydra/blob/main/hydra/modules/matrix_mixer.py" rel="external nofollow noopener" target="_blank">our efforts</a> for a comprehensive and fair comparison!</p> <table> <tr> <td style="font-weight:bold;">Structure</td> <td style="text-align:center;">Data Dependent</td> <td style="text-align:center;"># Params</td> <td style="text-align:center;">GLUE Avg</td> <td style="text-align:center;">Œî</td> </tr> <tr> <td style="font-weight:bold;">Dense</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.7</td> <td style="text-align:center;"></td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Toeplitz</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">75.8</td> <td rowspan="2" style="text-align:center;">+1.9</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">77.7</td> </tr> <tr> <td style="font-weight:bold;">DFT</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.7</td> <td rowspan="3" style="text-align:center;">+5.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Vandermonde</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">70.8</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">76.0</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Cauchy</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.2</td> <td rowspan="2" style="text-align:center;">+4.0</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.2</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Low-rank</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">74.9</td> <td rowspan="2" style="text-align:center;">+3.5</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.4</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Attention</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">71.9</td> <td rowspan="2" style="text-align:center;">+6.9</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">70M</td> <td style="text-align:center;">78.8</td> </tr> <tr> <td rowspan="2" style="font-weight:bold;">Quasiseparable</td> <td style="text-align:center;">‚ùå</td> <td style="text-align:center;">72M</td> <td style="text-align:center;">75.1</td> <td rowspan="2" style="text-align:center;">+4.6</td> </tr> <tr> <td style="text-align:center;">‚úÖ</td> <td style="text-align:center;">71M</td> <td style="text-align:center;">79.7</td> </tr> </table> <p>The results in the table above clearly demonstrate the importance of SAM. Regardless of the matrix class, incorporating the SAM property always leads to a significant performance boost. Additionally, our SAM-based Toeplitz, Cauchy, and low-rank mixers perform remarkably well, with quasiseparable mixers even surpassing Attention. These findings underscore the immense potential of structured matrix mixers as efficient yet powerful sequence mixers.</p> <h2 id="next-up">Next Up</h2> <p>Curious about the quasiseparable matrix mixer? In <a href="">the next part</a>, we‚Äôll introduce Hydra, our bidirectional extension of SSMs that not only surpasses Attention but also achieves sub-quadratic complexity. Stay tuned!</p> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/june.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> ¬© Copyright 2025 Goomba AI Lab. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cloud.umami.is/script.js" data-website-id="340bca3c-b84e-462f-98dd-f4f4629b9751"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>