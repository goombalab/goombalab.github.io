<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> H-Nets - the Future | Goomba Lab </title> <meta name="author" content="Goomba AI Lab"> <meta name="description" content="Homepage of the Goomba AI Lab @ CMU MLD. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/goomba_light.png?68e59a389531e710f3507b5f12827027"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goombalab.github.io/blog/2025/hnet-future/"> <script src="/assets/js/theme.js?daf0da4e15ae2df6b4045ab97d680f8d"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "H-Nets - the Future",
            "description": "",
            "published": "July 11, 2025",
            "authors": [
              
              {
                "author": "Albert Gu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia AI",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goomba Lab <img src="/assets/img/goomba_transparent.png" width="30" height="30"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>H-Nets - the Future</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#direct-applications">Direct Applications</a> </div> <ul> <li> <a href="#alternative-languages-and-modalities">Alternative languages and modalities</a> </li> <li> <a href="#multimodal-alignment">Multimodal alignment</a> </li> <li> <a href="#language-and-reasoning">Language (and reasoning)</a> </li> </ul> <div> <a href="#efficiency-and-engineering-and-a-connection-to-speculative-decoding">Efficiency and Engineering (and a Connection to Speculative Decoding)</a> </div> <ul> <li> <a href="#the-efficiency-quality-pareto-frontier">The efficiency ↔ quality Pareto frontier</a> </li> <li> <a href="#speculative-decoding">Speculative decoding</a> </li> <li> <a href="#engineering-challenges">Engineering challenges</a> </li> </ul> <div> <a href="#revival-of-architecture-research">Revival of Architecture Research</a> </div> <ul> <li> <a href="#hierarchical-sequence-models">Hierarchical sequence models</a> </li> <li> <a href="#long-context">Long context</a> </li> <li> <a href="#hybrid-models">Hybrid models</a> </li> <li> <a href="#it-s-the-wild-west">It's the wild west!</a> </li> </ul> <div> <a href="#closing">Closing</a> </div> </nav> </d-contents> <div> <p>This post is part of a two-part series.</p> <ol> <li><a href="/blog/2025/hnet-past/">H-Nets: the Past</a></li> <li><strong>H-Nets: the Future</strong></li> </ol> <p>[<a href="https://arxiv.org/abs/2507.07955" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/goombalab/hnet" rel="external nofollow noopener" target="_blank">Code</a>]</p> <p>In this post, I’m going to try to convince you why H-Nets are fundamental and important. There was only so much content that could make it to the paper, and I think there are a lot of downstream consequences and interesting technical connections that we didn’t cover. Much of this will be based on deeper (but mostly unvalidated) intuitions I have and speculative implications about H-Nets. For fun, I’ll formulate several concrete hypotheses and predictions about the future of this research direction – these are personal takes that aren’t meant to be calibrated or taken super seriously, but does crystallize what I think might be some important directions for the field.</p> <h2 id="direct-applications">Direct Applications</h2> <p>Let me first say that by “H-Net”, I don’t necessarily mean our <em>exact</em> model, but the concept we introduced and our general definition of H-Nets: end-to-end hierarchical networks with data-dependent, dynamic segmentation strategies.</p> <h3 id="alternative-languages-and-modalities">Alternative languages and modalities</h3> <p>The biggest use case of H-Net is, of course, languages and modalities that don’t have obvious syntactic boundaries for meaningful chunking of the data. (In contrast, the white spaces of English are heavily used in modern LM pipelines, namely through being <a href="https://huggingface.co/learn/llm-course/en/chapter6/4" rel="external nofollow noopener" target="_blank">hacked into the tokenization pipeline</a>.) This means they have much weaker tokenizers that should strongly benefit from using H-Net, which ideally would discover better chunking boundaries.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/bpb_chinese_code-480.webp 480w,/assets/img/2025-07-11-hnet/bpb_chinese_code-800.webp 800w,/assets/img/2025-07-11-hnet/bpb_chinese_code-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-11-hnet/bpb_chinese_code.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Indeed, the most striking results in our paper are for non-English languages. We showed that on both <strong>Chinese and code</strong>, H-Net scales much, <em>much</em> better than the standard tokenized language model. I imagine these results should hold for many other distributions, including <strong>math</strong> (as shown in the <a href="https://arxiv.org/abs/2404.14408" rel="external nofollow noopener" target="_blank">SpaceByte</a> paper <d-cite key="slagle2024spacebyte"></d-cite>) as well as <strong>most human languages</strong>. The linguists have done a much better job than us of explaining <a href="https://arxiv.org/abs/2103.06874" rel="external nofollow noopener" target="_blank">how broken tokenization is for certain languages</a> <d-cite key="clark2022canine"></d-cite>.</p> <p>Other types of sequential data such as <strong>scientific modalities</strong> are hungry for new architectures because the standard Transformer isn’t enough (as touched on in my <a href="/blog/2025/tradeoffs/#so-what-happens-without-tokenization">previous blog post</a>).</p> <p>Out of all the experiments we did in this paper, the DNA results seemed perhaps the most striking in terms of the improvement over standard architectures.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/bpb_dna-480.webp 480w,/assets/img/2025-07-11-hnet/bpb_dna-800.webp 800w,/assets/img/2025-07-11-hnet/bpb_dna-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-11-hnet/bpb_dna.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>I think this most directly validates the power of hierarchical modeling, as the baselines here are also operating on single base pair (non-tokenized) resolution. On data that isn’t pre-compressed by tokenizers, applying an end-to-end model that tries to learn hierarchical patterns from data seems like a free win!<d-footnote>I actually suspect that DNA modeling could benefit empirically from a BPE tokenizer just like other languages. I'm not quite sure why I haven't seen anyone use this; it will certainly still lead to a valid DNA language model. I guess it just seems too weird to the biologists? Well, another point against tokenizers.<br><br>Edit: It has been pointed out to me by Nate Tippens and Krithik Ramesh that <a href="https://arxiv.org/abs/2306.15006" rel="external nofollow noopener" target="_blank">DNABERT-2</a> does do this!</d-footnote></p> <p>I’ll also mention that we didn’t spend too much time on DNA and never tried iterating the H-Net to 2-stages. It might just get better for free, just like for language?</p> <blockquote class="block-tip"> <h4 id="hypothesis">Hypothesis</h4> <p>H-Nets will immediately be adopted for sequence modalities with unusual characteristics, like <strong>tail languages and genomics</strong>.</p> </blockquote> <p>A final type of application that H-Net could unlock is allowing “tokenization” of <strong>continuous-valued sequences</strong>. The concept of tokenization as applied to language models is predicated on discrete vocabularies – the process involves explicitly constructing new vocab words from existing ones. So it’s not applicable directly to non-discrete data like audio and video, even though those modalities seem like they should benefit from a form of dynamic compression (in time) as well! On the other hand, H-Nets are applicable to continuous-valued input sequences and can in principle chunk them in time, ideally grouping meaningful units like the role of tokenization in language. This can potentially be used to perform alignment problems (e.g. audio $\to$ phoneme) or just compress away redundancies (e.g. silence spans of audio or motionless video).</p> <blockquote class="block-tip"> <h4 id="hypothesis-1">Hypothesis</h4> <p>H-Nets will be useful for <strong>audio and video</strong>, but may require more research.</p> </blockquote> <h3 id="multimodal-alignment">Multimodal alignment</h3> <p>Speaking of different modalities, so far I’ve only talked about training models on a single modality at a time. The power of dynamic chunking will be even more important when moving to multimodal models that fuse different streams of data together.</p> <p>In particular, multimodal streams with temporal mismatch (e.g. text and audio) are difficult to synchronize. For example, language is currently tokenized into subwords, while audio is tokenized as raw waveforms or downsampled codecs, making them difficult to model jointly. Learned chunking mechanisms provide a path to fuse multimodal streams “ticking” at different rates, unlocking native <em>multi-channel</em> multimodal models operating at a higher abstraction level and enabling better transfer, reasoning, and understanding across modalities.</p> <blockquote class="block-tip"> <h4 id="hypothesis-2">Hypothesis</h4> <p>H-Nets will unlock <strong>multi-channel multimodal models</strong> with temporal fusion.</p> </blockquote> <h3 id="language-and-reasoning">Language (and reasoning)</h3> <p>Of course, the big question is how good this model <em>really</em> is for core language modeling, which is still at the heart of the most mainstream AI progress today. I’m personally very bullish on H-Nets – I wouldn’t have worked in this direction if not – but as always with architectural research, there are significant unknowns, risks, and barriers to adoption.</p> <p>The main reasons I think they’ll improve language modeling has been laid out in the paper, but in a nutshell, the goal of H-Nets is to <em>compress data into semantic units</em> and <strong>operate over higher levels of abstraction</strong>. It’s currently not known to what extent they can successfully do this right now (but I think the scaling behavior we showed is evidence for it), but if possible, it should just allow for stronger models that are more capable of reasoning intelligently.</p> <blockquote class="block-tip"> <h4 id="hypothesis-3">Hypothesis</h4> <p>H-Nets will have increased <strong>language modeling</strong> and <strong>reasoning</strong> abilities.</p> </blockquote> <p>We didn’t formally run out true scaling laws in the paper, which would require sweeping over many more model sizes and compute horizons. But as I mentioned in the previous part, I have some reasons to believe that they will have better scaling (i.e. shift the scaling law curve by a non-trivial amount). I started writing this up but got tired 😆 but maybe I’ll follow this up with another blog post.</p> <p>Given the importance of language and it being our main motivation for this model, the rest of this post will focus exclusively on language modeling intuitions.</p> <h2 id="efficiency-and-engineering-and-a-connection-to-speculative-decoding">Efficiency and Engineering (and a Connection to Speculative Decoding)</h2> <p>One of the first things that gets asked about any new architecture is how efficient it is. Much of my prior architecture research has specifically focused on being <a href="https://arxiv.org/abs/2111.00396" rel="external nofollow noopener" target="_blank">more</a> <a href="https://arxiv.org/abs/2312.00752" rel="external nofollow noopener" target="_blank">efficient</a> than the status quo. In the H-Net paper, we basically didn’t touch on efficiency, so what can we say about this?</p> <h3 id="the-efficiency---quality-pareto-frontier">The efficiency ↔ quality Pareto frontier</h3> <p>Well, the simple reason why we didn’t is because the intuition behind our method (chunking) is more obviously connected to model <em>quality</em> rather than <em>efficiency</em>, so we focused on those aspects. But of course, what really matters is the <em>interaction</em> between efficiency and quality. This is generally monotone, leading to an entire Pareto frontier of performance tradeoffs. At a superficial level, I expect that the quality gains of H-Nets would directly translate to efficiency gains as well.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/efficiency_quality-480.webp 480w,/assets/img/2025-07-11-hnet/efficiency_quality-800.webp 800w,/assets/img/2025-07-11-hnet/efficiency_quality-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-11-hnet/efficiency_quality.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">I realized this figure probably contains zero information content, <br> but since I already drew it for some reason I'm not gonna waste it.</figcaption> </figure> <p>But there can often be more nuance to this with architecture research because of qualitative differences between models.<d-footnote>For instance, the efficiency ↔ quality tradeoff of Mamba vs. Transformers isn't actually so clear-cut, as discussed in my previous blog post on the tradeoffs of SSMs and Transformers.</d-footnote> What people want to know is whether there are <strong>qualitative structural characteristics</strong> in the architecture that directly relate to its efficiency. (This is what made SSMs so appealing, I suppose, as these characteristics – the constant state size – are very intuitive.)</p> <p>This topic is a lot more subtle for H-Nets, but there are a few intriguing connections to highlight.</p> <h3 id="speculative-decoding">Speculative decoding</h3> <p>Let’s think about <strong>speculative decoding (specdec)</strong>, which is by now a universally used technique for LLM inference <d-cite key="leviathan2023fast"></d-cite><d-cite key="chen2023accelerating"></d-cite><d-cite key="xia2023speculative"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/speculative-480.webp 480w,/assets/img/2025-07-11-hnet/speculative-800.webp 800w,/assets/img/2025-07-11-hnet/speculative-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-11-hnet/speculative.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">The speculative decoding process of stepping a small model on every token and a large model on every few tokens strongly resembles the H-Net decoding process. [<a href="https://arxiv.org/abs/2203.16487" rel="external nofollow noopener" target="_blank">Source</a>]</figcaption> </figure> <h4 id="speculative-decoding-resembles-an-h-net">Speculative decoding resembles an H-Net</h4> <p>Without getting too in the weeds, speculative decoding consists of a <em>large model</em> (usually called the “verification model”) that we want to sample from, and a <em>small model</em> (usually called the “draft model”) that’ll help us sample from it faster. The decoding process basically looks like this:</p> <ol> <li>On every autoregressive step, the <strong>draft model</strong> will take a step to generate a new token.</li> <li>Every few steps, the <strong>verification model</strong> will verify the small model’s sequence of tokens, accepting as many of them as it can.</li> </ol> <p>At a high level, specdec improves generation speed by letting the large model only do a forward pass every few tokens. But this is incredibly similar to the decoding process of an H-Net!</p> <ol> <li>The H-Net <strong>encoder/decoder networks</strong> take a step on every token.</li> <li>The H-Net <strong>main network</strong> takes a step every few tokens (on every <em>chunk</em>).</li> </ol> <h4 id="speculative-decoding--h-net-is-redundant">Speculative decoding + H-Net is redundant</h4> <p>One can take this a step further and ask: what happens if we combine speculative decoding to try to speed up an H-Net? In an <em>idealized</em> case of speculative decoding, what might happen is:</p> <ol> <li>The small model (an auxiliary draft model) takes a few steps, say $k$, and proposes a number of tokens.</li> <li>The large model (H-Net) does a forward pass on these $k$ tokens. Suppose that $k$ lines up with the next chunk: then this amounts to <ul> <li>$1$ parallel pass (over $k$ tokens) of the H-Net’s encoder/decoder networks, and</li> <li>just $1$ standard decoding step of the H-Net’s main network.</li> </ul> </li> </ol> <p>Further suppose that the draft model has a similar network structure and size as the H-Net’s encoder/decoders. Then this is almost the same as the vanilla decoding process of an H-Net above, just with the sequential steps of the encoder/decoder swapped for sequential steps of an auxiliary draft model!</p> <p>So this application of speculative decoding was pretty much a no-op in terms of improving efficiency.</p> <p>Why is this? One way to interpret this is that the speculative decoding process is already <em>baked into</em> the H-Net structure, and once we move to H-Nets, we might not need inference techniques like specdec anymore.</p> <h4 id="entropy-strikes-again">Entropy strikes again</h4> <p>And there’s one final conceptual connection! Speculative decoding works because there’s some form of redundancy in token sequences that makes some of them easier to predict than others; the speedup happens exactly when there are small sequences of easy-to-predict tokens. Or put another way, when there are local strings of low-entropy or low-information tokens. But this is exactly one of the heuristics for how to think about dynamic hierarchical networks (as discussed in the <a href="/blog/2025/hnet-past/#information-based-chunking">previous post</a>) – they segment on surprising tokens, or more generally on some notion of information content in the data.</p> <p>All in all, there are a lot of striking similarities between speculative decoding and H-Nets! My hypothesis is that: <strong>the H-Net structure implicitly subsumes the speculative decoding process</strong>.</p> <p>What are the implications of this? Well, the obvious practical one is that despite seeming more complicated than standard models, H-Nets might not actually be much more complicated than modern LLM <em>pipelines</em> used in practice.</p> <blockquote class="block-tip"> <h4 id="hypothesis-4">Hypothesis</h4> <p>H-Net inference engineering will have a <strong>similar complexity</strong> to current LLM inference pipelines.<br> <br> Furthermore, inference tricks will become marginally less and less useful (and ideally not be necessary at all), as they become <strong>subsumed by end-to-end models</strong> that incorporate the underlying concepts in a more natural way.</p> </blockquote> <p>But to me, there’s a more important conceptual implication.</p> <ul> <li>Much of architecture efficiency optimization consists of asking: <strong>Given this model, how can I make it faster?</strong> </li> <li>One can flip this on its head and ask: <strong>Given that this model <em>could</em> be sped up, what does that imply about the original model?</strong> The way to think about this is that <em>the very fact that standard LLMs can be sped up</em> through tricks like speculative decoding means that <em>the original models have redundancies</em> and could be made more powerful to begin with.</li> </ul> <p>The H-Net structure is exactly the way to smooth out those redundancies, baking (something akin to) the speculative decoding process directly into the model, while <strong>leveraging parameters and compute</strong> more effectively and <strong>training everything end-to-end</strong>. In other words, the structure of the H-Net preserves the same characteristics of the <em>inference-optimized</em> standard LM, but with a better <em>training objective</em>.<d-footnote>Just to unpack a bit more: intuitively, the reason this should lead to a stronger model is because the main network (analogous to the target verification model in specdec) is trained directly on *chunk-level* modeling, the way they would be used at inference. On the other hand, the verification model of the specdec pipeline is trained on a more granular (*token-level*) objective and being used in a different way at inference.</d-footnote></p> <p>Thus, what I predict is that with optimized inference implementations for H-Nets, then for any target inference budget, an H-Net would be a stronger model than our current standard LLMs.</p> <h3 id="engineering-challenges">Engineering challenges</h3> <p>Okay, a lot of what I’ve talked about so far (and the way it’s implicitly described in the paper) is about <em>theoretical</em> efficiency; we considered FLOPs, not wall-clock time. A very important question is: is the theoretical efficiency realizable in practice?</p> <h4 id="training">Training</h4> <p>Training is more difficult than normal because sequences are dynamically subsampled, which causes load balance issues among other edge cases. Sukjun spent a while engineering our pipeline to be reasonably efficient by incorporating dynamic packing and such. Our current implementation is still a bit slower than isotropic models during training, but I expect to have substantial room for improvement. There has been a lot of work on mixture-of-experts (MoE) in the last few years, and I expect a lot of general ideas will transfer to H-Nets.</p> <h4 id="inference">Inference</h4> <p>Inference has largely been discussed in relation to speculative decoding; I think it’s going to take some work, but don’t see any fundamental barriers.</p> <p>Overall, engineering for H-Nets will be a substantial but surmountable problem for their adoption at scale.</p> <blockquote class="block-tip"> <h4 id="hypothesis-5">Hypothesis</h4> <p>H-Nets will require non-trivial <strong>research and infrastructure</strong> work for both <strong>training and inference</strong>; on the order of what was needed for tokenizers, mixture-of-experts, and speculative decoding pipelines.<br> <br> This effort will be worth the tradeoff to achieve higher quality and less brittle end-to-end models.</p> </blockquote> <h2 id="revival-of-architecture-research">Revival of Architecture Research</h2> <p>As multi-component pipelines are consolidated into end-to-end models, previous parts of the pipeline that required dedicated treatment will transform into added complexity in the model instead. Architectures will become somewhat more sophisticated and require new considerations and research. Here are a couple of such considerations.</p> <h3 id="hierarchical-sequence-models">Hierarchical sequence models</h3> <p>As I described in <a href="/blog/2025/hnet-past/#hierarchical-rnns">my own journey through sequence models</a>, hierarchy is far from new.</p> <p>There have been a few recent works that investigate hierarchical structures inside novel sequence model layers (i.e. variants of attention or SSMs). For example:</p> <ul> <li> <a href="https://arxiv.org/abs/2502.11089" rel="external nofollow noopener" target="_blank">Native Sparse Attention (NSA)</a> <d-cite key="yuan2025native"></d-cite> is a recent sparse attention model that performs a 2-stage process of aggregating information inside local blocks and then retrieving them.</li> <li> <a href="https://arxiv.org/abs/2506.04761" rel="external nofollow noopener" target="_blank">Log-linear attention</a> <d-cite key="guo2025log"></d-cite> and <a href="https://arxiv.org/abs/2506.10918" rel="external nofollow noopener" target="_blank">prefix scannable models (PSM)</a> <d-cite key="yau2025sequential"></d-cite> introduce new hierarchical layers that generalize modern recurrent models using a binary tree of hierarchies, improving their expressivity by increasing the constant size state to logarithmic (in sequence length).</li> </ul> <p>While these models are elegant exercises in algorithm design and engineering, and definitely valuable contributions to the community, I personally think there might be problems long-term with building hierarchy directly into the layer. The root cause is the difficulty of having a dynamic or flexible hierarchy, which also ties to hardware considerations.</p> <p>In NSA, for example, there is a <em>block size</em> hyperparameter (set to $64$ by default, I think) that governs the lower level of the hierarchy, motivated by hardware alignment. This doesn’t feel “right” to me for some reason.<d-footnote>Another appeal to aesthetics of the future rather than the current state of the world; I've been told NSA works pretty well in practice right now!</d-footnote> I guess it’s because I feel that while hardware considerations are important, they should be connected to the <em>model algorithm</em> rather than the <em>model definition</em>. For example, while <a href="https://arxiv.org/abs/2405.21060" rel="external nofollow noopener" target="_blank">Mamba-2</a> <d-cite key="dao2024transformers"></d-cite> also has a block size hyperparameter (also set to $64$ by default) related to the size of matrix multiplication tiles, this only affects its implementation/efficiency and not the <em>definition</em> of the model. In contrast, the block size hyperparameter of NSA fundamentally changes what functions (sequence transformations) it can represent.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/psm-480.webp 480w,/assets/img/2025-07-11-hnet/psm-800.webp 800w,/assets/img/2025-07-11-hnet/psm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-11-hnet/psm.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Concurrent works propose other types of hierarchical sequence models with logarithmically-scaling state sizes.</figcaption> </figure> <p>As another example, the log-linear models are tied to a static binary tree hierarchy. But a major theme of the H-Net paper is that static hierarchies are not the right structure!</p> <blockquote class="block-tip"> <h4 id="hypothesis-6">Hypothesis</h4> <p>The best way of building hierarchical models will be in the holistic architecture’s <strong>global network structure</strong> like H-Net, not in individual layers.</p> </blockquote> <p>A future direction of H-Nets is to see how far the hierarchy can extend. For example, one can build a binary-tree-like architecture with repeated downsampling by a factor of roughly 2 (a controllable parameter in the H-Net), which leads to the same holistic properties as log-linear layers – linear-scaling computation in sequence length with logarithmic-scaling state size – but with a <em>dynamic hierarchy</em>. By carefully balancing the depth/widths of sub-networks as well, one can actually get very fine control over the scaling of both compute and state size, potentially targeting <strong>polynomial-scaling state sizes</strong> which is <a href="http://arxiv.org/abs/2503.04725v1" rel="external nofollow noopener" target="_blank">hypothesized to be optimal for language modeling</a> <d-cite key="chen2025l"></d-cite>. I’m quite interested in exploring this, let me know if you want to collaborate!</p> <h3 id="long-context">Long context</h3> <p>One question I have is whether deep hierarchies can actually allow one to completely get rid of global attention. What happens if one uses a deep recursive H-Net with pure constant-state-size layers like SSMs or sliding window attention (SWA)? In a normal isotropic model, these suffer on long sequences because they simply can’t encode enough information. But in a deep hierarchical model, information is constantly being compressed and abstracted, shrinking the sequence length and perhaps substantially improving the effectiveness of these layers.</p> <p>Maybe some lightweight global attention will still be needed for certain retrieval abilities? But I think hierarchical structure can only help long context significantly (which is indeed an explicit motivation for many prior works on hierarchical sequence models!).</p> <blockquote class="block-tip"> <h4 id="hypothesis-7">Hypothesis</h4> <p>H-Nets will substantially <strong>improve long context abilities</strong>.</p> </blockquote> <p>We originally wanted to explore some long context benchmarks in the H-Net paper, but there were too many facets to show already so we didn’t get around to it. Hopefully someone will investigate this in the future!</p> <h3 id="hybrid-models">Hybrid models</h3> <p>While hybrid models combining linear layers with quadratic attention have become <a href="/blog/2025/tradeoffs/#a-coarse-analogy">much more popular</a>, I always wondered if the simple interleaving strategies were the most natural way.</p> <p>One nice thing about H-Nets is that they can hybridize linear and quadratic layers in a more elegant way, in my opinion. (In my head, another potential meaning of H-Net stands for <strong>hybrid network</strong>!) Linear layers go on the outside, both for efficiency <em>and</em> modeling reasons (as covered in <a href="/blog/2025/tradeoffs/#so-what-happens-without-tokenization">my Tradeoffs post</a>), and powerful quadratic attention layers can go on the inside, operating over higher levels of abstraction where they are most suited.</p> <p>However, figuring out the exact right combination of layers is pretty non-trivial. We did endless ablations over the course of this project (and included many of them in the paper, but that was only a small subset), and it was pretty hard to come to conclusive answers.</p> <p>For example, these were the conclusions found for a 2-stage H-Net (three sequence lengths):</p> <ul> <li> <strong>Outer</strong>: Pure Mamba layers perform best, and seem indispensable.</li> <li> <strong>Middle</strong>: After the outer layers have shrunk the sequences by a reasonable length (almost $3\times$), this is much closer to tokenized language, and I wouldn’t have been surprised if pure Transformer layers were fine here. But we found that Mamba was still important, which validates that its effect is not <em>just</em> because it’s good at high resolution, but because it’s doing a form of <a href="/blog/2025/hnet-past/#ssms-as-compressive-models">active compression that benefits dynamic chunking</a>.</li> <li> <strong>Inner</strong>: The innermost model has the most parameters and is essentially a standard isotropic language model operating on coarsely tokenized data (but with better “tokens” that are dynamic and learned from data!). In the paper, we stuck to pure Transformers because that was our main baseline. However, this is completely orthogonal to the rest of the H-Net design; we did experiment a bit and did an ablation showing that general findings for LM architectures still transfer, such as that <strong>hybrid main networks</strong> (we tried 3-to-1 Mamba-to-Transformer) <strong>still have noticeably better perplexity</strong> <d-cite key="waleffe2024empirical"></d-cite>!</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-11-hnet/hybrid-480.webp 480w,/assets/img/2025-07-11-hnet/hybrid-800.webp 800w,/assets/img/2025-07-11-hnet/hybrid-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-11-hnet/hybrid.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Ablating the main network architecture of a 2-stage H-Net.</figcaption> </figure> <p>More explicitly, I think the following is true (we didn’t show ablations but ran some early tests).</p> <blockquote> <p>H-Nets would <strong>work reasonably well without attention</strong> (only SSM layers), but <strong>not at all without SSMs</strong> (only Transformer layers).</p> </blockquote> <p>At the very least, moving toward such hierarchical models will necessitate expanding the space of primitives used; I’m pretty sure <a href="/blog/2025/tradeoffs/">standard attention is not sufficient</a>.</p> <blockquote class="block-tip"> <h4 id="hypothesis-8">Hypothesis</h4> <p>Linear sequence models such as <strong>state space models will become core primitives</strong> of language models, if only for acting as the byte-level interface.</p> </blockquote> <p>In turn, research papers on such models should start <strong>incorporating byte-level language modeling</strong> as a standard evaluation.</p> <h3 id="its-the-wild-west">It’s the wild west!</h3> <p>I have to emphasize again that creating the H-Net was a <a href="/blog/2025/hnet-past/#a-world-of-improvements">fiendishly difficult design problem</a>, and we still don’t completely understand how a lot of things work. I wouldn’t be too surprised if someone came out next week with a simplification of our routing mechanism that was better (well, I’d be pretty surprised actually – but I do expect it to happen at some point). At any rate, there are so many new axes of variation, knobs to turn, and completely new directions to explore. Things are just getting started!</p> <h2 id="closing">Closing</h2> <p>Let me return once again to the higher-level question: is all of this actually useful? Are hierarchical models the future?</p> <p>In this post, I haven’t said anything that was actually technical or rigorous, only a loose set of connections and intuitions. But somehow to me these point to some type of deeper truth. Something about certain models just “feels right” to me, and H-Nets <em>feel right</em>.</p> <p>Perhaps the most concrete answer I can give, though, can be summarized by just two points:</p> <ol> <li>Hierarchical pipelines are <em>already</em> used everywhere, often implicitly.</li> <li>Consolidating them into general, trainable methods is at the heart of AI (<a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="external nofollow noopener" target="_blank">The Bitter Lesson</a>).</li> </ol> <blockquote> <h4 id="observation">Observation</h4> <p>Existing LLM <strong>pipelines</strong> are already <strong>implicitly hierarchical</strong>, such as<br> (1) the core language model pipeline (tokenizer – language model)<br> (2) the speculative decoding pipeline (draft model – verification model)</p> </blockquote> <blockquote class="block-tip"> <h4 id="hypothesis-9">Hypothesis</h4> <p>As we get closer to finding “the right architecture”, these explicitly engineered pipelines will be subsumed by an end-to-end model. <strong><em>Maybe the H-Net?</em></strong></p> </blockquote> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/albert.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Goomba AI Lab. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cloud.umami.is/script.js" data-website-id="340bca3c-b84e-462f-98dd-f4f4629b9751"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>