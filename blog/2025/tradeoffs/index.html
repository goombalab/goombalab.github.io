<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> On the Tradeoffs of SSMs and Transformers | Goomba Lab </title> <meta name="author" content="Goomba AI Lab"> <meta name="description" content="(or - tokens are bs)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/goomba_light.png?68e59a389531e710f3507b5f12827027"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goombalab.github.io/blog/2025/tradeoffs/"> <script src="/assets/js/theme.js?daf0da4e15ae2df6b4045ab97d680f8d"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "On the Tradeoffs of SSMs and Transformers",
            "description": "(or - tokens are bs)",
            "published": "July 08, 2025",
            "authors": [
              
              {
                "author": "Albert Gu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "CMU, Cartesia AI",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Goomba Lab <img src="/assets/img/goomba_transparent.png" width="30" height="30"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">people </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>On the Tradeoffs of SSMs and Transformers</h1> <p>(or - tokens are bs)</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#state-space-models">State Space Models</a> </div> <ul> <li> <a href="#the-three-ingredients">The three ingredients</a> </li> <li> <a href="#mamba-putting-it-all-together">Mamba - putting it all together</a> </li> <li> <a href="#modern-recurrent-models">Modern recurrent models</a> </li> </ul> <div> <a href="#states-brains-and-databases">States, Brains, and Databases</a> </div> <ul> <li> <a href="#autoregressive-states-of-sequence-models">Autoregressive states of sequence models</a> </li> <li> <a href="#a-coarse-analogy">A coarse analogy</a> </li> </ul> <div> <a href="#is-attention-all-you-need">Is Attention All You Need?</a> </div> <ul> <li> <a href="#should-we-get-rid-of-tokenization">Should we get rid of tokenization?</a> </li> <li> <a href="#so-what-happens-without-tokenization">So what happens without tokenization?</a> </li> <li> <a href="#a-heuristic-explanation">A heuristic explanation</a> </li> <li> <a href="#a-hypothetical-litmus-test">A hypothetical litmus test</a> </li> <li> <a href="#is-attention-all-you-need-redux">Is attention all you need? (redux)</a> </li> </ul> <div> <a href="#the-tradeoffs-of-state-space-models-and-transformers">The Tradeoffs of State Space Models and Transformers</a> </div> <ul> <li> <a href="#state-space-models">State space models</a> </li> <li> <a href="#transformers">Transformers</a> </li> </ul> <div> <a href="#scaling-laws">Scaling Laws</a> </div> </nav> </d-contents> <div> <p>This blog post was adapted from a talk I’ve given a handful of times over the last year. It was meant to be a high-level talk accessible to a fairly broad audience, but hopefully has some interesting insights, opinions, and intuitions around sequence models for the dedicated researchers too.</p> <h2 id="state-space-models">State Space Models</h2> <p>Just so we’re on the same page, I’ll start by defining what I mean by a state space model. (This section isn’t strictly necessary to get to the main part of this post though; feel free to skip directly to <a href="#states-brains-and-databases">the next section</a>.)</p> \[\begin{equation} \label{eq:ssm} \begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} h_t \end{aligned} \end{equation}\] <p>These equations define the (structured) state space model (SSM) as developed in a line of work <d-cite key="gu2023thesis"></d-cite> culminating in Mamba <d-cite key="gu2023mamba"></d-cite>. They can be viewed as a modern version of a recurrent neural network (RNN) with a few key characteristics. While a lot of technical work was involved in getting this family of models to work, I’ll start by trying to abstract away what I view as the main high-level ingredients that made these models successful, e.g. match the performance of Transformers on language modeling.</p> <h3 id="the-three-ingredients">The three ingredients</h3> <h4 id="1-state-size">1. State size</h4> <p>A characteristic of the SSM is that its hidden state $h_t$ has a larger size than the the inputs and outputs $x_t, y_t$. The key idea is that the hidden state of any recurrent model is its only access to the model’s context (in an autoregressive setting). So for modeling information-dense modalities such as language, the model needs a large enough state to store the relevant information that it wants to access later.</p> <p>In SSMs, if each input $x_t$ is a 1-dimensional scalar, then the hidden state $h_t$ is an $\mathtt{N}$-dimensional vector, where $\mathtt{N}$ is an independent hyperparameter called the <em>state size, state dimension, or state expansion factor</em>. This is also known as a SISO (single-input single-output) SSM and allows the models to store $\mathtt{N}$ times as much information as older RNNs such as LSTMs and GRUs <d-cite key="lstm"></d-cite><d-cite key="chung2014empirical"></d-cite>.</p> <h4 id="2-state-expressivity">2. State expressivity</h4> <p>Not only does the model need to have a large enough state to <em>theoretically</em> store relevant context, it needs to have an expressive enough state update function to encode and access exactly the information it needs.</p> <p>Earlier versions of “linear time-invariant” SSMs used simple recurrences $h_{t} = A h_{t-1} + B x_t$ whose updates are constant at every time step <d-cite key="gu2023thesis"></d-cite>. While this works great for compressible data like audio, it doesn’t provide enough flexibility for sequences with variable information rates like language, where the model may have to selectively choose what information to remember. <strong>Selective SSMs</strong> like Mamba fix this by making the recurrence more expressive by letting the transition matrices vary through time and depend on the data itself. These mechanisms are closely related to the gating mechanisms of classical RNNs!</p> <p>This is the area with the most active research on modern recurrent models, which are focused on understanding the theoretical expressivity of different parameterizations of the transition matrix $A_t$ and what they allow the model to remember in its state.</p> <h4 id="3-training-efficiency">3. Training efficiency</h4> <p>Having a larger and more expressive recurrent state is important, but comes with a critical trade-off – the model becomes much harder to compute. Mamba addressed this with careful parameterization of the recurrence and utilizing the classic parallel scan algorithm<d-cite key="blelloch1990prefix"></d-cite><d-cite key="martin2018parallelizing"></d-cite>.</p> <p>Many other algorithmic innovations have appeared, all with a few shared characteristics:</p> <ul> <li> <strong>Parallelization</strong>: They aim to be parallelizable and practically efficient on accelerators like GPUs and TPUs – preferably leveraging matrix multiplications (matmuls) as the workhorse.</li> <li> <strong>Memory management</strong>: They have to control memory usage carefully. In particular, any model that uses state expansion can’t actually materialize the state in main memory! While Mamba brute-forced the problem using clever awareness of the GPU memory hierarchy <d-cite key="gu2023mamba"></d-cite>, most alternatives find ways of rewriting the equations entirely to use different computation paths that don’t need to compute the state explicitly during a parallel training pass.</li> <li> <strong>Linearity</strong>: The model generally has to be linear in $x_t$, leading some to call this whole family of models <em>linear recurrent models</em>. Linearity plays a role in both computational efficiency as well as modeling/optimization ability, which I won’t get into here.</li> </ul> <h3 id="mamba---putting-it-all-together">Mamba - putting it all together</h3> <p>None of these three ingredients is new:</p> <ol> <li>Linear attention <d-cite key="katharopoulos2020transformers"></d-cite><d-cite key="sun2023retentive"></d-cite> and earlier SSMs <d-cite key="gu2021combining"></d-cite><d-cite key="gu2022efficiently"></d-cite> had similar equations utilizing state expansion.</li> <li>Selectivity was inspired by, and closely related to, gating mechanisms in classical RNNs like the LSTM and GRU <d-cite key="lstm"></d-cite><d-cite key="chung2014empirical"></d-cite>.</li> <li>Parallel scans were utilized in earlier SSMs/linear RNNs like S5 <d-cite key="smith2023s5"></d-cite> and LRU <d-cite key="orvieto2023resurrecting"></d-cite>. Linear attention variants also used parallelizable training algorithms leveraging matmuls.</li> </ol> <p>What Mamba did was show that <strong>combining all of these together</strong> was the key to a step change in empirical performance and approaching Transformers on language modeling.</p> <h3 id="modern-recurrent-models">Modern recurrent models</h3> <p>Since then, there’s been a flurry of activity on continuing to understand and improve recurrent models. Many of them come from different motivations with different nomenclatures and terminologies.</p> <ul> <li>Some models such as RWKV <d-cite key="peng2023rwkv"></d-cite><d-cite key="peng2024eagle"></d-cite><d-cite key="peng2025rwkv"></d-cite>, xLSTM <d-cite key="katharopoulos2020transformers"></d-cite>, and Griffin <d-cite key="de2024griffin"></d-cite> come from an <strong>RNN-centric</strong> point of view and call Ingredient 1 <em>matrix-valued states</em> and Ingredient 2 <em>gating</em>.</li> <li> <strong>Linear attention</strong> <d-cite key="katharopoulos2020transformers"></d-cite> first combined Ingredients 1 and 3; later variants such as GLA<d-cite key="yang2024gated"></d-cite> and Gated DeltaNet<d-cite key="yang2025gated"></d-cite> incorporate various forms of selectivity (data-dependent recurrence) and use attention-based terminology such as using $(K, Q, V)$ instead of $(B, C, X)$. Mamba-2 can also be simultaneously seen as either an SSM or a linear attention <d-cite key="dao2024transformers"></d-cite>.</li> <li>Recently, many of these models have been cast into a framework of <strong>test-time training/regression</strong><d-cite key="liu2024longhorn"></d-cite><d-cite key="sun2024learning"></d-cite><d-cite key="wang2025test"></d-cite><d-cite key="von2025mesanet"></d-cite>, which views the recurrent update as online optimization on some objective for remembering the context. The state is viewed as an <em>associative memory</em> and parallelization happens through a notion of <em>minibatch gradient descent</em>.</li> </ul> <p>A core commonality is that almost all of these models can be cast into the same SSM equation \eqref{eq:ssm}, with the main axes of variations being in the structure of $A_t$ (Ingredient 2) and corresponding efficient training algorithms (Ingredient 3). So I’ll use the term <strong>state space model</strong> (or just “modern recurrent model”) to refer broadly to this large class of new models, as it captures their main shared characteristics (e.g. SISO linear recurrence with state expansion). But of course, there are many other reasonable names given the closely related ideas!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/recurrent_models-480.webp 480w,/assets/img/2025-07-08-tradeoffs/recurrent_models-800.webp 800w,/assets/img/2025-07-08-tradeoffs/recurrent_models-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/recurrent_models.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">This figure is from Songlin Yang's excellent <a href="https://arxiv.org/abs/2406.06484" rel="external nofollow noopener" target="_blank">DeltaNet</a> paper, which shows how the huge proliferation of modern recurrent models all fits into this framework (using linear attention notation).</figcaption> </figure> <p>Despite the accelerating amount of research into this direction and steady stream of new models, however, I think that all of them are still quite similar to each other and have roughly similar empirical performance, for the most part. In particular, <strong>all of these models are much more similar to each other than they are to quadratic attention</strong>. So in the rest of this post, we’re going to try to get a grasp on the higher-level tradeoffs between SSMs and Transformers.</p> <h2 id="states-brains-and-databases">States, Brains, and Databases</h2> <p>I claim that we can understand the trade-offs of different models better by looking at what they store in (and how they manipulate) their <strong>autoregressive state</strong>.</p> <p>What does that mean? In some sense, every <em>autoregressive model</em> – one that generates data sequentially left-to-right like modern LLMs – is a “state space model” that holds some state in memory and evolves it on every time step (e.g. in between every generated word for an LLM).</p> <h3 id="autoregressive-states-of-sequence-models">Autoregressive states of sequence models</h3> <p>(Causal) self-attention, the core component of autoregressive Transformers, is often defined through a specific operation involving computing the pairwise interactions between every element of the sequence <d-cite key="vaswani2017attention"></d-cite>. Consequently, its computation cost scales <em>quadratically</em> in the sequence length, which is often viewed as the main drawback of attention.</p> <p>On the other hand, because computing one step of the recurrence \eqref{eq:ssm} takes constant time, processing an entire sequence scales <em>linearly</em> with the length of the sequence, which is often viewed as the main advantage of state space models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/state-480.webp 480w,/assets/img/2025-07-08-tradeoffs/state-800.webp 800w,/assets/img/2025-07-08-tradeoffs/state-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/state.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>But instead of thinking of the training cost of these models, I find it more illuminating to think about what happens at inference time when they process a new input.</p> <ul> <li>When a self-attention layer receives a new token, it needs to compare it to all the previously seen elements of the sequence, which means that <em>it must have cached a representation for every single prior token in the context</em>. Every new input it sees must get added to the cache, which therefore grows linearly in the context size.</li> <li>On the other hand, a state space model has always summarized its context $x_1, \cdots, x_t$ into the hidden state $h_t$ (equation \eqref{eq:ssm}), which always has a constant size. This fixed-size state is the only means by which the model can interact with data: it streams data in, compresses it into its state, and uses that to make decisions or produce new outputs.</li> </ul> <p>Without even getting into the details of the definitions of these various models, I think it’s roughly accurate to say that we could have defined them from first principles through their autoregressive states:</p> <ul> <li> <strong>Transformers (self-attention) are characterized by a state that caches every element of its history</strong>, and interacts with new data by doing a pass over every element of the cache.</li> <li> <strong>SSMs are characterized by a state that compresses all its history</strong>, and interacts with new data in an online streaming fashion.</li> </ul> <details><summary>Aside: The “KV” cache</summary> <p>The Transformer cache is, of course, more formally known as the <strong>KV cache</strong>, where “KV” refers to specific parts of how attention was first defined and named (key and value).</p> <p>But the point of this description is that I think that rather than defining the KV cache as a consequence of attention; perhaps in an alternative universe, (causal) self-attention could have been derived from first principles as the canonical model that stores a cache (“KV” or not) of its context. So in this post, I mainly call it a “context cache” or “token cache” instead to abstract out the main principle instead of implementation detail.</p> <p>As an aside, it’s rather interesting/amusing to me that often when I talk to LLM researchers, they call the recurrent state of SSMs a “type of KV cache” rather than calling the KV cache a type of state, which IMO is much more accurate and descriptive.</p> </details> <h3 id="a-coarse-analogy">A coarse analogy</h3> <p>Although SSMs are often viewed as more efficient but somewhat weaker versions of Transformers, it’s not as simple as that. Even ignoring computational efficiency, these models do have different tradeoffs in their inductive biases (or modeling power). Given the nature of the way they process data, here’s a rough analogy that I like.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/analogy-480.webp 480w,/assets/img/2025-07-08-tradeoffs/analogy-800.webp 800w,/assets/img/2025-07-08-tradeoffs/analogy-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/analogy.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Transformers are like databases</strong>: they treat every new observation as an important item that is filed away for future reference.</p> <p>On the other hand, <strong>SSMs are like brains</strong>: finite-sized memories that are always on, processing new inputs and producing new outputs in real-time.</p> <p>This analogy is a bit superficial, but does help intuitively explain some of the empirical behaviors that are observed. For example, SSMs can’t memorize a phonebook in one pass and then recite it back, or recall an arbitary person’s phone number from memory <d-cite key="jelassi2024repeat"></d-cite><d-cite key="waleffe2024empirical"></d-cite>. But then of course, neither can humans – we’re hopelessly bad at exact memorization and retrieval – but that doesn’t seem to hinder intelligence from arising! On the other hand, Transformers have a fundamental hard limit on context length (once the cache size is exceeded), while recurrent models like SSMs can hypothetically maintain an infinitely long (but fuzzy) memory of the past like humans have.</p> <details><summary>Aside: Context compression</summary> <p>The aforementioned limitation on context length might be circumvented by newer context compression techniques, which involve a more clever iterative process of throwing out the entire cache and trying to compress it into a shorter summary, so that new information can be processed that otherwise would overflow the cache. This of course must be lossy, and makes the whole system start resembling an SSM more.</p> <p>Similarly, the limitations of SSM may be alleviated by more clever iterative techniques of interacting with the data. For example, issues with recall might be remedied by giving them another pass over the data – just as how humans will look things up in external references.</p> <p>The theme here is that sometimes limitations of methods are not so black-and-white. They can depend on the way in which models are used and more generally on higher system-level changes. But we’re not going to get into these nuances for the purposes of this post.</p> </details> <details><summary>Aside: Long context</summary> <p>Something worth pointing out is that “long context” is a very popular, but horribly overloaded and ill-defined term. Both Transformers and SSMs have been touted as having better “long-context abilities” as a blanket statement, which can’t both be accurate.</p> <p>The reason is because they have very different <em>types</em> of memory. Going back to the analogy, I wouldn’t say that there is a clear winner comparing, say, my own memory vs. my research notes. They’re both just different: my notes lets me refer back to specific details I may have forgotten, but my brain remembers a much longer history of fuzzy context. Transformers and SSMs probably have similar qualitative differences that are difficult to measure.</p> <p>I’m very curious, for example, if large-scale SSMs (if trained properly with modern <a href="https://goombalab.github.io/blog/2025/improving-length-generalization/">length extrapolation techniques</a> <d-cite key="buitrago2025understanding"></d-cite>) would overcome the finite context problem that some chatbot users have complained about. Maintaining a continual conversation with an assistant is much more like human conversations and relationships: what matters is a long, persistent <em>summary</em> of the context, remembering the <em>shape and flow</em> of the interactions without needing to recall every specific detail. No one needs a scratchpad to have a continual relationship with their friend. This is exactly where the more brain-like nature of SSMs is more suitable than the database-like nature of Transformers, which instead may be better suited for AI tasks requiring precision and retrieval.</p> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-480.webp 480w,/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-800.webp 800w,/assets/img/2025-07-08-tradeoffs/intelligence_hybrid-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/intelligence_hybrid.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>A more intriguing empirical finding that might be predicted from the analogy is that combining both types of information processing may be even more capable! Just as human intelligence is augmented by having explicit scratch pads and external references, language models get better when combining SSMs with attention layers by a simple interleaving strategy.</p> <p>And what’s even more intriguing is that the optimal ratio of these layers, as independently verified by dozens of research groups by now (<a href="https://arxiv.org/abs/2212.14052" rel="external nofollow noopener" target="_blank">H3</a>, <a href="https://arxiv.org/abs/2403.19887" rel="external nofollow noopener" target="_blank">Jamba</a>, <a href="https://arxiv.org/abs/2405.16712" rel="external nofollow noopener" target="_blank">Zamba</a>, <a href="https://arxiv.org/abs/2406.07522" rel="external nofollow noopener" target="_blank">Samba</a>, and many more that followed after)<d-cite key="dao2023hungry"></d-cite><d-cite key="lieber2024jamba"></d-cite><d-cite key="glorioso2024zamba"></d-cite><d-cite key="ren2025samba"></d-cite>, is somewhere between a roughly 3:1 to 10:1 ratio of SSM:attention layers.<d-footnote>Note that this isn't factoring in computation cost (which is usually what's highlighted when comparing Transformers vs SSMs) - we're just talking about raw modeling ability. Put another way, taking a pure Transformer model and replacing some (or most) of the layers with SSM layers would both improve efficiency *and* performance.</d-footnote> This might track the coarse analogy if one believed that human intelligence is mostly in the brain and augmented by lightweight access to external databases! These hybrid models have now been scaled up to very serious sizes (e.g. MoE with 560B total parameters) by major labs, like NVIDIA’s <a href="https://research.nvidia.com/labs/adlr/nemotronh/" rel="external nofollow noopener" target="_blank">Nemotron-H</a> <d-cite key="blakeman2025nemotron"></d-cite> and Tencent’s <a href="https://tencent.github.io/llm.hunyuan.T1/README_EN.html" rel="external nofollow noopener" target="_blank">T1/TurboS</a> <d-cite key="liu2025hunyuan"></d-cite> with state-of-the-art performance.<d-footnote>Fun fact: both of these models were announced on the same day, my birthday 😂 (completely by coincidence)</d-footnote></p> <details><summary>Aside: Perplexity</summary> <p>When I talk about performance here, I’m specifically referring to perplexity. As a community, we now know that there are more nuances to the downstream performance, in particular <em>algorithmic capabilities</em> of different types of models<d-cite key="bick2025understanding"></d-cite>. But perplexity is still perhaps the most pure metric of the <em>statistical ability to model language as a distribution of sequences</em>, the original definition of language modeling.</p> <p>I actually believe that pound-for-pound (or FLOP-for-FLOP), SSMs are better than Transformers at modeling language, in this sense. But of course, there are many other downstream capabilities that have other differences and are important to understand.</p> </details> <h2 id="is-attention-all-you-need">Is Attention All You Need?</h2> <p>So <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">attention is all you need</a>, right? There’s a perception of Transformers being the ultimate architecture that can learn anything from raw data, the more the better, with having enough compute being the only bottleneck.</p> <blockquote class="block-danger"> <h4 id="myth">Myth</h4> <p>Just throw your data at a Transformer <em>🙂</em></p> </blockquote> <p>Well, not quite. Attention is indeed amazing and has become an effective backbone for pretty much all modalities, from its original applications in language to <a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">vision</a> and <a href="https://arxiv.org/abs/2005.08100" rel="external nofollow noopener" target="_blank">audio</a> and beyond<d-cite key="dosovitskiy2021image"></d-cite><d-cite key="gulati2020conformer"></d-cite>. But there is some more nuance to it.</p> <blockquote class="block-tip"> <h4 id="reality">Reality</h4> <p>Attention is most effective on<br> <strong>pre-compressed data</strong> at the “<strong><em>right level of abstraction</em></strong>”</p> </blockquote> <p>I claim instead that in order to use a Transformer effectively, the <em>data has to be substantially processed</em>. To support this claim, let’s look at how they’re actually used in practice.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/patches-480.webp 480w,/assets/img/2025-07-08-tradeoffs/patches-800.webp 800w,/assets/img/2025-07-08-tradeoffs/patches-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/patches.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/tokenizers-480.webp 480w,/assets/img/2025-07-08-tradeoffs/tokenizers-800.webp 800w,/assets/img/2025-07-08-tradeoffs/tokenizers-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/tokenizers.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In pretty much all real pipelines, raw data is processed by an encoder before being fed to a Transformer, for example:</p> <ul> <li>The <strong>patchification</strong> step in vision pipelines (whether <a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">classification</a> or <a href="https://arxiv.org/abs/2212.09748" rel="external nofollow noopener" target="_blank">generation</a>)<d-cite key="dosovitskiy2021image"></d-cite><d-cite key="peebles2023scalable"></d-cite>.</li> <li>The <strong>tokenization</strong> step of language modeling.</li> </ul> <p>This may seem intuitive: after all, because of the quadratic complexity of attention, of course it makes sense to try to simplify the data (such as shortening input sequences).</p> <p>But my claim is <em>not just about computational efficiency</em>; I’m making a stronger statement about limitations in <em>modeling power</em>.</p> <p>Let’s dig in more here.</p> <h3 id="should-we-get-rid-of-tokenization">Should we get rid of tokenization?</h3> <p>Tokenization is a notorious step of all language modeling pipelines (most commonly the “BPE” algorithm <d-cite key="sennrich2016neural"></d-cite>, which I’ll use interchangeably with “tokenization”), where textual data is processed into contiguous chunks, essentially encoding them into coarser features than the raw character-level data. It has a number of failure modes such as the <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation" rel="external nofollow noopener" target="_blank">SolidGoldMagikarp</a> edge case and the infamous “How many R‘s are there in the word ‘strawberry’?” test.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/karpathy-480.webp 480w,/assets/img/2025-07-08-tradeoffs/karpathy-800.webp 800w,/assets/img/2025-07-08-tradeoffs/karpathy-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/karpathy.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Taken with permission from the most prominent <a href="https://x.com/karpathy/status/1657949234535211009" rel="external nofollow noopener" target="_blank">hater</a> <a href="https://x.com/karpathy/status/1759996551378940395" rel="external nofollow noopener" target="_blank">of</a> <a href="https://x.com/karpathy/status/1816637781659254908" rel="external nofollow noopener" target="_blank">tokenizers</a>. The enemy of my enemy is a friend of mine!</figcaption> </figure> <p>So why do we use it?</p> <p>From polling a lot of opinions, almost everyone agrees that tokenizers are clunky and ugly, but a “necessary evil”.<d-footnote>It's kind of uncanny how many people use this exact phrasing!</d-footnote> Practically speaking, they sub-sample the sequence by a factor of around $5\times$ which dramatically improves the efficiency of the core language model. Despite the edge cases – which are gradually being understood and patched out – they <em>just work</em>, for the most part. It would be <em>nice</em> to get rid of them, but it’s not worth a dedicated effort.</p> <p>I, on the other hand, <strong>deeply believe that we should get rid of tokenization</strong>. I’m driven by aesthetics much more than the average person, I’d guess, and it’s because I think that they are rooted in intuition and intangible reasons that usually lead to deeper consequences down the line, even if we can’t predict them. In this case, I think that the consequences of overcoming tokenization <em>will extend far beyond the surface-level implications</em>.</p> <blockquote> <p>We should care about removing tokenization, not (just) for the practical reasons, but for the aesthetic and intangible reasons.</p> </blockquote> <p>Besides fixing the edge cases, removing tokenization simply <strong>adheres closer to the spirit of deep learning</strong>. Deep learning has always been about replacing handcrafted feature engineering with powerful end-to-end neural networks that can learn patterns automatically from data. From CNNs replacing manually engineered edge detectors in computer vision, to Transformers replacing linguistic features in NLP, major advances in AI have always happened with <strong>less data processing and more automatic learning</strong> (as popularly espoused by <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="external nofollow noopener" target="_blank">The Bitter Lesson</a>)<d-cite key="sutton2019bitter"></d-cite>.</p> <p>I believe that replacing tokenization with end-to-end models will have huge consequences for</p> <ul> <li> <strong>scaling laws</strong>: learning better patterns from raw data always leads to more powerful models;</li> <li> <strong>multilingual and multimodal</strong>: tokenization is notoriously hard or impossible for certain languages and other types of sequential data;</li> <li> <strong>reasoning</strong>: because models can learn more semantically meaningful patterns from the data, and reason over higher levels of abstraction;</li> </ul> <p>and much more, including probably a lot of implications I haven’t foreseen yet.</p> <p>(As I was writing this post up, Luca Perić released a parallel blog post focused specifically on tokenization and tokenizer-free architectures. <a href="https://lucalp.dev/bitter-lesson-tokenization-and-blt/" rel="external nofollow noopener" target="_blank">Check it out</a>!)</p> <h3 id="so-what-happens-without-tokenization">So what happens without tokenization?</h3> <p>In the modern era of LLMs, there’ve been astonishingly few papers that have thought about or tried to address this problem. It’s hard to even find trustworthy benchmarks about the performance of tokenizer-free models.</p> <p>So here’s a plot from our upcoming paper where we carefully ran standard architectures on byte-level language modeling (essentially, treating each English character as a separate token). (Note: Byte-level modeling with Mamba was first attempted by <a href="https://arxiv.org/abs/2401.13660" rel="external nofollow noopener" target="_blank">MambaByte</a> <d-cite key="wang2024mambabyte"></d-cite> from Sasha Rush’s group. This is a reproduction.)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/bpb_curve-480.webp 480w,/assets/img/2025-07-08-tradeoffs/bpb_curve-800.webp 800w,/assets/img/2025-07-08-tradeoffs/bpb_curve-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/bpb_curve.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Byte-level models trained on FineWeb-Edu (context length 8192). Sliding window attention (width=1024) is FLOP matched to Mamba, while global attention uses $2\times$ the FLOPs.</figcaption> </figure> <p>There are a number of implications here that most LLM researchers I’ve talked to find surprising.</p> <p>The first thing to note is that the SSM performs <em>much</em> better than the FLOP-matched Transformer. This might not seem surprising to many because byte sequences are much longer than BPE-token sequences, and the quadratic complexity of Transformers kicks in.</p> <p>But as I said earlier, the weakness of Transformers is not (just) about efficiency, but about modeling power. And what’s notable about this plot (in particular, focusing on global attention) is that <strong>when matching for <em>data</em> instead of compute, allowing the Transformer to use many more FLOPs, the SSM still outperforms it consistently</strong>!<d-footnote>This plot is the result after we specifically tuned for the global Transformer baseline; In other settings (e.g. different combinations of network width/depth/optimizer hyperparameters), there was a much larger gap between Mamba and global attention.</d-footnote></p> <p>For contrast: if we compared these models on the <em>exact same data, but tokenized</em><d-footnote>This experiment used sequences of 8k characters, which would be roughly 2k tokens long, a standard length for LLMs where we understand the empirical performance of different backbones well.</d-footnote>, their perplexity curves would look approximately the same (or the Transformer would be slightly better), and their FLOPs would also be similar. So keeping the <em>same models</em> and the <em>same data</em>, but simply untokenizing the inputs, simultaneously <strong>lets the Transformer use much more compute</strong> but also <strong>decreases its performance relative to the SSM</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/dna_scaling-480.webp 480w,/assets/img/2025-07-08-tradeoffs/dna_scaling-800.webp 800w,/assets/img/2025-07-08-tradeoffs/dna_scaling-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/dna_scaling.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Here’s another example. This plot is from the original Mamba paper, where we showed that Mamba scaled substantially better than Transformer out-of-the-box on DNA language modeling. Once again, this is a “tokenization-free” language with high-resolution input and small vocabulary size (just 4!), and the SSM strongly outperforms the Transformer when <em>data-matched</em> (while using less compute).</p> <p>(By the way, I hypothesize that these results for tokenizer-free models would hold for any reasonable variant of SSMs, such as probably most of the [<a href="#modern-recurrent-models">modern recurrent models</a>].)</p> <h3 id="a-heuristic-explanation">A heuristic explanation</h3> <p>A useful model of what’s happening is to turn back to the autoregressive state. In a nutshell, because Transformers have an explicit cache of all prior tokens, they have an <strong>inductive bias to pay attention to individual tokens</strong>.<d-footnote>To be precise, though, while the first layer sees individual tokens, token boundaries then get blurred in later layers as each sequence position can represent some combination of all tokens before it. This is meant to be an intuitive heuristic that I believe is useful, not a mechanistic explanation.</d-footnote> Or, perhaps more succinctly:</p> <blockquote> <p>The <strong>inductive bias</strong> of soft attention is <strong>hard attention</strong>.</p> </blockquote> <p>Here are some useful heuristics for when attention is naturally suited to the task:</p> <ul> <li>Does caching a representation for every “token” of data make sense?</li> <li>Does hard attention (focusing on or recalling an individual token) make sense?</li> </ul> <p>These questions point at the following idea: <strong>is each individual token semantically meaningful?</strong> For example, when reading language, we pay attention to units at the level of words (or subwords like prefixes/suffixes), which have <em>meaning</em>. But on the other hand, when this doesn’t hold – for example, it’s rare that we’d ever want to pay attention to an individual <em>character</em> when reading – the performance of attention suffers.<d-footnote>A slightly different explanation that some would propose is that attention simply gets confused by distractors in general, which is exacerbated when the data is too high-resolution, like at the character level. This explanation is also useful and I think actually points to the same underlying principle as mine.</d-footnote><d-footnote>On a related note, another researcher hypothesized that SSMs may be less prone to hallucination than Transformers; it hasn't been fleshed out, but if true would make sense from this intuition.</d-footnote></p> <p>What’s interesting is thinking about many other types of data which lie somewhere in between. For example, image patches can be quite meaningful when they capture some feature, but often can be useless or only partially meaningful.</p> <table> <thead> <tr> <th>Data</th> <th>Is a token “meaningful”?</th> </tr> </thead> <tbody> <tr> <td>Words / subword tokens</td> <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td> </tr> <tr> <td>Characters</td> <td><img class="emoji" title=":x:" alt=":x:" src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png" height="20" width="20"></td> </tr> <tr> <td>DNA base pairs</td> <td><img class="emoji" title=":x:" alt=":x:" src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png" height="20" width="20"></td> </tr> <tr> <td>Image, video, audio patches</td> <td><img class="emoji" title=":question:" alt=":question:" src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png" height="20" width="20"></td> </tr> <tr> <td>Time series datapoints</td> <td><img class="emoji" title=":question:" alt=":question:" src="https://github.githubassets.com/images/icons/emoji/unicode/2753.png" height="20" width="20"></td> </tr> </tbody> </table> <p>This is why I do think that attention is indispensable for data like tokenized language, which has largely been processed to a degree of meaning.<d-footnote>Many people will nitpick about whether BPE tokens represent any meaning. For sure they don't -- which is again a major reason I think tokenization needs to go. But to some approximation they do tend to find important repeated subwords like prefixes; and moreover there are a lot of hacks built-in, such as first segmenting on whitespace so that tokens can't cross word boundaries (which is very important to its performance; another indicator of just how broken tokenization is). So in practice, LLM vocabularies tend to contain lots of actual words, which could be considered "meaningful".</d-footnote></p> <p>On the other hand, when the data is generally not meaningful (in the sense of requiring a model to pay attention to individual units), such as character-level language or DNA<d-footnote>I'm aware that sometimes you do need to pay attention to individual characters or base pairs, and that understanding the interactions of single base pairs is actually a big problem for machine learning on DNA. This heuristic is a deliberate oversimplification that I still think is generally useful.</d-footnote>, Transformers don’t work well, and other models like SSMs hold a clear edge. SSMs in particular, with their compressed states, may be particularly suited for these because when data appears at resolutions that are too high to be useful, what the model needs to do is <strong>compress the data into more meaningful abstractions</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/applications-480.webp 480w,/assets/img/2025-07-08-tradeoffs/applications-800.webp 800w,/assets/img/2025-07-08-tradeoffs/applications-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/applications.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Mamba applications in the first 6 months after its release.</figcaption> </figure> <p>The above figure, which was helpfully sent to me by the hosts of <a href="https://www.cognitiverevolution.ai/" rel="external nofollow noopener" target="_blank">The Cognitive Revolution</a> podcast, shows the breakdown of where Mamba was actually used after being published. Despite being motivated by and focusing on language modeling in the paper, the majority of its applications were actually in other modalities!<d-footnote>I don't work in computer vision, and part of me is unsure how much of Mamba's popularity there is just trend following 😜 but I've been told, at least, that SSMs work pretty well!</d-footnote><d-footnote>Without giving away too much, I can also say that they are certainly <a href="https://cartesia.ai" rel="external nofollow noopener" target="_blank">very powerful for audio</a> if understood well and used correctly.</d-footnote> I think this is probably related to the above explanation: it’s very hard to find good “tokenizers” that provide meaning in data like time series, audio, and vision. And models that naturally compress, like SSMs, may have an advantage in inductive bias over Transformers.</p> <p>These heuristics are, of course, very unrefined, and I’m sure many researchers would take issue with this depiction. But I’ve found it helpful for intuition and has been pretty good at predicting when various models are effective in practice.</p> <details><summary>Aside: Theories of tokenization</summary> <p>As people start thinking about tokenization more, there are some interesting theoretical results that have emerged which support this central thesis (that Transformers require meaningful tokens).</p> <ol> <li> <p><a href="https://arxiv.org/abs/2402.18376" rel="external nofollow noopener" target="_blank">Tokenization Is More Than Compression</a> examined the hypothesis that <em>the primary role of tokenization is to shrink the input sequence length</em>. They invented a new tokenizer that has even higher compression rates than BPE (actually, they even keep the same vocabulary, but simply find different segmentations that are more compressed) yet leads to worse language models, providing evidence against the hypothesis<d-cite key="schmidt2024tokenization"></d-cite>.</p> </li> <li> <p><a href="https://openreview.net/forum?id=wm9JZq7RCe" rel="external nofollow noopener" target="_blank">An Analysis of Tokenization: Transformers under Markov Data</a> showed that for certain data distributions, applying tokenization qualitatively changes what Transformers can learn. Intuitively, commonly used tokenizers like BPE and Unigram are somewhat based in information-theoretic heuristics, and play a particular role in smoothing out the non-uniform information rate of raw data into a form that’s more easily processed by a Transformer<d-cite key="rajaraman2024analysis"></d-cite>.</p> </li> </ol> </details> <details><summary>Aside: Do SSMs not need meaningful input?</summary> <p>Of course, working on more meaningful inputs would benefit all models, not just Transformers. But I hypothesize that Transformers particularly rely on it.</p> <p>In one of the iterations that I gave this talk, an audience member asked me the question of what I thought would happen if Transformers or SSMs were run on “$n$-gram tokenized” language (instead of using BPE tokens, divide up the text into fixed windows of $n$ characters) or some other suboptimal tokenization.</p> <p>I predicted that both models would get worse on poorly segmented data, but it would affect SSMs less: in order of performance,</p> <p><code class="language-plaintext highlighter-rouge">Transformer (bad tokens) &lt; SSM (bad tokens) &lt; SSM (good tokens) &lt;= Transformer (good tokens)</code></p> <p>Byte/character-level modeling (equivalent to $n$=1) certainly provides some evidence for this.</p> </details> <h3 id="a-hypothetical-litmus-test">A hypothetical litmus test</h3> <p>Another thought experiment that’s intrigued me is what happens in the presence of noise. LLM data notoriously requires immense amounts of processing, filtering, and cleaning, but real-world data (and other modalities) aren’t like that. Humans also learn just fine from noisy data!</p> <p>So, what happens in a very simple scenario where information-less filler tokens are inserted into the sequence?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/thoughtexperiment-480.webp 480w,/assets/img/2025-07-08-tradeoffs/thoughtexperiment-800.webp 800w,/assets/img/2025-07-08-tradeoffs/thoughtexperiment-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/thoughtexperiment.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This figure illustrates a redundancy factor of $2\times$, but of course this can be arbitrarily increased to $k\times$ in the thought experiment. I think this shows another clear failure mode of standard attention: the compute shouldn’t be scaling as $k^2$, and the (inference) memory certainly shouldn’t scale in $k$ either – caching the noise tokens is pointless.</p> <p>SSMs are much better: as $k$ increases, the memory doesn’t grow. But it actually doesn’t fully fix the problem either, as <em>any</em> standard architecture would have compute scaling with $k$ (since every token is processed by the entire model). And so all LLMs suffer from this sort of noise and redundancy.<d-footnote>More recent ideas like mixture-of-depths and other conditional compute approaches may make some progress here, but I think don't sufficiently address it yet and I'm guessing would be brittle.</d-footnote></p> <p>In fact, I think thought experiments like this provide useful litmus tests for what <strong>“the right architecture”</strong> should look like. And I’ll informally propose this one as a goal for the future of architecture design (maybe someone will help me formalize it in a paper someday?).</p> <blockquote> <h4 id="a-litmus-test">A Litmus Test</h4> <p>An ideal architecture should be able to process this sequence-with-fillers task <strong>without (substantially) increased compute or memory usage</strong>.</p> </blockquote> <p>More generally, suppose we have two copies of a dataset, one of which contains a lot of extra noise, but overall they have essentially the same “information content”. We should expect “the right architecture” to behave essentially identically on both of these data sets.</p> <details><summary>Aside: Convolutions for language modeling</summary> <p>On a somewhat tangential note, I originally came up with the thought experiment in the figure above years ago, as a means to convince myself that convolutions don’t work for language modeling. When <a href="https://arxiv.org/abs/2111.00396" rel="external nofollow noopener" target="_blank">S4</a> was published, the community was excited about its potential on various modalities, and it spawned a wave of follow-up work on <a href="https://arxiv.org/abs/2302.10866" rel="external nofollow noopener" target="_blank">pure convolutional language models</a> <d-cite key="gu2022efficiently"></d-cite><d-cite key="poli2023hyena"></d-cite>.</p> <p>But over the course of working on linear time-invariant SSMs, I quickly realized they were hopeless for language modeling. This example shows why: because language doesn’t have an intrinsic “sampling rate”, tokens can be spaced somewhat arbitrarily. Clearly, even simple mis-spacings would drastically change what features a convolution could pick up on – in the above example, the convolution could not possibly output the same feature on both of those input sequences, in contrast to input-dependent sequence mixing layers like attention or selective SSMs.</p> <p>On the other hand, convolutions exhibit strong inductive bias exactly when there’s a notion of sampling rate that spaces inputs out at a consistent rate. This is another way of phrasing the “shift equivariant” inductive bias that makes them so great for (raw) perceptual modalities like vision and audio.</p> </details> <h3 id="is-attention-all-you-need-redux">Is attention all you need? (redux)</h3> <p>So through these discussions and examples, hopefully I’ve made a case for my original claim, which I’ll repeat here:</p> <blockquote> <p>Attention is most effective on<br> <strong>pre-compressed data</strong> at the “<strong><em><span style="color:red">right level of abstraction</span></em></strong>”</p> </blockquote> <p>This is, of course, an oversimplification of the picture – and I wouldn’t even know how to try to formally define a “level of abstraction” – but I do believe this is true in some fuzzy sense.</p> <h2 id="the-tradeoffs-of-state-space-models-and-transformers">The Tradeoffs of State Space Models and Transformers</h2> <p>Let’s finally return to the main topic for this blog post.</p> <h3 id="state-space-models-1">State space models</h3> <p>The trade-offs of SSMs are pretty clear from thinking intuitively about its autoregressive state.</p> <blockquote class="block-tip"> <h4 id="the-strength">The Strength</h4> <p>SSMs are the natural <strong>stateful model</strong> with efficient, interactive, online processing.</p> </blockquote> <blockquote class="block-danger"> <h4 id="the-weakness">The Weakness</h4> <p>SSMs lack fine-grained <strong>recall and retrieval</strong> abilities.</p> </blockquote> <p>Both of these are two sides of the same coin – consequences of its compressed state.</p> <p>I want to note, however, that I think there are strengths that are more subtle, and difficult to measure or even articulate.</p> <p>Going back to the [<a href="#a-coarse-analogy">brain analogy</a>], one question that intrigues me is whether <strong>compression is actually fundamental to intelligence</strong>.<d-footnote>My student Isaac has explored this hypothesis from a different angle: <a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html" rel="external nofollow noopener" target="_blank">[link]</a></d-footnote> Is it possible that forcing information into a smaller state forces a model to learn more useful patterns and abstractions? While compressed states are often viewed as a <a href="https://arxiv.org/abs/2402.01032" rel="external nofollow noopener" target="_blank">drawback</a> in the literature<d-cite key="jelassi2024repeat"></d-cite>, I think it might be because it’s very easy to measure these particular weaknesses but very hard to measure more subtle qualitative effects.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/bugfeature-480.webp 480w,/assets/img/2025-07-08-tradeoffs/bugfeature-800.webp 800w,/assets/img/2025-07-08-tradeoffs/bugfeature-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/bugfeature.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>At any rate, there are certainly many interesting applications where SSMs are the right tool for the job right now. And in my lab’s next release, we’ll show another interesting and important use case (for language!) where the <em>compressive inductive bias of SSMs turns out to be essential</em>. Stay tuned!</p> <h3 id="transformers">Transformers</h3> <p>Transformers perform exceptionally well, and in fact are pretty much the only tool for the job, on tasks that require paying attention to individual tokens in the context.</p> <blockquote class="block-tip"> <h4 id="the-strength-1">The Strength</h4> <p>Transformers have <strong>perfect recall</strong> and <strong>fine-grained manipulation</strong> of individual tokens in their context.</p> </blockquote> <p>And what about the downsides? Everyone knows that the main weakness of Transformers is their quadratic complexity, right?</p> <p>Not exactly. The main theme of this post is to convey that Transformers <em>do have inductive biases</em> that gives them weaknesses in terms of modeling power, not just efficiency. And just as with SSMs, both the high-level strengths and weaknesses of Transformers are two sides of the same coin, consequences of the structure of their autoregressive state: the token cache <strong>maintains the granularity of the input resolution</strong> it’s given.</p> <blockquote class="block-danger"> <h4 id="the-weakness-1">The Weakness</h4> <p>Transformers are <strong><em>beholden</em></strong> to the <strong>tokens</strong> they are given.</p> </blockquote> <p>In other words, they are more sensitive to the <em>resolution</em> and <em>semantic content</em> of the data. Transformers are characterized by their context cache, which stores a separate representation for every element of the sequence, which means that every element better be useful.</p> <details><summary>Aside: What about efficient attention?</summary> <p>Many variants of attention exist, which have been primarily motivated by the efficiency angle. I think my framing gives us better intuition of how these variants might behave. For example, I hypothesize that the same weakness is present for any variant of attention that maintains an explicit token cache; in particular, for example, any type of sparse attention. The core weakness is still there (and perhaps even exacerbated in the case of sparse attention): the model is biased toward <em>attending</em> to individual tokens.</p> <p>On the other hand, some variants of efficient attention “blur” the boundaries of tokens, including <a href="https://arxiv.org/abs/2006.04768" rel="external nofollow noopener" target="_blank">low-rank approximations</a><d-cite key="wang2020linformer"></d-cite> and any variant of linear attention. (More abstractly, these belong to a larger family of attention variants that make <em><a href="https://arxiv.org/abs/2405.21060" rel="external nofollow noopener" target="_blank">structured approximations</a></em> to the quadratic attention matrix<d-cite key="dao2024transformers"></d-cite>, any of which would have similar properties, I think.) Because of lacking a token-level cache, these models would not have the same weakness and would instead inherit properties much closer to SSMs.</p> <p>Incidentally, this is another more subtle reason why I somewhat prefer using “state space model” or “recurrent model” as a descriptive term over “linear attention”. To me, the term “attention” is <em>characterized</em> by maintaining a token-resolution state and having access to individual elements – in other words, being able to <strong>pay attention</strong> to a single token.</p> </details> <h2 id="scaling-laws">Scaling Laws</h2> <p>To end, let’s talk about one of the major drivers of the current wave of progress in AI:<br> <strong>scaling laws</strong>, or the phenomenon that spending more compute on models consistently leads to more capabilities.</p> <p>These laws are always plotted with FLOPs on the x-axis and some measure of performance on the y-axis, with the idea being that the slope of this line measures “the rate at which <strong>compute</strong> is converted into <strong>capabilities</strong>”. Indeed, I think there’s a popular viewpoint that Transformers are simply a vehicle that optimally performs this conversion.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/scaling-480.webp 480w,/assets/img/2025-07-08-tradeoffs/scaling-800.webp 800w,/assets/img/2025-07-08-tradeoffs/scaling-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/scaling.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>And I think this is a great depiction of the goal of architecture research. We’re simply looking for <strong>the black box that performs this conversion in the best way possible</strong>. From this perspective, there is only one central question:</p> <blockquote> <p>Is my model using its compute wisely?</p> </blockquote> <p>In other words, we want every FLOP to count. And as is hopefully clear after this post (at least, I’ve convinced myself!), Transformers are far from optimal.</p> <details><summary>Aside: Does it actually matter?</summary> <p>There’s another layer to the picture that I haven’t touched on, which is the practical efficiency of models. As <a href="https://tridao.me/" rel="external nofollow noopener" target="_blank">Tri</a> likes to frame it, what we actually care about is “dollars-to-capabilities”, which can be factored into (1) “dollars-to-FLOPs” and (2) “FLOPs-to-capabilities”. One might need to balance these two, for example, by accepting a suboptimal architecture for (2) in return for much more efficient (1). And some might say that Transformers have optimized the combination of these two.</p> <p>I still care primarily about question (2), partly because I personally find it more intellectually interesting, but also because I truly believe there are substantial advances to be made that change the balance even factoring in (1).</p> <p>A second higher-level question touching on whether it actually matters is: do we need to improve on anything to get to AGI/ASI? The answer here might be no – tokenized Transformers may very well represent a viable path – but I think that finding improvements may either get us there faster, or ultimately lead to more intelligent models.</p> </details> <p>Don’t get me wrong: despite being known as a leader of the Transformer-alternatives direction, I think Transformers are amazing and <em>attention is truly a fundamental modeling primitive</em>. But I also think it’s clear that they, by themselves, are not the final solution. We still have work to do.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-07-08-tradeoffs/meme-480.webp 480w,/assets/img/2025-07-08-tradeoffs/meme-800.webp 800w,/assets/img/2025-07-08-tradeoffs/meme-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-07-08-tradeoffs/meme.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="whats-next">What’s next</h3> <p>Part of my reason for writing this post was to broadcast this content to a wider audience, and so this talk can be sunsetted :)</p> <p>But it’s also setting up for the next major architecture advancement…</p> <h3 id="acknowledgements">Acknowledgements</h3> <p>Thanks to Tri Dao and Luca Perić for feedback on this post.</p> <h3 id="cite-this-post">Cite this post</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@online{gu2025tradeoffs,
  author  = {Albert Gu},
  title   = {On the Tradeoffs of State Space Models and Transformers},
  year    = {2025},
  url     = {https://goombalab.github.io/blog/2025/tradeoffs/},
}
</code></pre></div></div> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/albert.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Goomba AI Lab. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cloud.umami.is/script.js" data-website-id="340bca3c-b84e-462f-98dd-f4f4629b9751"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>